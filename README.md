<div align="center">

# ğŸ¨ Scene2Story ğŸ“¸â†’ğŸ“–

<h3>âœ¨ Transform Images into Compelling Narratives using Deep Learning âœ¨</h3>

<p align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&size=22&duration=3000&pause=1000&color=6366F1&center=true&vCenter=true&width=600&lines=AI-Powered+Story+Generation;ResNet50+%2B+GPT-2+Architecture;From+Pixels+to+Prose" alt="Typing SVG" />
</p>

---

### ğŸš€ Tech Stack

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Transformers](https://img.shields.io/badge/ğŸ¤—_Transformers-Latest-FFD21E?style=for-the-badge)
![CLIP](https://img.shields.io/badge/CLIP-Vision_Encoder-FF6B6B?style=for-the-badge)
![GPT-2](https://img.shields.io/badge/GPT--2-Language_Model-4ECDC4?style=for-the-badge)
![Streamlit](https://img.shields.io/badge/Streamlit-Web_UI-FF4B4B?style=for-the-badge)

### ğŸ“Š Project Status

![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)
![Maintenance](https://img.shields.io/badge/Maintained-Yes-brightgreen?style=for-the-badge)
![Platform](https://img.shields.io/badge/Platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey?style=for-the-badge)
![Code Style](https://img.shields.io/badge/Code_Style-Black-000000?style=for-the-badge)

---

### ğŸ§­ Quick Navigation

<p align="center">
  <a href="#-features">ğŸŒŸ Features</a> â€¢
  <a href="#-installation">ğŸ“¦ Installation</a> â€¢
  <a href="#-usage">ğŸš€ Usage</a> â€¢
  <a href="#ï¸-configuration">âš™ï¸ Configuration</a> â€¢
  <a href="#-model-training">ğŸ¯ Training</a>
</p>

</div>

<br>

## ğŸ¯ Overview

<div align="center">
<table>
<tr>
<td width="50%">

**ğŸ” What it does:**
- Analyzes images using CLIP vision encoder
- Extracts meaningful visual features
- Generates creative narratives
- Combines CLIP + GPT-2 architectures
- Interactive web interface

</td>
<td width="50%">

**ğŸª Perfect for:**
- Creative writing assistance
- Educational storytelling
- Content generation
- AI research projects

</td>
</tr>
</table>
</div>

> **Scene2Story** is an AI-powered system that transforms images into compelling narratives by seamlessly combining computer vision and natural language processing technologies.

## âœ¨ Features

- ğŸ–¼ï¸ **Image Feature Extraction** - CLIP-based visual encoding
- ğŸ“ **Story Generation** - GPT-2 powered narrative creation
- ğŸ”„ **Custom Dataset Pipeline** - Flickr30k integration
- ğŸ§  **Model Training** - Complete training pipeline with checkpointing
- âš™ï¸ **YAML Configuration** - Easy parameter tuning
- ğŸ“Š **Logging System** - Comprehensive tracking
- ğŸ¨ **Multimodal Architecture** - Custom image token integration
- ğŸ’¾ **Model Persistence** - Automatic checkpoint saving and loading
- ğŸŒ **Web Interface** - Streamlit-based interactive UI
- ğŸš€ **FastAPI Support** - Ready for API deployment

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8+
- CUDA (optional, for GPU acceleration)

### Setup

```bash
# Clone repository
git clone https://github.com/Alibubere/scene2story.git
cd scene2story

# Install dependencies
pip install -r requirements.txt

# Run the web interface
streamlit run app/ui.py
```

### Dataset Download

ğŸ“¥ **[Download Flickr30k Dataset](https://huggingface.co/datasets/nlphuji/flickr30k/resolve/main/flickr30k-images.zip?download=true)** (Auto-download)

Extract the downloaded zip file to `data/raw/flickr30k-images/`

## ğŸš€ Usage

### Training the Model

```bash
# Start training from scratch
python main.py

# Training will automatically resume from latest checkpoint if available
```

### Web Interface Usage

```bash
# Launch the interactive web interface
streamlit run app/ui.py
```

### Quick Start Example

```python
from src.data_prep.dataset import StoryImageDataset
from src.models.multimodel_gpt2 import MultimodelGPT2
from src.features.clip_encoder import get_pretrained_clip_encoder

# Load dataset
dataset = StoryImageDataset("data/processed/stories_train.jsonl")

# Initialize model
model = MultimodelGPT2(
    gpt2_model_name="gpt2",
    num_img_tokens=4,
    num_unfreeze_layers=4
)

# Load trained weights
checkpoint = torch.load("checkpoints/best.pth")
model.load_state_dict(checkpoint["model_state"])
```

## ğŸ“ Project Structure

```
scene2story/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ ui.py                    # Streamlit web interface
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best.pth                 # Best model checkpoint
â”‚   â””â”€â”€ latest.pth               # Latest training checkpoint
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # Configuration file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/               # Processed datasets
â”‚   â”‚   â”œâ”€â”€ stories_train.jsonl  # Training data
â”‚   â”‚   â””â”€â”€ stories_val.jsonl    # Validation data
â”‚   â””â”€â”€ raw/                     # Raw Flickr30k data
â”‚       â”œâ”€â”€ flickr30k-images/    # Image files
â”‚       â””â”€â”€ flickr_annotations_30k.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep/
â”‚   â”‚   â”œâ”€â”€ dataloader.py        # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ dataset.py           # PyTorch dataset
â”‚   â”‚   â”œâ”€â”€ flickr_loader.py     # Flickr data processing
â”‚   â”‚   â”œâ”€â”€ story_generator.py   # Story creation logic
â”‚   â”‚   â””â”€â”€ save_story_dataset.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ clip_encoder.py      # CLIP vision encoder
â”‚   â”‚   â””â”€â”€ extract_image_features.py  # Feature extraction utilities
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ multimodel_gpt2.py   # Main multimodal model
â”‚   â”‚   â”œâ”€â”€ story_generation.py  # Generation utilities
â”‚   â”‚   â”œâ”€â”€ train.py             # Training script
â”‚   â”‚   â”œâ”€â”€ train_loop.py        # Training loop logic
â”‚   â”‚   â””â”€â”€ training_utils.py    # Training utilities
â”‚   â””â”€â”€ text/
â”‚       â””â”€â”€ tokenizer_utils.py   # GPT-2 tokenization
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ Pipeline.log             # Training logs
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ LICENCE                      # MIT License
â””â”€â”€ main.py                      # Training entry point
```

## âš™ï¸ Configuration

Edit `configs/config.yaml`:

```yaml
paths:
  annotations_csv: "data/raw/flickr_annotations_30k.csv"
  images_dir: "data/raw/flickr30k-images"

data:
  use_split: "train"   # 'train', 'val', 'test', or 'all'
  num_preview: 3

clean_paths:
  save_dir: "data/processed"

model:
  gpt2_type: "gpt2"           # 'gpt2' or 'gpt2-medium'
  d_model: 768                # GPT-2 hidden size
  num_img_tokens: 4           # Number of image tokens
  num_unfreeze_layers: 4      # Trainable transformer layers
  dropout: 0.1

training:
  num_epochs: 20
  batch_size: 8
  lr: 0.00005
  weight_decay: 0.005
  resume_from_checkpoint: True
  use_amp: False
  max_grad_norm: 1.0
  early_stop_patience: 7
```

## ğŸ› ï¸ Tech Stack

- **Deep Learning**: PyTorch, torchvision
- **NLP**: Hugging Face Transformers (GPT-2)
- **Vision**: CLIP (OpenAI)
- **Data**: Pandas, PIL
- **Config**: PyYAML
- **Web Interface**: Streamlit
- **API**: FastAPI, Uvicorn
- **Training**: Mixed precision training, automatic checkpointing
- **Logging**: Comprehensive training metrics

## ğŸŒ Web Interface

The project includes a user-friendly Streamlit web interface:

- **Image Upload**: Drag and drop or browse for images
- **Custom Prompts**: Add optional story prompts
- **Real-time Generation**: Generate stories with adjustable parameters
- **Interactive Display**: View generated stories alongside input images

### Launch Web Interface

```bash
streamlit run app/ui.py
```

## ğŸ¯ Model Training

The model supports:
- **Automatic Checkpointing**: Saves best and latest model states
- **Resume Training**: Automatically resumes from the latest checkpoint
- **Mixed Precision**: Efficient GPU memory usage
- **Validation Tracking**: Monitors validation loss for best model selection
- **Comprehensive Logging**: Detailed training metrics in `logs/Pipeline.log`

### Training Progress

Monitor training progress through:
- Real-time loss tracking every 250 batches
- Validation loss evaluation after each epoch
- Generated story samples for quality assessment
- Automatic best model saving based on validation performance

## ğŸ‘¨ğŸ’» Author

**Alibubere**

[![GitHub](https://img.shields.io/badge/GitHub-Alibubere-181717?style=flat&logo=github)](https://github.com/Alibubere)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Alibubere-0A66C2?style=flat&logo=linkedin)](https://www.linkedin.com/in/mohammad-ali-bubere-a6b830384/)
[![Email](https://img.shields.io/badge/Email-Contact-D14836?style=flat&logo=gmail&logoColor=white)](mailto:alibubere989@gmail.com)

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

1. Fork the project
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## â­ Show Your Support

Give a â­ï¸ if you like this project!

## ğŸ“® Contact

For questions or feedback, please open an issue on GitHub.

---

<div align="center">
Made with â¤ï¸ by Alibubere
</div>
