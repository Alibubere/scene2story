2025-12-07 21:00:43,387 - INFO - Logging initialize successfully
2025-12-07 21:00:43,522 - INFO - Loaded 29000 rows for split='train'.
2025-12-07 21:00:44,864 - INFO - Built 29000 samples from annotaions
2025-12-07 21:00:44,865 - INFO - Total samples: 29000
2025-12-07 21:03:51,037 - INFO - Logging initialize successfully
2025-12-07 21:03:51,172 - INFO - Loaded 29000 rows for split='train'.
2025-12-07 21:03:52,497 - INFO - Built 29000 samples from annotaions
2025-12-07 21:03:52,498 - INFO - Total samples: 29000
2025-12-07 21:04:50,218 - INFO - Logging initialize successfully
2025-12-07 21:04:50,338 - INFO - Loaded 29000 rows for split='train'.
2025-12-07 21:04:51,637 - INFO - Built 29000 samples from annotaions
2025-12-07 21:04:51,639 - INFO - Total samples: 29000
2025-12-08 12:40:20,696 - INFO - Logging initialize successfully
2025-12-08 12:40:20,928 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 12:40:23,343 - INFO - Built 29000 samples from annotaions
2025-12-08 12:40:23,349 - INFO - Total samples: 29000
2025-12-08 12:40:43,724 - INFO - Logging initialize successfully
2025-12-08 12:40:43,924 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 12:40:45,952 - INFO - Built 29000 samples from annotaions
2025-12-08 12:40:45,952 - INFO - Total samples: 29000
2025-12-08 12:57:57,493 - INFO - Logging initialize successfully
2025-12-08 12:57:57,722 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 12:57:59,880 - INFO - Built 29000 samples from annotaions
2025-12-08 12:57:59,880 - INFO - Total samples: 29000
2025-12-08 12:58:31,813 - INFO - Logging initialize successfully
2025-12-08 12:58:32,009 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 12:58:33,580 - INFO - Built 29000 samples from annotaions
2025-12-08 12:58:33,580 - INFO - Total samples: 29000
2025-12-08 12:59:02,916 - INFO - Logging initialize successfully
2025-12-08 12:59:03,118 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 12:59:04,940 - INFO - Built 29000 samples from annotaions
2025-12-08 12:59:04,940 - INFO - Total samples: 29000
2025-12-08 13:00:27,376 - INFO - Logging initialize successfully
2025-12-08 13:00:27,582 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 13:00:29,266 - INFO - Built 29000 samples from annotaions
2025-12-08 13:00:29,266 - INFO - Total samples: 29000
2025-12-08 13:00:45,144 - INFO - Logging initialize successfully
2025-12-08 13:00:45,366 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 13:00:47,346 - INFO - Built 29000 samples from annotaions
2025-12-08 13:00:47,346 - INFO - Total samples: 29000
2025-12-08 13:03:33,287 - INFO - Logging initialize successfully
2025-12-08 13:03:33,505 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 13:03:35,705 - INFO - Built 29000 samples from annotaions
2025-12-08 13:03:35,712 - INFO - Total samples: 29000
2025-12-08 14:03:23,882 - INFO - Logging initialize successfully
2025-12-08 14:03:24,024 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:03:25,572 - INFO - Built 29000 samples from annotaions
2025-12-08 14:03:25,572 - INFO - Total samples: 29000
2025-12-08 14:04:24,621 - INFO - Logging initialize successfully
2025-12-08 14:04:24,813 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:04:26,879 - INFO - Built 29000 samples from annotaions
2025-12-08 14:04:26,879 - INFO - Total samples: 29000
2025-12-08 14:06:13,308 - INFO - Logging initialize successfully
2025-12-08 14:06:13,444 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:06:15,034 - INFO - Built 29000 samples from annotaions
2025-12-08 14:06:15,034 - INFO - Total samples: 29000
2025-12-08 14:08:06,507 - INFO - Logging initialize successfully
2025-12-08 14:08:06,721 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:08:08,769 - INFO - Built 29000 samples from annotaions
2025-12-08 14:08:08,769 - INFO - Total samples: 29000
2025-12-08 14:08:46,488 - INFO - Logging initialize successfully
2025-12-08 14:08:46,687 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:08:49,019 - INFO - Built 29000 samples from annotaions
2025-12-08 14:08:49,025 - INFO - Total samples: 29000
2025-12-08 14:10:03,485 - INFO - Logging initialize successfully
2025-12-08 14:10:03,695 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:10:05,465 - INFO - Built 29000 samples from annotaions
2025-12-08 14:10:05,465 - INFO - Total samples: 29000
2025-12-08 14:10:53,535 - INFO - Logging initialize successfully
2025-12-08 14:10:53,744 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:10:55,346 - INFO - Built 29000 samples from annotaions
2025-12-08 14:10:55,346 - INFO - Total samples: 29000
2025-12-08 14:12:18,991 - INFO - Logging initialize successfully
2025-12-08 14:12:19,223 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:12:21,683 - INFO - Built 29000 samples from annotaions
2025-12-08 14:12:21,683 - INFO - Total samples: 29000
2025-12-08 14:20:35,771 - INFO - Logging initialize successfully
2025-12-08 14:20:36,004 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:20:38,388 - INFO - Built 29000 samples from annotaions
2025-12-08 14:20:38,388 - INFO - Total samples: 29000
2025-12-08 14:22:10,097 - INFO - Logging initialize successfully
2025-12-08 14:22:10,317 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:22:12,671 - INFO - Built 29000 samples from annotaions
2025-12-08 14:22:12,671 - INFO - Total samples: 29000
2025-12-08 14:25:48,144 - INFO - Logging initialize successfully
2025-12-08 14:25:48,360 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:25:50,967 - INFO - Built 29000 samples from annotaions
2025-12-08 14:25:50,967 - INFO - Total samples: 29000
2025-12-08 14:34:33,027 - INFO - Logging initialize successfully
2025-12-08 14:34:33,251 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:34:35,757 - INFO - Built 29000 samples from annotaions
2025-12-08 14:34:35,761 - INFO - Total samples: 29000
2025-12-08 14:35:07,901 - INFO - Logging initialize successfully
2025-12-08 14:35:08,121 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:35:10,496 - INFO - Built 29000 samples from annotaions
2025-12-08 14:35:10,496 - INFO - Total samples: 29000
2025-12-08 14:36:31,954 - INFO - Logging initialize successfully
2025-12-08 14:36:32,178 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:36:34,612 - INFO - Built 29000 samples from annotaions
2025-12-08 14:36:34,613 - INFO - Total samples: 29000
2025-12-08 14:38:25,806 - INFO - Logging initialize successfully
2025-12-08 14:38:26,021 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:38:28,446 - INFO - Built 29000 samples from annotaions
2025-12-08 14:38:28,448 - INFO - Total samples: 29000
2025-12-08 14:40:02,087 - INFO - Logging initialize successfully
2025-12-08 14:40:02,303 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:40:04,784 - INFO - Built 29000 samples from annotaions
2025-12-08 14:40:04,784 - INFO - Total samples: 29000
2025-12-08 14:47:33,387 - INFO - Logging initialize successfully
2025-12-08 14:47:33,625 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:47:36,124 - INFO - Built 29000 samples from annotaions
2025-12-08 14:47:36,124 - INFO - Total samples: 29000
2025-12-08 14:48:55,038 - INFO - Logging initialize successfully
2025-12-08 14:48:55,276 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:48:57,718 - INFO - Built 29000 samples from annotaions
2025-12-08 14:48:57,718 - INFO - Total samples: 29000
2025-12-08 14:48:57,881 - INFO - Built 29000 samples for story 
2025-12-08 14:49:06,881 - INFO - Logging initialize successfully
2025-12-08 14:49:07,091 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:49:09,475 - INFO - Built 29000 samples from annotaions
2025-12-08 14:49:09,475 - INFO - Total samples: 29000
2025-12-08 14:49:09,634 - INFO - Built 29000 samples for story 
2025-12-08 14:49:17,233 - INFO - Logging initialize successfully
2025-12-08 14:49:17,518 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:49:19,907 - INFO - Built 29000 samples from annotaions
2025-12-08 14:49:19,907 - INFO - Total samples: 29000
2025-12-08 14:49:20,062 - INFO - Built 29000 samples for story 
2025-12-08 14:50:28,757 - INFO - Logging initialize successfully
2025-12-08 14:50:28,986 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:50:31,428 - INFO - Built 29000 samples from annotaions
2025-12-08 14:50:31,430 - INFO - Total samples: 29000
2025-12-08 14:50:31,623 - INFO - Built 29000 samples for story 
2025-12-08 14:51:27,027 - INFO - Logging initialize successfully
2025-12-08 14:51:27,249 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 14:51:29,605 - INFO - Built 29000 samples from annotaions
2025-12-08 14:51:29,606 - INFO - Total samples: 29000
2025-12-08 14:51:29,749 - INFO - Built 29000 samples for story 
2025-12-08 15:04:51,266 - INFO - Logging initialize successfully
2025-12-08 15:04:51,396 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:04:52,678 - INFO - Built 29000 samples from annotaions
2025-12-08 15:04:52,688 - INFO - Total samples: 29000
2025-12-08 15:04:52,765 - INFO - Built 29000 samples for story 
2025-12-08 15:05:02,790 - INFO - Logging initialize successfully
2025-12-08 15:05:02,920 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:05:04,187 - INFO - Built 29000 samples from annotaions
2025-12-08 15:05:04,187 - INFO - Total samples: 29000
2025-12-08 15:05:04,260 - INFO - Built 29000 samples for story 
2025-12-08 15:06:15,073 - INFO - Logging initialize successfully
2025-12-08 15:06:15,207 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:06:16,467 - INFO - Built 29000 samples from annotaions
2025-12-08 15:06:16,467 - INFO - Total samples: 29000
2025-12-08 15:06:16,562 - INFO - Built 29000 samples for story 
2025-12-08 15:16:40,263 - INFO - Logging initialize successfully
2025-12-08 15:16:40,384 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:16:41,701 - INFO - Built 29000 samples from annotaions
2025-12-08 15:16:41,701 - INFO - Total samples: 29000
2025-12-08 15:16:41,784 - INFO - Built 29000 samples for story 
2025-12-08 15:16:59,301 - INFO - Logging initialize successfully
2025-12-08 15:16:59,425 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:17:00,725 - INFO - Built 29000 samples from annotaions
2025-12-08 15:17:00,725 - INFO - Total samples: 29000
2025-12-08 15:17:00,814 - INFO - Built 29000 samples for story 
2025-12-08 15:18:10,755 - INFO - Logging initialize successfully
2025-12-08 15:18:10,892 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:18:12,172 - INFO - Built 29000 samples from annotaions
2025-12-08 15:18:12,172 - INFO - Total samples: 29000
2025-12-08 15:18:12,260 - INFO - Built 29000 samples for story 
2025-12-08 15:27:27,312 - INFO - Logging initialize successfully
2025-12-08 15:28:26,896 - INFO - Logging initialize successfully
2025-12-08 15:28:27,018 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:28:28,307 - INFO - Built 29000 samples from annotaions
2025-12-08 15:28:28,307 - INFO - Total samples: 29000
2025-12-08 15:28:28,400 - INFO - Built 29000 samples for story 
2025-12-08 15:32:14,314 - INFO - Logging initialize successfully
2025-12-08 15:32:14,435 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:32:15,751 - INFO - Built 29000 samples from annotaions
2025-12-08 15:32:15,751 - INFO - Total samples: 29000
2025-12-08 15:32:15,826 - INFO - Built 29000 samples for story 
2025-12-08 15:37:15,215 - INFO - Logging initialize successfully
2025-12-08 15:37:15,356 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:37:16,640 - INFO - Built 29000 samples from annotaions
2025-12-08 15:37:16,656 - INFO - Total samples: 29000
2025-12-08 15:37:16,738 - INFO - Built 29000 samples for story 
2025-12-08 15:37:17,187 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-08 15:38:45,152 - INFO - Logging initialize successfully
2025-12-08 15:38:45,272 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:38:46,595 - INFO - Built 29000 samples from annotaions
2025-12-08 15:38:46,596 - INFO - Total samples: 29000
2025-12-08 15:38:46,676 - INFO - Built 29000 samples for story 
2025-12-08 15:38:47,059 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-08 15:39:58,266 - INFO - Logging initialize successfully
2025-12-08 15:39:58,404 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:39:59,732 - INFO - Built 29000 samples from annotaions
2025-12-08 15:39:59,734 - INFO - Total samples: 29000
2025-12-08 15:39:59,820 - INFO - Built 29000 samples for story 
2025-12-08 15:40:14,267 - INFO - Logging initialize successfully
2025-12-08 15:40:14,402 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 15:40:15,727 - INFO - Built 29000 samples from annotaions
2025-12-08 15:40:15,727 - INFO - Total samples: 29000
2025-12-08 15:40:15,819 - INFO - Built 29000 samples for story 
2025-12-08 15:40:15,966 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-08 17:18:21,274 - INFO - Logging initialize successfully
2025-12-08 17:18:21,417 - INFO - Loaded 1000 rows for split='test'.
2025-12-08 17:18:21,475 - INFO - Built 1000 samples from annotaions
2025-12-08 17:18:21,475 - INFO - Total samples: 1000
2025-12-08 17:18:21,478 - INFO - Built 1000 samples for story 
2025-12-08 17:18:21,478 - INFO - Cleaned data saved to data/processed\stories_test.jsonl
2025-12-08 19:50:18,596 - INFO - Logging initialize successfully
2025-12-08 19:50:18,740 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 19:50:20,117 - INFO - Built 29000 samples from annotaions
2025-12-08 19:50:20,117 - INFO - Total samples: 29000
2025-12-08 19:50:20,186 - INFO - Built 29000 samples for story 
2025-12-08 19:50:20,327 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-08 19:51:36,810 - INFO - Logging initialize successfully
2025-12-08 19:51:36,948 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 19:51:38,317 - INFO - Built 29000 samples from annotaions
2025-12-08 19:51:38,317 - INFO - Total samples: 29000
2025-12-08 19:51:38,379 - INFO - Built 29000 samples for story 
2025-12-08 19:51:38,524 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-08 19:56:15,571 - INFO - Logging initialize successfully
2025-12-08 19:56:15,710 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 19:56:17,073 - INFO - Built 29000 samples from annotaions
2025-12-08 19:56:17,073 - INFO - Total samples: 29000
2025-12-08 19:56:17,140 - INFO - Built 29000 samples for story 
2025-12-08 19:56:17,279 - INFO - Cleaned data saved to C:\Users\MUIN\Desktop\Ali\scene2story\data\processed\stories_train.jsonl
2025-12-08 19:57:17,218 - INFO - Logging initialize successfully
2025-12-08 19:59:10,212 - INFO - Logging initialize successfully
2025-12-08 19:59:10,345 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 19:59:11,724 - INFO - Built 29000 samples from annotaions
2025-12-08 19:59:11,725 - INFO - Total samples: 29000
2025-12-08 19:59:11,791 - INFO - Built 29000 samples for story 
2025-12-08 19:59:11,933 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-08 19:59:11,933 - ERROR - Data path must ends with .jsonl but got data/processed/stories_train_jsonl
2025-12-08 19:59:29,299 - INFO - Logging initialize successfully
2025-12-08 19:59:29,437 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 19:59:30,810 - INFO - Built 29000 samples from annotaions
2025-12-08 19:59:30,810 - INFO - Total samples: 29000
2025-12-08 19:59:30,879 - INFO - Built 29000 samples for story 
2025-12-08 19:59:31,027 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-08 20:01:17,963 - INFO - Logging initialize successfully
2025-12-08 20:01:18,095 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:01:19,485 - INFO - Built 29000 samples from annotaions
2025-12-08 20:01:19,485 - INFO - Total samples: 29000
2025-12-08 20:01:19,553 - INFO - Built 29000 samples for story 
2025-12-08 20:04:14,575 - INFO - Logging initialize successfully
2025-12-08 20:04:14,714 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:04:16,089 - INFO - Built 29000 samples from annotaions
2025-12-08 20:04:16,091 - INFO - Total samples: 29000
2025-12-08 20:04:16,157 - INFO - Built 29000 samples for story 
2025-12-08 20:04:28,896 - INFO - Logging initialize successfully
2025-12-08 20:04:29,018 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:04:30,332 - INFO - Built 29000 samples from annotaions
2025-12-08 20:04:30,332 - INFO - Total samples: 29000
2025-12-08 20:04:30,404 - INFO - Built 29000 samples for story 
2025-12-08 20:04:30,550 - INFO - Cleaned data saved to data/processed/stories_train.jsonl
2025-12-08 20:04:50,324 - INFO - Logging initialize successfully
2025-12-08 20:04:50,439 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:04:51,809 - INFO - Built 29000 samples from annotaions
2025-12-08 20:04:51,824 - INFO - Total samples: 29000
2025-12-08 20:04:51,889 - INFO - Built 29000 samples for story 
2025-12-08 20:04:52,032 - INFO - Cleaned data saved to data/processed/stories_train.jsonl
2025-12-08 20:06:12,870 - INFO - Logging initialize successfully
2025-12-08 20:06:12,985 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:06:14,336 - INFO - Built 29000 samples from annotaions
2025-12-08 20:06:14,336 - INFO - Total samples: 29000
2025-12-08 20:06:14,391 - INFO - Built 29000 samples for story 
2025-12-08 20:06:32,820 - INFO - Logging initialize successfully
2025-12-08 20:06:32,955 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:06:34,282 - INFO - Built 29000 samples from annotaions
2025-12-08 20:06:34,289 - INFO - Total samples: 29000
2025-12-08 20:06:34,351 - INFO - Built 29000 samples for story 
2025-12-08 20:06:34,504 - INFO - Cleaned data saved to data/processed/stories_train.jsonl
2025-12-08 20:07:00,459 - INFO - Logging initialize successfully
2025-12-08 20:07:00,567 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:07:01,917 - INFO - Built 29000 samples from annotaions
2025-12-08 20:07:01,933 - INFO - Total samples: 29000
2025-12-08 20:07:01,989 - INFO - Built 29000 samples for story 
2025-12-08 20:07:02,142 - INFO - Cleaned data saved to data/processed/stories_train.jsonl
2025-12-08 20:11:44,809 - INFO - Logging initialize successfully
2025-12-08 20:11:44,926 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:11:46,287 - INFO - Built 29000 samples from annotaions
2025-12-08 20:11:46,290 - INFO - Total samples: 29000
2025-12-08 20:11:46,354 - INFO - Built 29000 samples for story 
2025-12-08 20:11:46,509 - INFO - Cleaned data saved to data/processed/stories_train.jsonl
2025-12-08 20:12:54,762 - INFO - Logging initialize successfully
2025-12-08 20:12:54,898 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:12:56,258 - INFO - Built 29000 samples from annotaions
2025-12-08 20:12:56,258 - INFO - Total samples: 29000
2025-12-08 20:12:56,336 - INFO - Built 29000 samples for story 
2025-12-08 20:12:56,481 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-08 20:14:24,188 - INFO - Logging initialize successfully
2025-12-08 20:14:24,325 - INFO - Loaded 29000 rows for split='train'.
2025-12-08 20:14:25,720 - INFO - Built 29000 samples from annotaions
2025-12-08 20:14:25,725 - INFO - Total samples: 29000
2025-12-08 20:14:25,782 - INFO - Built 29000 samples for story 
2025-12-08 20:14:25,935 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 13:52:02,717 - INFO - Logging initialize successfully
2025-12-09 13:52:02,878 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 13:52:04,205 - INFO - Built 29000 samples from annotaions
2025-12-09 13:52:04,205 - INFO - Total samples: 29000
2025-12-09 13:52:04,274 - INFO - Built 29000 samples for story 
2025-12-09 13:52:04,413 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 13:53:13,130 - INFO - Logging initialize successfully
2025-12-09 13:53:13,251 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 13:53:14,546 - INFO - Built 29000 samples from annotaions
2025-12-09 13:53:14,546 - INFO - Total samples: 29000
2025-12-09 13:53:14,619 - INFO - Built 29000 samples for story 
2025-12-09 13:53:14,768 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 13:54:40,409 - INFO - Logging initialize successfully
2025-12-09 13:54:40,542 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 13:54:41,837 - INFO - Built 29000 samples from annotaions
2025-12-09 13:54:41,837 - INFO - Total samples: 29000
2025-12-09 13:54:41,910 - INFO - Built 29000 samples for story 
2025-12-09 13:54:42,053 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 13:55:04,811 - INFO - Logging initialize successfully
2025-12-09 13:55:04,917 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 13:55:06,161 - INFO - Built 29000 samples from annotaions
2025-12-09 13:55:06,161 - INFO - Total samples: 29000
2025-12-09 13:55:06,237 - INFO - Built 29000 samples for story 
2025-12-09 13:55:06,372 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 13:56:37,795 - INFO - Logging initialize successfully
2025-12-09 13:56:37,922 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 13:56:39,158 - INFO - Built 29000 samples from annotaions
2025-12-09 13:56:39,161 - INFO - Total samples: 29000
2025-12-09 13:56:39,227 - INFO - Built 29000 samples for story 
2025-12-09 13:56:39,378 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 13:58:46,475 - INFO - Logging initialize successfully
2025-12-09 13:58:46,595 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 13:58:47,844 - INFO - Built 29000 samples from annotaions
2025-12-09 13:58:47,850 - INFO - Total samples: 29000
2025-12-09 13:58:47,914 - INFO - Built 29000 samples for story 
2025-12-09 13:58:48,054 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 13:59:45,519 - INFO - Logging initialize successfully
2025-12-09 13:59:45,653 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 13:59:46,908 - INFO - Built 29000 samples from annotaions
2025-12-09 13:59:46,908 - INFO - Total samples: 29000
2025-12-09 13:59:46,971 - INFO - Built 29000 samples for story 
2025-12-09 13:59:47,115 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 14:52:39,377 - INFO - Logging initialize successfully
2025-12-09 14:52:39,584 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 14:52:41,894 - INFO - Built 29000 samples from annotaions
2025-12-09 14:52:41,894 - INFO - Total samples: 29000
2025-12-09 14:52:42,012 - INFO - Built 29000 samples for story 
2025-12-09 14:52:42,274 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 14:58:58,777 - INFO - Logging initialize successfully
2025-12-09 14:58:58,929 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 14:59:00,420 - INFO - Built 29000 samples from annotaions
2025-12-09 14:59:00,430 - INFO - Total samples: 29000
2025-12-09 14:59:00,508 - INFO - Built 29000 samples for story 
2025-12-09 14:59:00,681 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 15:05:20,168 - INFO - Logging initialize successfully
2025-12-09 15:05:20,305 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 15:05:21,749 - INFO - Built 29000 samples from annotaions
2025-12-09 15:05:21,749 - INFO - Total samples: 29000
2025-12-09 15:05:21,890 - INFO - Built 29000 samples for story 
2025-12-09 15:05:22,039 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 15:06:07,008 - INFO - Logging initialize successfully
2025-12-09 15:06:07,141 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 15:06:08,583 - INFO - Built 29000 samples from annotaions
2025-12-09 15:06:08,595 - INFO - Total samples: 29000
2025-12-09 15:06:08,665 - INFO - Built 29000 samples for story 
2025-12-09 15:06:08,831 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 15:08:03,634 - INFO - Logging initialize successfully
2025-12-09 15:08:03,778 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 15:08:05,260 - INFO - Built 29000 samples from annotaions
2025-12-09 15:08:05,260 - INFO - Total samples: 29000
2025-12-09 15:08:05,331 - INFO - Built 29000 samples for story 
2025-12-09 15:08:05,499 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 15:10:10,298 - INFO - Logging initialize successfully
2025-12-09 15:10:10,439 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 15:10:12,047 - INFO - Built 29000 samples from annotaions
2025-12-09 15:10:12,049 - INFO - Total samples: 29000
2025-12-09 15:10:12,132 - INFO - Built 29000 samples for story 
2025-12-09 15:10:12,313 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 15:11:32,260 - INFO - Logging initialize successfully
2025-12-09 15:11:32,401 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 15:11:33,885 - INFO - Built 29000 samples from annotaions
2025-12-09 15:11:33,885 - INFO - Total samples: 29000
2025-12-09 15:11:33,967 - INFO - Built 29000 samples for story 
2025-12-09 15:11:34,137 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 15:15:10,676 - INFO - Logging initialize successfully
2025-12-09 15:15:10,853 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 15:15:12,444 - INFO - Built 29000 samples from annotaions
2025-12-09 15:15:12,444 - INFO - Total samples: 29000
2025-12-09 15:15:12,525 - INFO - Built 29000 samples for story 
2025-12-09 15:15:12,698 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-09 15:16:53,919 - INFO - Logging initialize successfully
2025-12-09 15:16:54,081 - INFO - Loaded 29000 rows for split='train'.
2025-12-09 15:16:55,656 - INFO - Built 29000 samples from annotaions
2025-12-09 15:16:55,658 - INFO - Total samples: 29000
2025-12-09 15:16:55,736 - INFO - Built 29000 samples for story 
2025-12-09 15:16:55,909 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 11:35:52,865 - INFO - Logging initialize successfully
2025-12-10 11:35:53,023 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 11:35:54,562 - INFO - Built 29000 samples from annotaions
2025-12-10 11:35:54,562 - INFO - Total samples: 29000
2025-12-10 11:35:54,638 - INFO - Built 29000 samples for story 
2025-12-10 11:35:54,812 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 15:03:45,598 - INFO - Logging initialize successfully
2025-12-10 15:03:48,743 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 15:03:50,952 - INFO - Built 29000 samples from annotaions
2025-12-10 15:03:50,954 - INFO - Total samples: 29000
2025-12-10 15:03:51,033 - INFO - Built 29000 samples for story 
2025-12-10 15:03:51,214 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 15:04:45,168 - INFO - Logging initialize successfully
2025-12-10 15:04:47,543 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 15:04:49,707 - INFO - Built 29000 samples from annotaions
2025-12-10 15:04:49,716 - INFO - Total samples: 29000
2025-12-10 15:04:49,801 - INFO - Built 29000 samples for story 
2025-12-10 15:04:49,992 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:32:48,636 - INFO - Logging initialize successfully
2025-12-10 16:32:51,550 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:32:53,154 - INFO - Built 29000 samples from annotaions
2025-12-10 16:32:53,154 - INFO - Total samples: 29000
2025-12-10 16:32:53,245 - INFO - Built 29000 samples for story 
2025-12-10 16:32:53,418 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:34:03,131 - INFO - Logging initialize successfully
2025-12-10 16:34:06,175 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:34:08,064 - INFO - Built 29000 samples from annotaions
2025-12-10 16:34:08,067 - INFO - Total samples: 29000
2025-12-10 16:34:08,173 - INFO - Built 29000 samples for story 
2025-12-10 16:34:08,345 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:34:41,727 - INFO - Logging initialize successfully
2025-12-10 16:34:44,172 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:34:45,882 - INFO - Built 29000 samples from annotaions
2025-12-10 16:34:45,882 - INFO - Total samples: 29000
2025-12-10 16:34:45,962 - INFO - Built 29000 samples for story 
2025-12-10 16:34:46,141 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:44:12,952 - INFO - Logging initialize successfully
2025-12-10 16:44:15,186 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:44:16,622 - INFO - Built 29000 samples from annotaions
2025-12-10 16:44:16,624 - INFO - Total samples: 29000
2025-12-10 16:44:16,688 - INFO - Built 29000 samples for story 
2025-12-10 16:44:16,836 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:44:59,636 - INFO - Logging initialize successfully
2025-12-10 16:45:02,013 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:45:04,886 - INFO - Built 29000 samples from annotaions
2025-12-10 16:45:04,886 - INFO - Total samples: 29000
2025-12-10 16:45:05,032 - INFO - Built 29000 samples for story 
2025-12-10 16:45:05,460 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:47:12,164 - INFO - Logging initialize successfully
2025-12-10 16:47:14,075 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:47:15,477 - INFO - Built 29000 samples from annotaions
2025-12-10 16:47:15,479 - INFO - Total samples: 29000
2025-12-10 16:47:15,544 - INFO - Built 29000 samples for story 
2025-12-10 16:47:15,674 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:50:00,062 - INFO - Logging initialize successfully
2025-12-10 16:50:02,353 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:50:03,702 - INFO - Built 29000 samples from annotaions
2025-12-10 16:50:03,718 - INFO - Total samples: 29000
2025-12-10 16:50:03,782 - INFO - Built 29000 samples for story 
2025-12-10 16:50:03,924 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:53:08,656 - INFO - Logging initialize successfully
2025-12-10 16:53:10,567 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:53:11,942 - INFO - Built 29000 samples from annotaions
2025-12-10 16:53:11,943 - INFO - Total samples: 29000
2025-12-10 16:53:12,009 - INFO - Built 29000 samples for story 
2025-12-10 16:53:12,153 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:54:45,041 - INFO - Logging initialize successfully
2025-12-10 16:54:47,053 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:54:48,532 - INFO - Built 29000 samples from annotaions
2025-12-10 16:54:48,532 - INFO - Total samples: 29000
2025-12-10 16:54:48,597 - INFO - Built 29000 samples for story 
2025-12-10 16:54:48,743 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:56:24,111 - INFO - Logging initialize successfully
2025-12-10 16:56:26,305 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:56:27,716 - INFO - Built 29000 samples from annotaions
2025-12-10 16:56:27,718 - INFO - Total samples: 29000
2025-12-10 16:56:27,783 - INFO - Built 29000 samples for story 
2025-12-10 16:56:27,927 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:57:14,501 - INFO - Logging initialize successfully
2025-12-10 16:57:16,396 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:57:17,804 - INFO - Built 29000 samples from annotaions
2025-12-10 16:57:17,806 - INFO - Total samples: 29000
2025-12-10 16:57:17,871 - INFO - Built 29000 samples for story 
2025-12-10 16:57:18,014 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 16:57:56,309 - INFO - Logging initialize successfully
2025-12-10 16:57:58,571 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 16:57:59,947 - INFO - Built 29000 samples from annotaions
2025-12-10 16:57:59,947 - INFO - Total samples: 29000
2025-12-10 16:58:00,018 - INFO - Built 29000 samples for story 
2025-12-10 16:58:00,155 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 17:10:52,258 - INFO - Logging initialize successfully
2025-12-10 17:10:54,224 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 17:10:55,578 - INFO - Built 29000 samples from annotaions
2025-12-10 17:10:55,578 - INFO - Total samples: 29000
2025-12-10 17:10:55,655 - INFO - Built 29000 samples for story 
2025-12-10 17:10:55,804 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 17:13:12,632 - INFO - Logging initialize successfully
2025-12-10 17:13:14,714 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 17:13:16,088 - INFO - Built 29000 samples from annotaions
2025-12-10 17:13:16,088 - INFO - Total samples: 29000
2025-12-10 17:13:16,157 - INFO - Built 29000 samples for story 
2025-12-10 17:13:16,306 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 17:20:03,152 - INFO - Logging initialize successfully
2025-12-10 17:20:04,906 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 17:20:06,288 - INFO - Built 29000 samples from annotaions
2025-12-10 17:20:06,288 - INFO - Total samples: 29000
2025-12-10 17:20:06,361 - INFO - Built 29000 samples for story 
2025-12-10 17:20:06,504 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-10 17:21:50,651 - INFO - Logging initialize successfully
2025-12-10 17:21:52,915 - INFO - Loaded 29000 rows for split='train'.
2025-12-10 17:21:54,311 - INFO - Built 29000 samples from annotaions
2025-12-10 17:21:54,327 - INFO - Total samples: 29000
2025-12-10 17:21:54,387 - INFO - Built 29000 samples for story 
2025-12-10 17:21:54,534 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 10:14:38,395 - INFO - Logging initialize successfully
2025-12-11 10:14:41,694 - INFO - Loaded 1014 rows for split='val'.
2025-12-11 10:14:41,805 - INFO - Built 1014 samples from annotaions
2025-12-11 10:14:41,805 - INFO - Total samples: 1014
2025-12-11 10:14:41,808 - INFO - Built 1014 samples for story 
2025-12-11 10:14:41,823 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 10:16:24,987 - INFO - Logging initialize successfully
2025-12-11 10:16:27,470 - INFO - Loaded 1014 rows for split='val'.
2025-12-11 10:16:27,546 - INFO - Built 1014 samples from annotaions
2025-12-11 10:16:27,557 - INFO - Total samples: 1014
2025-12-11 10:16:27,561 - INFO - Built 1014 samples for story 
2025-12-11 10:16:27,566 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-11 10:17:15,305 - INFO - Logging initialize successfully
2025-12-11 10:17:17,722 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 10:17:19,384 - INFO - Built 29000 samples from annotaions
2025-12-11 10:17:19,384 - INFO - Total samples: 29000
2025-12-11 10:17:19,465 - INFO - Built 29000 samples for story 
2025-12-11 10:17:19,699 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:17:53,659 - INFO - Logging initialize successfully
2025-12-11 11:17:56,218 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:17:58,186 - INFO - Built 29000 samples from annotaions
2025-12-11 11:17:58,186 - INFO - Total samples: 29000
2025-12-11 11:17:58,273 - INFO - Built 29000 samples for story 
2025-12-11 11:17:58,452 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:18:20,267 - INFO - Logging initialize successfully
2025-12-11 11:18:22,517 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:18:24,200 - INFO - Built 29000 samples from annotaions
2025-12-11 11:18:24,200 - INFO - Total samples: 29000
2025-12-11 11:18:24,281 - INFO - Built 29000 samples for story 
2025-12-11 11:18:24,454 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:20:15,994 - INFO - Logging initialize successfully
2025-12-11 11:20:18,288 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:20:20,039 - INFO - Built 29000 samples from annotaions
2025-12-11 11:20:20,039 - INFO - Total samples: 29000
2025-12-11 11:20:20,122 - INFO - Built 29000 samples for story 
2025-12-11 11:20:20,295 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:20:42,507 - INFO - Logging initialize successfully
2025-12-11 11:20:44,914 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:20:46,624 - INFO - Built 29000 samples from annotaions
2025-12-11 11:20:46,630 - INFO - Total samples: 29000
2025-12-11 11:20:46,704 - INFO - Built 29000 samples for story 
2025-12-11 11:20:46,877 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:21:27,337 - INFO - Logging initialize successfully
2025-12-11 11:21:29,726 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:21:31,673 - INFO - Built 29000 samples from annotaions
2025-12-11 11:21:31,684 - INFO - Total samples: 29000
2025-12-11 11:21:31,761 - INFO - Built 29000 samples for story 
2025-12-11 11:21:31,933 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:22:26,039 - INFO - Logging initialize successfully
2025-12-11 11:22:28,478 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:22:30,145 - INFO - Built 29000 samples from annotaions
2025-12-11 11:22:30,153 - INFO - Total samples: 29000
2025-12-11 11:22:30,228 - INFO - Built 29000 samples for story 
2025-12-11 11:22:30,407 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:22:57,470 - INFO - Logging initialize successfully
2025-12-11 11:22:59,813 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:23:01,637 - INFO - Built 29000 samples from annotaions
2025-12-11 11:23:01,639 - INFO - Total samples: 29000
2025-12-11 11:23:01,717 - INFO - Built 29000 samples for story 
2025-12-11 11:23:01,883 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:25:57,936 - INFO - Logging initialize successfully
2025-12-11 11:26:00,463 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:26:03,110 - INFO - Built 29000 samples from annotaions
2025-12-11 11:26:03,119 - INFO - Total samples: 29000
2025-12-11 11:26:03,244 - INFO - Built 29000 samples for story 
2025-12-11 11:26:03,513 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:27:37,501 - INFO - Logging initialize successfully
2025-12-11 11:27:40,004 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:27:42,665 - INFO - Built 29000 samples from annotaions
2025-12-11 11:27:42,665 - INFO - Total samples: 29000
2025-12-11 11:27:42,787 - INFO - Built 29000 samples for story 
2025-12-11 11:27:43,066 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:29:14,008 - INFO - Logging initialize successfully
2025-12-11 11:29:16,588 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:29:19,200 - INFO - Built 29000 samples from annotaions
2025-12-11 11:29:19,217 - INFO - Total samples: 29000
2025-12-11 11:29:19,337 - INFO - Built 29000 samples for story 
2025-12-11 11:29:19,604 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:35:31,886 - INFO - Logging initialize successfully
2025-12-11 11:35:34,397 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:35:37,095 - INFO - Built 29000 samples from annotaions
2025-12-11 11:35:37,095 - INFO - Total samples: 29000
2025-12-11 11:35:37,237 - INFO - Built 29000 samples for story 
2025-12-11 11:35:37,540 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 11:39:42,968 - INFO - Logging initialize successfully
2025-12-11 11:39:45,495 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 11:39:48,231 - INFO - Built 29000 samples from annotaions
2025-12-11 11:39:48,247 - INFO - Total samples: 29000
2025-12-11 11:39:48,352 - INFO - Built 29000 samples for story 
2025-12-11 11:39:48,616 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 15:14:08,034 - INFO - Logging initialize successfully
2025-12-11 15:14:10,044 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 15:14:11,483 - INFO - Built 29000 samples from annotaions
2025-12-11 15:14:11,483 - INFO - Total samples: 29000
2025-12-11 15:14:11,566 - INFO - Built 29000 samples for story 
2025-12-11 15:14:11,726 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 15:17:17,107 - INFO - Logging initialize successfully
2025-12-11 15:17:18,992 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 15:17:20,350 - INFO - Built 29000 samples from annotaions
2025-12-11 15:17:20,352 - INFO - Total samples: 29000
2025-12-11 15:17:20,422 - INFO - Built 29000 samples for story 
2025-12-11 15:17:20,571 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 15:18:00,844 - INFO - Logging initialize successfully
2025-12-11 15:18:03,155 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 15:18:04,562 - INFO - Built 29000 samples from annotaions
2025-12-11 15:18:04,562 - INFO - Total samples: 29000
2025-12-11 15:18:04,633 - INFO - Built 29000 samples for story 
2025-12-11 15:18:04,777 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 15:52:25,663 - INFO - Logging initialize successfully
2025-12-11 15:52:27,683 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 15:52:29,035 - INFO - Built 29000 samples from annotaions
2025-12-11 15:52:29,035 - INFO - Total samples: 29000
2025-12-11 15:52:29,110 - INFO - Built 29000 samples for story 
2025-12-11 15:52:29,248 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 16:59:57,837 - INFO - Logging initialize successfully
2025-12-11 17:00:00,416 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 17:00:00,661 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 17:00:03,179 - INFO - Built 29000 samples from annotaions
2025-12-11 17:00:03,184 - INFO - Total train samples: 29000
2025-12-11 17:00:04,533 - INFO - Built 29000 samples from annotaions
2025-12-11 17:00:04,537 - INFO - Total val samples: 29000
2025-12-11 17:00:04,616 - INFO - Built 29000 samples for story 
2025-12-11 17:00:04,700 - INFO - Built 29000 samples for story 
2025-12-11 17:00:04,915 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 17:00:05,089 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-11 17:01:21,138 - INFO - Logging initialize successfully
2025-12-11 17:01:23,515 - INFO - Loaded 29000 rows for split='train'.
2025-12-11 17:01:23,719 - INFO - Loaded 1014 rows for split='val'.
2025-12-11 17:01:25,813 - INFO - Built 29000 samples from annotaions
2025-12-11 17:01:25,813 - INFO - Total train samples: 29000
2025-12-11 17:01:25,861 - INFO - Built 1014 samples from annotaions
2025-12-11 17:01:25,861 - INFO - Total val samples: 1014
2025-12-11 17:01:25,944 - INFO - Built 29000 samples for story 
2025-12-11 17:01:25,950 - INFO - Built 1014 samples for story 
2025-12-11 17:01:26,118 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-11 17:01:26,125 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-12 15:07:59,660 - INFO - Logging initialize successfully
2025-12-12 15:08:03,521 - INFO - Loaded 29000 rows for split='train'.
2025-12-12 15:08:03,738 - INFO - Loaded 1014 rows for split='val'.
2025-12-12 15:08:06,301 - INFO - Built 29000 samples from annotaions
2025-12-12 15:08:06,301 - INFO - Total train samples: 29000
2025-12-12 15:08:06,391 - INFO - Built 1014 samples from annotaions
2025-12-12 15:08:06,391 - INFO - Total val samples: 1014
2025-12-12 15:08:06,519 - INFO - Built 29000 samples for story 
2025-12-12 15:08:06,522 - INFO - Built 1014 samples for story 
2025-12-12 15:08:06,791 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-12 15:08:06,806 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-12 15:10:05,863 - INFO - Logging initialize successfully
2025-12-12 15:10:08,305 - INFO - Loaded 29000 rows for split='train'.
2025-12-12 15:10:08,534 - INFO - Loaded 1014 rows for split='val'.
2025-12-12 15:10:11,124 - INFO - Built 29000 samples from annotaions
2025-12-12 15:10:11,124 - INFO - Total train samples: 29000
2025-12-12 15:10:11,203 - INFO - Built 1014 samples from annotaions
2025-12-12 15:10:11,203 - INFO - Total val samples: 1014
2025-12-12 15:10:11,318 - INFO - Built 29000 samples for story 
2025-12-12 15:10:11,326 - INFO - Built 1014 samples for story 
2025-12-12 15:10:11,586 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-12 15:10:11,586 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-12 15:11:43,781 - INFO - Logging initialize successfully
2025-12-12 15:11:46,161 - INFO - Loaded 29000 rows for split='train'.
2025-12-12 15:11:46,371 - INFO - Loaded 1014 rows for split='val'.
2025-12-12 15:11:49,002 - INFO - Built 29000 samples from annotaions
2025-12-12 15:11:49,002 - INFO - Total train samples: 29000
2025-12-12 15:11:49,076 - INFO - Built 1014 samples from annotaions
2025-12-12 15:11:49,076 - INFO - Total val samples: 1014
2025-12-12 15:11:49,195 - INFO - Built 29000 samples for story 
2025-12-12 15:11:49,198 - INFO - Built 1014 samples for story 
2025-12-12 15:11:49,492 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-12 15:11:49,492 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 14:38:18,695 - INFO - Logging initialize successfully
2025-12-13 14:38:21,858 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 14:38:22,057 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 14:38:23,684 - INFO - Built 29000 samples from annotaions
2025-12-13 14:38:23,686 - INFO - Total train samples: 29000
2025-12-13 14:38:23,728 - INFO - Built 1014 samples from annotaions
2025-12-13 14:38:23,728 - INFO - Total val samples: 1014
2025-12-13 14:38:23,805 - INFO - Built 29000 samples for story 
2025-12-13 14:38:23,820 - INFO - Built 1014 samples for story 
2025-12-13 14:38:23,989 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 14:38:24,009 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 14:38:29,946 - INFO - No valid checkpoint found or resume disabled. Starting from scratch.
2025-12-13 14:38:52,496 - INFO - Epoch [1] Batch [0/7250]loss: 11.1202 
2025-12-13 14:38:53,116 - INFO - Epoch [1] Batch [10/7250]loss: 6.0506 
2025-12-13 14:38:53,728 - INFO - Epoch [1] Batch [20/7250]loss: 5.9497 
2025-12-13 14:38:54,364 - INFO - Epoch [1] Batch [30/7250]loss: 6.5085 
2025-12-13 14:38:55,089 - INFO - Epoch [1] Batch [40/7250]loss: 6.0402 
2025-12-13 14:38:55,802 - INFO - Epoch [1] Batch [50/7250]loss: 5.2765 
2025-12-13 14:38:56,515 - INFO - Epoch [1] Batch [60/7250]loss: 6.0757 
2025-12-13 14:38:57,230 - INFO - Epoch [1] Batch [70/7250]loss: 5.4768 
2025-12-13 14:38:57,942 - INFO - Epoch [1] Batch [80/7250]loss: 5.8543 
2025-12-13 14:38:58,658 - INFO - Epoch [1] Batch [90/7250]loss: 5.9304 
2025-12-13 14:38:59,371 - INFO - Epoch [1] Batch [100/7250]loss: 6.0616 
2025-12-13 14:39:00,084 - INFO - Epoch [1] Batch [110/7250]loss: 5.1211 
2025-12-13 14:39:00,801 - INFO - Epoch [1] Batch [120/7250]loss: 5.4131 
2025-12-13 14:39:01,513 - INFO - Epoch [1] Batch [130/7250]loss: 5.3081 
2025-12-13 14:39:02,233 - INFO - Epoch [1] Batch [140/7250]loss: 5.1989 
2025-12-13 14:40:42,990 - INFO - Logging initialize successfully
2025-12-13 14:40:44,894 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 14:40:45,004 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 14:40:46,344 - INFO - Built 29000 samples from annotaions
2025-12-13 14:40:46,344 - INFO - Total train samples: 29000
2025-12-13 14:40:46,387 - INFO - Built 1014 samples from annotaions
2025-12-13 14:40:46,387 - INFO - Total val samples: 1014
2025-12-13 14:40:46,440 - INFO - Built 29000 samples for story 
2025-12-13 14:40:46,456 - INFO - Built 1014 samples for story 
2025-12-13 14:40:46,596 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 14:40:46,596 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 14:40:50,583 - INFO - No valid checkpoint found or resume disabled. Starting from scratch.
2025-12-13 14:41:37,544 - INFO - Logging initialize successfully
2025-12-13 14:41:39,688 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 14:41:39,789 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 14:41:41,165 - INFO - Built 29000 samples from annotaions
2025-12-13 14:41:41,172 - INFO - Total train samples: 29000
2025-12-13 14:41:41,208 - INFO - Built 1014 samples from annotaions
2025-12-13 14:41:41,208 - INFO - Total val samples: 1014
2025-12-13 14:41:41,262 - INFO - Built 29000 samples for story 
2025-12-13 14:41:41,262 - INFO - Built 1014 samples for story 
2025-12-13 14:41:41,417 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 14:41:41,422 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 14:41:45,212 - INFO - No valid checkpoint found or resume disabled. Starting from scratch.
2025-12-13 14:42:03,080 - INFO - Epoch [1] Batch [0/7250] loss: 10.9764 
2025-12-13 14:42:03,630 - INFO - Epoch [1] Batch [10/7250] loss: 6.1081 
2025-12-13 14:42:04,186 - INFO - Epoch [1] Batch [20/7250] loss: 5.6708 
2025-12-13 14:42:04,740 - INFO - Epoch [1] Batch [30/7250] loss: 5.8751 
2025-12-13 14:42:05,300 - INFO - Epoch [1] Batch [40/7250] loss: 5.6083 
2025-12-13 14:42:05,880 - INFO - Epoch [1] Batch [50/7250] loss: 5.5389 
2025-12-13 14:42:06,449 - INFO - Epoch [1] Batch [60/7250] loss: 5.7501 
2025-12-13 14:42:06,999 - INFO - Epoch [1] Batch [70/7250] loss: 5.9456 
2025-12-13 14:42:07,557 - INFO - Epoch [1] Batch [80/7250] loss: 5.9486 
2025-12-13 14:42:08,106 - INFO - Epoch [1] Batch [90/7250] loss: 5.6255 
2025-12-13 14:42:08,667 - INFO - Epoch [1] Batch [100/7250] loss: 5.5337 
2025-12-13 14:42:09,230 - INFO - Epoch [1] Batch [110/7250] loss: 5.4625 
2025-12-13 14:42:09,787 - INFO - Epoch [1] Batch [120/7250] loss: 5.1778 
2025-12-13 14:42:10,340 - INFO - Epoch [1] Batch [130/7250] loss: 5.4842 
2025-12-13 14:42:10,903 - INFO - Epoch [1] Batch [140/7250] loss: 5.8251 
2025-12-13 14:42:11,452 - INFO - Epoch [1] Batch [150/7250] loss: 5.4571 
2025-12-13 14:42:12,016 - INFO - Epoch [1] Batch [160/7250] loss: 5.4026 
2025-12-13 14:42:12,571 - INFO - Epoch [1] Batch [170/7250] loss: 5.4609 
2025-12-13 14:42:13,130 - INFO - Epoch [1] Batch [180/7250] loss: 5.7368 
2025-12-13 14:42:13,703 - INFO - Epoch [1] Batch [190/7250] loss: 5.3448 
2025-12-13 14:42:14,257 - INFO - Epoch [1] Batch [200/7250] loss: 5.1419 
2025-12-13 14:42:14,817 - INFO - Epoch [1] Batch [210/7250] loss: 5.0701 
2025-12-13 14:42:15,372 - INFO - Epoch [1] Batch [220/7250] loss: 5.5348 
2025-12-13 14:42:15,921 - INFO - Epoch [1] Batch [230/7250] loss: 5.1739 
2025-12-13 14:42:16,497 - INFO - Epoch [1] Batch [240/7250] loss: 5.0488 
2025-12-13 14:42:17,051 - INFO - Epoch [1] Batch [250/7250] loss: 5.6328 
2025-12-13 14:42:17,613 - INFO - Epoch [1] Batch [260/7250] loss: 5.3697 
2025-12-13 14:42:18,168 - INFO - Epoch [1] Batch [270/7250] loss: 6.0649 
2025-12-13 14:42:18,733 - INFO - Epoch [1] Batch [280/7250] loss: 5.1350 
2025-12-13 14:42:19,285 - INFO - Epoch [1] Batch [290/7250] loss: 5.3090 
2025-12-13 14:42:19,829 - INFO - Epoch [1] Batch [300/7250] loss: 5.2611 
2025-12-13 14:42:20,382 - INFO - Epoch [1] Batch [310/7250] loss: 5.1433 
2025-12-13 14:42:20,943 - INFO - Epoch [1] Batch [320/7250] loss: 5.1571 
2025-12-13 14:42:21,491 - INFO - Epoch [1] Batch [330/7250] loss: 5.5601 
2025-12-13 14:42:22,037 - INFO - Epoch [1] Batch [340/7250] loss: 5.5097 
2025-12-13 14:42:22,593 - INFO - Epoch [1] Batch [350/7250] loss: 5.5963 
2025-12-13 14:42:23,152 - INFO - Epoch [1] Batch [360/7250] loss: 5.2759 
2025-12-13 14:42:23,712 - INFO - Epoch [1] Batch [370/7250] loss: 5.2785 
2025-12-13 14:42:24,272 - INFO - Epoch [1] Batch [380/7250] loss: 5.3819 
2025-12-13 14:42:24,825 - INFO - Epoch [1] Batch [390/7250] loss: 5.6103 
2025-12-13 14:42:25,390 - INFO - Epoch [1] Batch [400/7250] loss: 5.1263 
2025-12-13 14:42:25,947 - INFO - Epoch [1] Batch [410/7250] loss: 5.0850 
2025-12-13 14:42:26,498 - INFO - Epoch [1] Batch [420/7250] loss: 5.3092 
2025-12-13 14:42:27,059 - INFO - Epoch [1] Batch [430/7250] loss: 5.3298 
2025-12-13 14:42:27,618 - INFO - Epoch [1] Batch [440/7250] loss: 5.1677 
2025-12-13 14:42:28,179 - INFO - Epoch [1] Batch [450/7250] loss: 5.6510 
2025-12-13 14:42:28,735 - INFO - Epoch [1] Batch [460/7250] loss: 5.2931 
2025-12-13 14:42:29,290 - INFO - Epoch [1] Batch [470/7250] loss: 5.1840 
2025-12-13 14:42:29,844 - INFO - Epoch [1] Batch [480/7250] loss: 5.0370 
2025-12-13 14:42:30,412 - INFO - Epoch [1] Batch [490/7250] loss: 5.4454 
2025-12-13 14:42:30,975 - INFO - Epoch [1] Batch [500/7250] loss: 5.1370 
2025-12-13 14:42:31,529 - INFO - Epoch [1] Batch [510/7250] loss: 5.1619 
2025-12-13 14:42:32,097 - INFO - Epoch [1] Batch [520/7250] loss: 5.1003 
2025-12-13 14:42:32,650 - INFO - Epoch [1] Batch [530/7250] loss: 5.5218 
2025-12-13 14:42:33,194 - INFO - Epoch [1] Batch [540/7250] loss: 5.1662 
2025-12-13 14:42:33,749 - INFO - Epoch [1] Batch [550/7250] loss: 5.0325 
2025-12-13 14:42:34,313 - INFO - Epoch [1] Batch [560/7250] loss: 5.0555 
2025-12-13 14:42:34,880 - INFO - Epoch [1] Batch [570/7250] loss: 4.8872 
2025-12-13 14:42:35,435 - INFO - Epoch [1] Batch [580/7250] loss: 5.5252 
2025-12-13 14:42:35,999 - INFO - Epoch [1] Batch [590/7250] loss: 5.0401 
2025-12-13 14:42:36,553 - INFO - Epoch [1] Batch [600/7250] loss: 5.0765 
2025-12-13 14:42:37,114 - INFO - Epoch [1] Batch [610/7250] loss: 5.0126 
2025-12-13 14:42:37,665 - INFO - Epoch [1] Batch [620/7250] loss: 5.3016 
2025-12-13 14:42:38,220 - INFO - Epoch [1] Batch [630/7250] loss: 4.9623 
2025-12-13 14:42:38,775 - INFO - Epoch [1] Batch [640/7250] loss: 5.5182 
2025-12-13 14:42:39,331 - INFO - Epoch [1] Batch [650/7250] loss: 5.0075 
2025-12-13 14:42:39,902 - INFO - Epoch [1] Batch [660/7250] loss: 5.1615 
2025-12-13 14:42:40,450 - INFO - Epoch [1] Batch [670/7250] loss: 5.2708 
2025-12-13 14:42:41,017 - INFO - Epoch [1] Batch [680/7250] loss: 5.0960 
2025-12-13 14:42:41,577 - INFO - Epoch [1] Batch [690/7250] loss: 4.9775 
2025-12-13 14:42:42,139 - INFO - Epoch [1] Batch [700/7250] loss: 5.0107 
2025-12-13 14:42:42,707 - INFO - Epoch [1] Batch [710/7250] loss: 5.4981 
2025-12-13 14:42:43,270 - INFO - Epoch [1] Batch [720/7250] loss: 5.3557 
2025-12-13 14:42:43,829 - INFO - Epoch [1] Batch [730/7250] loss: 5.2703 
2025-12-13 14:42:44,393 - INFO - Epoch [1] Batch [740/7250] loss: 5.1641 
2025-12-13 14:42:44,941 - INFO - Epoch [1] Batch [750/7250] loss: 4.9438 
2025-12-13 14:42:45,495 - INFO - Epoch [1] Batch [760/7250] loss: 5.3467 
2025-12-13 14:42:46,052 - INFO - Epoch [1] Batch [770/7250] loss: 5.1549 
2025-12-13 14:42:46,604 - INFO - Epoch [1] Batch [780/7250] loss: 5.1313 
2025-12-13 14:42:47,160 - INFO - Epoch [1] Batch [790/7250] loss: 5.0352 
2025-12-13 14:42:47,725 - INFO - Epoch [1] Batch [800/7250] loss: 5.1330 
2025-12-13 14:42:48,281 - INFO - Epoch [1] Batch [810/7250] loss: 5.7082 
2025-12-13 14:42:48,842 - INFO - Epoch [1] Batch [820/7250] loss: 5.1403 
2025-12-13 14:42:49,395 - INFO - Epoch [1] Batch [830/7250] loss: 5.1123 
2025-12-13 14:42:49,954 - INFO - Epoch [1] Batch [840/7250] loss: 5.2734 
2025-12-13 14:42:50,501 - INFO - Epoch [1] Batch [850/7250] loss: 4.8015 
2025-12-13 14:42:51,041 - INFO - Epoch [1] Batch [860/7250] loss: 4.9082 
2025-12-13 14:42:51,479 - INFO - Epoch [1] Batch [870/7250] loss: nan 
2025-12-13 14:42:51,842 - INFO - Epoch [1] Batch [880/7250] loss: 5.0490 
2025-12-13 14:42:52,207 - INFO - Epoch [1] Batch [890/7250] loss: nan 
2025-12-13 14:42:52,576 - INFO - Epoch [1] Batch [900/7250] loss: nan 
2025-12-13 14:42:52,910 - INFO - Epoch [1] Batch [910/7250] loss: nan 
2025-12-13 14:42:53,243 - INFO - Epoch [1] Batch [920/7250] loss: nan 
2025-12-13 14:42:53,569 - INFO - Epoch [1] Batch [930/7250] loss: nan 
2025-12-13 14:42:53,892 - INFO - Epoch [1] Batch [940/7250] loss: nan 
2025-12-13 14:42:54,232 - INFO - Epoch [1] Batch [950/7250] loss: nan 
2025-12-13 14:42:54,562 - INFO - Epoch [1] Batch [960/7250] loss: nan 
2025-12-13 14:42:54,909 - INFO - Epoch [1] Batch [970/7250] loss: nan 
2025-12-13 14:42:55,234 - INFO - Epoch [1] Batch [980/7250] loss: nan 
2025-12-13 14:42:55,571 - INFO - Epoch [1] Batch [990/7250] loss: nan 
2025-12-13 14:42:55,896 - INFO - Epoch [1] Batch [1000/7250] loss: nan 
2025-12-13 14:42:56,228 - INFO - Epoch [1] Batch [1010/7250] loss: nan 
2025-12-13 14:42:56,569 - INFO - Epoch [1] Batch [1020/7250] loss: nan 
2025-12-13 14:42:56,917 - INFO - Epoch [1] Batch [1030/7250] loss: nan 
2025-12-13 14:42:57,249 - INFO - Epoch [1] Batch [1040/7250] loss: nan 
2025-12-13 14:42:57,580 - INFO - Epoch [1] Batch [1050/7250] loss: nan 
2025-12-13 14:42:57,904 - INFO - Epoch [1] Batch [1060/7250] loss: nan 
2025-12-13 14:42:58,242 - INFO - Epoch [1] Batch [1070/7250] loss: nan 
2025-12-13 14:42:58,575 - INFO - Epoch [1] Batch [1080/7250] loss: nan 
2025-12-13 14:42:58,902 - INFO - Epoch [1] Batch [1090/7250] loss: nan 
2025-12-13 14:42:59,243 - INFO - Epoch [1] Batch [1100/7250] loss: nan 
2025-12-13 14:42:59,578 - INFO - Epoch [1] Batch [1110/7250] loss: nan 
2025-12-13 14:42:59,908 - INFO - Epoch [1] Batch [1120/7250] loss: nan 
2025-12-13 14:43:00,247 - INFO - Epoch [1] Batch [1130/7250] loss: nan 
2025-12-13 14:43:00,584 - INFO - Epoch [1] Batch [1140/7250] loss: nan 
2025-12-13 14:43:00,922 - INFO - Epoch [1] Batch [1150/7250] loss: nan 
2025-12-13 14:43:01,257 - INFO - Epoch [1] Batch [1160/7250] loss: nan 
2025-12-13 14:43:01,597 - INFO - Epoch [1] Batch [1170/7250] loss: nan 
2025-12-13 14:43:01,933 - INFO - Epoch [1] Batch [1180/7250] loss: nan 
2025-12-13 14:43:02,263 - INFO - Epoch [1] Batch [1190/7250] loss: nan 
2025-12-13 14:43:02,603 - INFO - Epoch [1] Batch [1200/7250] loss: nan 
2025-12-13 14:43:02,931 - INFO - Epoch [1] Batch [1210/7250] loss: nan 
2025-12-13 14:43:03,262 - INFO - Epoch [1] Batch [1220/7250] loss: nan 
2025-12-13 14:43:03,589 - INFO - Epoch [1] Batch [1230/7250] loss: nan 
2025-12-13 14:43:03,925 - INFO - Epoch [1] Batch [1240/7250] loss: nan 
2025-12-13 14:43:04,250 - INFO - Epoch [1] Batch [1250/7250] loss: nan 
2025-12-13 14:43:04,596 - INFO - Epoch [1] Batch [1260/7250] loss: nan 
2025-12-13 14:43:04,918 - INFO - Epoch [1] Batch [1270/7250] loss: nan 
2025-12-13 14:43:05,254 - INFO - Epoch [1] Batch [1280/7250] loss: nan 
2025-12-13 14:43:05,583 - INFO - Epoch [1] Batch [1290/7250] loss: nan 
2025-12-13 14:43:05,900 - INFO - Epoch [1] Batch [1300/7250] loss: nan 
2025-12-13 14:43:06,259 - INFO - Epoch [1] Batch [1310/7250] loss: nan 
2025-12-13 14:43:06,579 - INFO - Epoch [1] Batch [1320/7250] loss: nan 
2025-12-13 14:43:06,921 - INFO - Epoch [1] Batch [1330/7250] loss: nan 
2025-12-13 14:43:07,252 - INFO - Epoch [1] Batch [1340/7250] loss: nan 
2025-12-13 14:43:07,606 - INFO - Epoch [1] Batch [1350/7250] loss: nan 
2025-12-13 14:43:07,944 - INFO - Epoch [1] Batch [1360/7250] loss: nan 
2025-12-13 14:43:08,307 - INFO - Epoch [1] Batch [1370/7250] loss: nan 
2025-12-13 14:43:08,634 - INFO - Epoch [1] Batch [1380/7250] loss: nan 
2025-12-13 14:43:08,973 - INFO - Epoch [1] Batch [1390/7250] loss: nan 
2025-12-13 14:43:09,305 - INFO - Epoch [1] Batch [1400/7250] loss: nan 
2025-12-13 14:43:09,636 - INFO - Epoch [1] Batch [1410/7250] loss: nan 
2025-12-13 14:43:09,968 - INFO - Epoch [1] Batch [1420/7250] loss: nan 
2025-12-13 14:43:10,299 - INFO - Epoch [1] Batch [1430/7250] loss: nan 
2025-12-13 14:43:10,633 - INFO - Epoch [1] Batch [1440/7250] loss: nan 
2025-12-13 14:43:10,973 - INFO - Epoch [1] Batch [1450/7250] loss: nan 
2025-12-13 14:43:11,305 - INFO - Epoch [1] Batch [1460/7250] loss: nan 
2025-12-13 14:43:11,631 - INFO - Epoch [1] Batch [1470/7250] loss: nan 
2025-12-13 14:43:11,990 - INFO - Epoch [1] Batch [1480/7250] loss: nan 
2025-12-13 14:43:12,317 - INFO - Epoch [1] Batch [1490/7250] loss: nan 
2025-12-13 14:43:12,648 - INFO - Epoch [1] Batch [1500/7250] loss: nan 
2025-12-13 14:43:12,970 - INFO - Epoch [1] Batch [1510/7250] loss: nan 
2025-12-13 14:43:13,297 - INFO - Epoch [1] Batch [1520/7250] loss: nan 
2025-12-13 14:43:13,628 - INFO - Epoch [1] Batch [1530/7250] loss: nan 
2025-12-13 14:43:13,972 - INFO - Epoch [1] Batch [1540/7250] loss: nan 
2025-12-13 14:43:14,301 - INFO - Epoch [1] Batch [1550/7250] loss: nan 
2025-12-13 14:43:14,636 - INFO - Epoch [1] Batch [1560/7250] loss: nan 
2025-12-13 14:43:14,957 - INFO - Epoch [1] Batch [1570/7250] loss: nan 
2025-12-13 14:43:15,297 - INFO - Epoch [1] Batch [1580/7250] loss: nan 
2025-12-13 14:43:15,625 - INFO - Epoch [1] Batch [1590/7250] loss: nan 
2025-12-13 14:43:15,963 - INFO - Epoch [1] Batch [1600/7250] loss: nan 
2025-12-13 14:43:16,285 - INFO - Epoch [1] Batch [1610/7250] loss: nan 
2025-12-13 14:43:16,610 - INFO - Epoch [1] Batch [1620/7250] loss: nan 
2025-12-13 14:43:16,940 - INFO - Epoch [1] Batch [1630/7250] loss: nan 
2025-12-13 14:43:17,257 - INFO - Epoch [1] Batch [1640/7250] loss: nan 
2025-12-13 14:43:17,601 - INFO - Epoch [1] Batch [1650/7250] loss: nan 
2025-12-13 14:43:17,937 - INFO - Epoch [1] Batch [1660/7250] loss: nan 
2025-12-13 14:43:18,261 - INFO - Epoch [1] Batch [1670/7250] loss: nan 
2025-12-13 14:43:18,608 - INFO - Epoch [1] Batch [1680/7250] loss: nan 
2025-12-13 14:43:18,948 - INFO - Epoch [1] Batch [1690/7250] loss: nan 
2025-12-13 14:43:19,277 - INFO - Epoch [1] Batch [1700/7250] loss: nan 
2025-12-13 14:43:19,617 - INFO - Epoch [1] Batch [1710/7250] loss: nan 
2025-12-13 14:43:19,943 - INFO - Epoch [1] Batch [1720/7250] loss: nan 
2025-12-13 14:43:20,264 - INFO - Epoch [1] Batch [1730/7250] loss: nan 
2025-12-13 14:43:20,598 - INFO - Epoch [1] Batch [1740/7250] loss: nan 
2025-12-13 14:43:20,981 - INFO - Epoch [1] Batch [1750/7250] loss: nan 
2025-12-13 14:43:21,332 - INFO - Epoch [1] Batch [1760/7250] loss: nan 
2025-12-13 14:43:21,692 - INFO - Epoch [1] Batch [1770/7250] loss: nan 
2025-12-13 14:43:22,062 - INFO - Epoch [1] Batch [1780/7250] loss: nan 
2025-12-13 14:43:22,409 - INFO - Epoch [1] Batch [1790/7250] loss: nan 
2025-12-13 14:43:22,749 - INFO - Epoch [1] Batch [1800/7250] loss: nan 
2025-12-13 14:43:23,097 - INFO - Epoch [1] Batch [1810/7250] loss: nan 
2025-12-13 14:43:23,455 - INFO - Epoch [1] Batch [1820/7250] loss: nan 
2025-12-13 14:43:23,819 - INFO - Epoch [1] Batch [1830/7250] loss: nan 
2025-12-13 14:43:24,204 - INFO - Epoch [1] Batch [1840/7250] loss: nan 
2025-12-13 14:43:24,583 - INFO - Epoch [1] Batch [1850/7250] loss: nan 
2025-12-13 14:43:24,951 - INFO - Epoch [1] Batch [1860/7250] loss: nan 
2025-12-13 14:43:25,333 - INFO - Epoch [1] Batch [1870/7250] loss: nan 
2025-12-13 14:43:25,695 - INFO - Epoch [1] Batch [1880/7250] loss: nan 
2025-12-13 14:43:26,069 - INFO - Epoch [1] Batch [1890/7250] loss: nan 
2025-12-13 14:43:26,429 - INFO - Epoch [1] Batch [1900/7250] loss: nan 
2025-12-13 14:43:26,810 - INFO - Epoch [1] Batch [1910/7250] loss: nan 
2025-12-13 14:43:27,166 - INFO - Epoch [1] Batch [1920/7250] loss: nan 
2025-12-13 14:43:27,530 - INFO - Epoch [1] Batch [1930/7250] loss: nan 
2025-12-13 14:43:27,883 - INFO - Epoch [1] Batch [1940/7250] loss: nan 
2025-12-13 14:43:28,225 - INFO - Epoch [1] Batch [1950/7250] loss: nan 
2025-12-13 14:43:28,575 - INFO - Epoch [1] Batch [1960/7250] loss: nan 
2025-12-13 14:43:28,923 - INFO - Epoch [1] Batch [1970/7250] loss: nan 
2025-12-13 14:43:29,268 - INFO - Epoch [1] Batch [1980/7250] loss: nan 
2025-12-13 14:43:29,618 - INFO - Epoch [1] Batch [1990/7250] loss: nan 
2025-12-13 14:43:29,980 - INFO - Epoch [1] Batch [2000/7250] loss: nan 
2025-12-13 14:43:30,347 - INFO - Epoch [1] Batch [2010/7250] loss: nan 
2025-12-13 14:43:30,701 - INFO - Epoch [1] Batch [2020/7250] loss: nan 
2025-12-13 14:43:31,048 - INFO - Epoch [1] Batch [2030/7250] loss: nan 
2025-12-13 14:43:31,424 - INFO - Epoch [1] Batch [2040/7250] loss: nan 
2025-12-13 14:43:31,795 - INFO - Epoch [1] Batch [2050/7250] loss: nan 
2025-12-13 14:43:32,160 - INFO - Epoch [1] Batch [2060/7250] loss: nan 
2025-12-13 14:43:32,538 - INFO - Epoch [1] Batch [2070/7250] loss: nan 
2025-12-13 14:43:32,920 - INFO - Epoch [1] Batch [2080/7250] loss: nan 
2025-12-13 14:43:33,318 - INFO - Epoch [1] Batch [2090/7250] loss: nan 
2025-12-13 14:43:33,696 - INFO - Epoch [1] Batch [2100/7250] loss: nan 
2025-12-13 14:43:34,075 - INFO - Epoch [1] Batch [2110/7250] loss: nan 
2025-12-13 14:43:34,440 - INFO - Epoch [1] Batch [2120/7250] loss: nan 
2025-12-13 14:43:34,814 - INFO - Epoch [1] Batch [2130/7250] loss: nan 
2025-12-13 14:43:35,157 - INFO - Epoch [1] Batch [2140/7250] loss: nan 
2025-12-13 14:43:35,512 - INFO - Epoch [1] Batch [2150/7250] loss: nan 
2025-12-13 14:43:35,867 - INFO - Epoch [1] Batch [2160/7250] loss: nan 
2025-12-13 14:43:36,199 - INFO - Epoch [1] Batch [2170/7250] loss: nan 
2025-12-13 14:43:36,551 - INFO - Epoch [1] Batch [2180/7250] loss: nan 
2025-12-13 14:43:36,910 - INFO - Epoch [1] Batch [2190/7250] loss: nan 
2025-12-13 14:43:37,249 - INFO - Epoch [1] Batch [2200/7250] loss: nan 
2025-12-13 14:43:37,598 - INFO - Epoch [1] Batch [2210/7250] loss: nan 
2025-12-13 14:43:37,970 - INFO - Epoch [1] Batch [2220/7250] loss: nan 
2025-12-13 14:43:38,328 - INFO - Epoch [1] Batch [2230/7250] loss: nan 
2025-12-13 14:43:38,688 - INFO - Epoch [1] Batch [2240/7250] loss: nan 
2025-12-13 14:43:39,029 - INFO - Epoch [1] Batch [2250/7250] loss: nan 
2025-12-13 14:43:39,386 - INFO - Epoch [1] Batch [2260/7250] loss: nan 
2025-12-13 14:43:39,740 - INFO - Epoch [1] Batch [2270/7250] loss: nan 
2025-12-13 14:43:40,071 - INFO - Epoch [1] Batch [2280/7250] loss: nan 
2025-12-13 14:43:40,410 - INFO - Epoch [1] Batch [2290/7250] loss: nan 
2025-12-13 14:43:40,752 - INFO - Epoch [1] Batch [2300/7250] loss: nan 
2025-12-13 14:43:41,100 - INFO - Epoch [1] Batch [2310/7250] loss: nan 
2025-12-13 14:43:41,449 - INFO - Epoch [1] Batch [2320/7250] loss: nan 
2025-12-13 14:43:41,789 - INFO - Epoch [1] Batch [2330/7250] loss: nan 
2025-12-13 14:43:42,136 - INFO - Epoch [1] Batch [2340/7250] loss: nan 
2025-12-13 14:43:42,490 - INFO - Epoch [1] Batch [2350/7250] loss: nan 
2025-12-13 14:43:42,825 - INFO - Epoch [1] Batch [2360/7250] loss: nan 
2025-12-13 14:43:43,166 - INFO - Epoch [1] Batch [2370/7250] loss: nan 
2025-12-13 14:43:43,528 - INFO - Epoch [1] Batch [2380/7250] loss: nan 
2025-12-13 14:43:43,888 - INFO - Epoch [1] Batch [2390/7250] loss: nan 
2025-12-13 14:43:44,256 - INFO - Epoch [1] Batch [2400/7250] loss: nan 
2025-12-13 14:43:44,633 - INFO - Epoch [1] Batch [2410/7250] loss: nan 
2025-12-13 14:43:45,028 - INFO - Epoch [1] Batch [2420/7250] loss: nan 
2025-12-13 14:43:45,390 - INFO - Epoch [1] Batch [2430/7250] loss: nan 
2025-12-13 14:43:45,751 - INFO - Epoch [1] Batch [2440/7250] loss: nan 
2025-12-13 14:43:46,111 - INFO - Epoch [1] Batch [2450/7250] loss: nan 
2025-12-13 14:43:46,459 - INFO - Epoch [1] Batch [2460/7250] loss: nan 
2025-12-13 14:43:46,805 - INFO - Epoch [1] Batch [2470/7250] loss: nan 
2025-12-13 14:43:47,155 - INFO - Epoch [1] Batch [2480/7250] loss: nan 
2025-12-13 14:43:47,524 - INFO - Epoch [1] Batch [2490/7250] loss: nan 
2025-12-13 14:43:47,873 - INFO - Epoch [1] Batch [2500/7250] loss: nan 
2025-12-13 14:43:48,230 - INFO - Epoch [1] Batch [2510/7250] loss: nan 
2025-12-13 14:43:48,585 - INFO - Epoch [1] Batch [2520/7250] loss: nan 
2025-12-13 14:43:48,947 - INFO - Epoch [1] Batch [2530/7250] loss: nan 
2025-12-13 14:43:49,305 - INFO - Epoch [1] Batch [2540/7250] loss: nan 
2025-12-13 14:43:49,674 - INFO - Epoch [1] Batch [2550/7250] loss: nan 
2025-12-13 14:43:50,057 - INFO - Epoch [1] Batch [2560/7250] loss: nan 
2025-12-13 14:43:50,424 - INFO - Epoch [1] Batch [2570/7250] loss: nan 
2025-12-13 14:43:50,798 - INFO - Epoch [1] Batch [2580/7250] loss: nan 
2025-12-13 14:43:51,167 - INFO - Epoch [1] Batch [2590/7250] loss: nan 
2025-12-13 14:43:51,555 - INFO - Epoch [1] Batch [2600/7250] loss: nan 
2025-12-13 14:43:51,930 - INFO - Epoch [1] Batch [2610/7250] loss: nan 
2025-12-13 14:43:52,278 - INFO - Epoch [1] Batch [2620/7250] loss: nan 
2025-12-13 14:43:52,645 - INFO - Epoch [1] Batch [2630/7250] loss: nan 
2025-12-13 14:43:52,993 - INFO - Epoch [1] Batch [2640/7250] loss: nan 
2025-12-13 14:43:53,342 - INFO - Epoch [1] Batch [2650/7250] loss: nan 
2025-12-13 14:43:53,708 - INFO - Epoch [1] Batch [2660/7250] loss: nan 
2025-12-13 14:43:54,064 - INFO - Epoch [1] Batch [2670/7250] loss: nan 
2025-12-13 14:43:54,421 - INFO - Epoch [1] Batch [2680/7250] loss: nan 
2025-12-13 14:43:54,769 - INFO - Epoch [1] Batch [2690/7250] loss: nan 
2025-12-13 14:43:55,152 - INFO - Epoch [1] Batch [2700/7250] loss: nan 
2025-12-13 14:43:55,509 - INFO - Epoch [1] Batch [2710/7250] loss: nan 
2025-12-13 14:43:55,852 - INFO - Epoch [1] Batch [2720/7250] loss: nan 
2025-12-13 14:43:56,193 - INFO - Epoch [1] Batch [2730/7250] loss: nan 
2025-12-13 14:43:56,539 - INFO - Epoch [1] Batch [2740/7250] loss: nan 
2025-12-13 14:43:56,885 - INFO - Epoch [1] Batch [2750/7250] loss: nan 
2025-12-13 14:43:57,236 - INFO - Epoch [1] Batch [2760/7250] loss: nan 
2025-12-13 14:43:57,569 - INFO - Epoch [1] Batch [2770/7250] loss: nan 
2025-12-13 14:43:57,910 - INFO - Epoch [1] Batch [2780/7250] loss: nan 
2025-12-13 14:43:58,245 - INFO - Epoch [1] Batch [2790/7250] loss: nan 
2025-12-13 14:43:58,596 - INFO - Epoch [1] Batch [2800/7250] loss: nan 
2025-12-13 14:43:58,935 - INFO - Epoch [1] Batch [2810/7250] loss: nan 
2025-12-13 14:43:59,291 - INFO - Epoch [1] Batch [2820/7250] loss: nan 
2025-12-13 14:43:59,643 - INFO - Epoch [1] Batch [2830/7250] loss: nan 
2025-12-13 14:43:59,979 - INFO - Epoch [1] Batch [2840/7250] loss: nan 
2025-12-13 14:44:00,323 - INFO - Epoch [1] Batch [2850/7250] loss: nan 
2025-12-13 14:44:00,668 - INFO - Epoch [1] Batch [2860/7250] loss: nan 
2025-12-13 14:44:01,013 - INFO - Epoch [1] Batch [2870/7250] loss: nan 
2025-12-13 14:44:01,381 - INFO - Epoch [1] Batch [2880/7250] loss: nan 
2025-12-13 14:44:01,718 - INFO - Epoch [1] Batch [2890/7250] loss: nan 
2025-12-13 14:44:02,061 - INFO - Epoch [1] Batch [2900/7250] loss: nan 
2025-12-13 14:44:02,402 - INFO - Epoch [1] Batch [2910/7250] loss: nan 
2025-12-13 14:44:02,749 - INFO - Epoch [1] Batch [2920/7250] loss: nan 
2025-12-13 14:44:03,087 - INFO - Epoch [1] Batch [2930/7250] loss: nan 
2025-12-13 14:44:03,438 - INFO - Epoch [1] Batch [2940/7250] loss: nan 
2025-12-13 14:44:03,784 - INFO - Epoch [1] Batch [2950/7250] loss: nan 
2025-12-13 14:44:04,130 - INFO - Epoch [1] Batch [2960/7250] loss: nan 
2025-12-13 14:44:04,485 - INFO - Epoch [1] Batch [2970/7250] loss: nan 
2025-12-13 14:44:04,839 - INFO - Epoch [1] Batch [2980/7250] loss: nan 
2025-12-13 14:44:05,176 - INFO - Epoch [1] Batch [2990/7250] loss: nan 
2025-12-13 14:44:05,511 - INFO - Epoch [1] Batch [3000/7250] loss: nan 
2025-12-13 14:44:05,858 - INFO - Epoch [1] Batch [3010/7250] loss: nan 
2025-12-13 14:44:06,207 - INFO - Epoch [1] Batch [3020/7250] loss: nan 
2025-12-13 14:44:06,556 - INFO - Epoch [1] Batch [3030/7250] loss: nan 
2025-12-13 14:44:06,898 - INFO - Epoch [1] Batch [3040/7250] loss: nan 
2025-12-13 14:44:07,250 - INFO - Epoch [1] Batch [3050/7250] loss: nan 
2025-12-13 14:44:07,590 - INFO - Epoch [1] Batch [3060/7250] loss: nan 
2025-12-13 14:44:07,939 - INFO - Epoch [1] Batch [3070/7250] loss: nan 
2025-12-13 14:44:08,292 - INFO - Epoch [1] Batch [3080/7250] loss: nan 
2025-12-13 14:44:08,636 - INFO - Epoch [1] Batch [3090/7250] loss: nan 
2025-12-13 14:44:08,985 - INFO - Epoch [1] Batch [3100/7250] loss: nan 
2025-12-13 14:44:09,325 - INFO - Epoch [1] Batch [3110/7250] loss: nan 
2025-12-13 14:44:09,677 - INFO - Epoch [1] Batch [3120/7250] loss: nan 
2025-12-13 14:44:10,015 - INFO - Epoch [1] Batch [3130/7250] loss: nan 
2025-12-13 14:44:10,361 - INFO - Epoch [1] Batch [3140/7250] loss: nan 
2025-12-13 14:44:10,700 - INFO - Epoch [1] Batch [3150/7250] loss: nan 
2025-12-13 14:44:11,028 - INFO - Epoch [1] Batch [3160/7250] loss: nan 
2025-12-13 14:44:11,370 - INFO - Epoch [1] Batch [3170/7250] loss: nan 
2025-12-13 14:44:11,725 - INFO - Epoch [1] Batch [3180/7250] loss: nan 
2025-12-13 14:44:12,071 - INFO - Epoch [1] Batch [3190/7250] loss: nan 
2025-12-13 14:44:12,420 - INFO - Epoch [1] Batch [3200/7250] loss: nan 
2025-12-13 14:44:12,758 - INFO - Epoch [1] Batch [3210/7250] loss: nan 
2025-12-13 14:44:13,108 - INFO - Epoch [1] Batch [3220/7250] loss: nan 
2025-12-13 14:44:13,456 - INFO - Epoch [1] Batch [3230/7250] loss: nan 
2025-12-13 14:44:13,792 - INFO - Epoch [1] Batch [3240/7250] loss: nan 
2025-12-13 14:44:14,138 - INFO - Epoch [1] Batch [3250/7250] loss: nan 
2025-12-13 14:44:14,483 - INFO - Epoch [1] Batch [3260/7250] loss: nan 
2025-12-13 14:44:14,819 - INFO - Epoch [1] Batch [3270/7250] loss: nan 
2025-12-13 14:44:15,167 - INFO - Epoch [1] Batch [3280/7250] loss: nan 
2025-12-13 14:44:15,505 - INFO - Epoch [1] Batch [3290/7250] loss: nan 
2025-12-13 14:44:15,844 - INFO - Epoch [1] Batch [3300/7250] loss: nan 
2025-12-13 14:44:16,190 - INFO - Epoch [1] Batch [3310/7250] loss: nan 
2025-12-13 14:44:16,542 - INFO - Epoch [1] Batch [3320/7250] loss: nan 
2025-12-13 14:44:16,884 - INFO - Epoch [1] Batch [3330/7250] loss: nan 
2025-12-13 14:44:17,225 - INFO - Epoch [1] Batch [3340/7250] loss: nan 
2025-12-13 14:44:17,568 - INFO - Epoch [1] Batch [3350/7250] loss: nan 
2025-12-13 14:44:17,916 - INFO - Epoch [1] Batch [3360/7250] loss: nan 
2025-12-13 14:44:18,245 - INFO - Epoch [1] Batch [3370/7250] loss: nan 
2025-12-13 14:44:18,584 - INFO - Epoch [1] Batch [3380/7250] loss: nan 
2025-12-13 14:44:18,929 - INFO - Epoch [1] Batch [3390/7250] loss: nan 
2025-12-13 14:44:19,287 - INFO - Epoch [1] Batch [3400/7250] loss: nan 
2025-12-13 14:44:19,648 - INFO - Epoch [1] Batch [3410/7250] loss: nan 
2025-12-13 14:44:19,984 - INFO - Epoch [1] Batch [3420/7250] loss: nan 
2025-12-13 14:44:20,331 - INFO - Epoch [1] Batch [3430/7250] loss: nan 
2025-12-13 14:44:20,665 - INFO - Epoch [1] Batch [3440/7250] loss: nan 
2025-12-13 14:44:21,050 - INFO - Epoch [1] Batch [3450/7250] loss: nan 
2025-12-13 14:44:21,383 - INFO - Epoch [1] Batch [3460/7250] loss: nan 
2025-12-13 14:44:21,743 - INFO - Epoch [1] Batch [3470/7250] loss: nan 
2025-12-13 14:44:22,081 - INFO - Epoch [1] Batch [3480/7250] loss: nan 
2025-12-13 14:44:22,423 - INFO - Epoch [1] Batch [3490/7250] loss: nan 
2025-12-13 14:44:22,774 - INFO - Epoch [1] Batch [3500/7250] loss: nan 
2025-12-13 14:44:23,118 - INFO - Epoch [1] Batch [3510/7250] loss: nan 
2025-12-13 14:44:23,461 - INFO - Epoch [1] Batch [3520/7250] loss: nan 
2025-12-13 14:44:23,798 - INFO - Epoch [1] Batch [3530/7250] loss: nan 
2025-12-13 14:44:24,157 - INFO - Epoch [1] Batch [3540/7250] loss: nan 
2025-12-13 14:44:24,499 - INFO - Epoch [1] Batch [3550/7250] loss: nan 
2025-12-13 14:44:24,861 - INFO - Epoch [1] Batch [3560/7250] loss: nan 
2025-12-13 14:44:25,197 - INFO - Epoch [1] Batch [3570/7250] loss: nan 
2025-12-13 14:44:25,548 - INFO - Epoch [1] Batch [3580/7250] loss: nan 
2025-12-13 14:44:25,893 - INFO - Epoch [1] Batch [3590/7250] loss: nan 
2025-12-13 14:44:26,241 - INFO - Epoch [1] Batch [3600/7250] loss: nan 
2025-12-13 14:44:26,594 - INFO - Epoch [1] Batch [3610/7250] loss: nan 
2025-12-13 14:44:26,931 - INFO - Epoch [1] Batch [3620/7250] loss: nan 
2025-12-13 14:44:27,274 - INFO - Epoch [1] Batch [3630/7250] loss: nan 
2025-12-13 14:44:27,615 - INFO - Epoch [1] Batch [3640/7250] loss: nan 
2025-12-13 14:44:27,956 - INFO - Epoch [1] Batch [3650/7250] loss: nan 
2025-12-13 14:44:28,307 - INFO - Epoch [1] Batch [3660/7250] loss: nan 
2025-12-13 14:44:28,653 - INFO - Epoch [1] Batch [3670/7250] loss: nan 
2025-12-13 14:44:29,008 - INFO - Epoch [1] Batch [3680/7250] loss: nan 
2025-12-13 14:44:29,361 - INFO - Epoch [1] Batch [3690/7250] loss: nan 
2025-12-13 14:44:29,710 - INFO - Epoch [1] Batch [3700/7250] loss: nan 
2025-12-13 14:44:30,056 - INFO - Epoch [1] Batch [3710/7250] loss: nan 
2025-12-13 14:44:30,395 - INFO - Epoch [1] Batch [3720/7250] loss: nan 
2025-12-13 14:44:30,743 - INFO - Epoch [1] Batch [3730/7250] loss: nan 
2025-12-13 14:44:31,092 - INFO - Epoch [1] Batch [3740/7250] loss: nan 
2025-12-13 14:44:31,435 - INFO - Epoch [1] Batch [3750/7250] loss: nan 
2025-12-13 14:44:31,777 - INFO - Epoch [1] Batch [3760/7250] loss: nan 
2025-12-13 14:44:32,132 - INFO - Epoch [1] Batch [3770/7250] loss: nan 
2025-12-13 14:44:32,474 - INFO - Epoch [1] Batch [3780/7250] loss: nan 
2025-12-13 14:44:32,820 - INFO - Epoch [1] Batch [3790/7250] loss: nan 
2025-12-13 14:44:33,167 - INFO - Epoch [1] Batch [3800/7250] loss: nan 
2025-12-13 14:44:33,509 - INFO - Epoch [1] Batch [3810/7250] loss: nan 
2025-12-13 14:44:33,844 - INFO - Epoch [1] Batch [3820/7250] loss: nan 
2025-12-13 14:44:34,192 - INFO - Epoch [1] Batch [3830/7250] loss: nan 
2025-12-13 14:44:34,531 - INFO - Epoch [1] Batch [3840/7250] loss: nan 
2025-12-13 14:44:34,866 - INFO - Epoch [1] Batch [3850/7250] loss: nan 
2025-12-13 14:44:35,234 - INFO - Epoch [1] Batch [3860/7250] loss: nan 
2025-12-13 14:44:35,586 - INFO - Epoch [1] Batch [3870/7250] loss: nan 
2025-12-13 14:44:35,937 - INFO - Epoch [1] Batch [3880/7250] loss: nan 
2025-12-13 14:44:36,267 - INFO - Epoch [1] Batch [3890/7250] loss: nan 
2025-12-13 14:44:36,619 - INFO - Epoch [1] Batch [3900/7250] loss: nan 
2025-12-13 14:44:36,959 - INFO - Epoch [1] Batch [3910/7250] loss: nan 
2025-12-13 14:44:37,297 - INFO - Epoch [1] Batch [3920/7250] loss: nan 
2025-12-13 14:44:37,641 - INFO - Epoch [1] Batch [3930/7250] loss: nan 
2025-12-13 14:44:37,983 - INFO - Epoch [1] Batch [3940/7250] loss: nan 
2025-12-13 14:44:38,314 - INFO - Epoch [1] Batch [3950/7250] loss: nan 
2025-12-13 14:44:38,656 - INFO - Epoch [1] Batch [3960/7250] loss: nan 
2025-12-13 14:44:39,002 - INFO - Epoch [1] Batch [3970/7250] loss: nan 
2025-12-13 14:44:39,337 - INFO - Epoch [1] Batch [3980/7250] loss: nan 
2025-12-13 14:44:39,686 - INFO - Epoch [1] Batch [3990/7250] loss: nan 
2025-12-13 14:44:40,034 - INFO - Epoch [1] Batch [4000/7250] loss: nan 
2025-12-13 14:44:40,378 - INFO - Epoch [1] Batch [4010/7250] loss: nan 
2025-12-13 14:44:40,717 - INFO - Epoch [1] Batch [4020/7250] loss: nan 
2025-12-13 14:44:41,052 - INFO - Epoch [1] Batch [4030/7250] loss: nan 
2025-12-13 14:44:41,386 - INFO - Epoch [1] Batch [4040/7250] loss: nan 
2025-12-13 14:44:41,724 - INFO - Epoch [1] Batch [4050/7250] loss: nan 
2025-12-13 14:44:42,075 - INFO - Epoch [1] Batch [4060/7250] loss: nan 
2025-12-13 14:44:42,442 - INFO - Epoch [1] Batch [4070/7250] loss: nan 
2025-12-13 14:44:42,786 - INFO - Epoch [1] Batch [4080/7250] loss: nan 
2025-12-13 14:44:43,138 - INFO - Epoch [1] Batch [4090/7250] loss: nan 
2025-12-13 14:44:43,496 - INFO - Epoch [1] Batch [4100/7250] loss: nan 
2025-12-13 14:44:43,842 - INFO - Epoch [1] Batch [4110/7250] loss: nan 
2025-12-13 14:44:44,182 - INFO - Epoch [1] Batch [4120/7250] loss: nan 
2025-12-13 14:44:44,527 - INFO - Epoch [1] Batch [4130/7250] loss: nan 
2025-12-13 14:44:44,870 - INFO - Epoch [1] Batch [4140/7250] loss: nan 
2025-12-13 14:44:45,222 - INFO - Epoch [1] Batch [4150/7250] loss: nan 
2025-12-13 14:44:45,553 - INFO - Epoch [1] Batch [4160/7250] loss: nan 
2025-12-13 14:44:45,910 - INFO - Epoch [1] Batch [4170/7250] loss: nan 
2025-12-13 14:44:46,252 - INFO - Epoch [1] Batch [4180/7250] loss: nan 
2025-12-13 14:44:46,596 - INFO - Epoch [1] Batch [4190/7250] loss: nan 
2025-12-13 14:44:46,955 - INFO - Epoch [1] Batch [4200/7250] loss: nan 
2025-12-13 14:44:47,297 - INFO - Epoch [1] Batch [4210/7250] loss: nan 
2025-12-13 14:44:47,629 - INFO - Epoch [1] Batch [4220/7250] loss: nan 
2025-12-13 14:44:47,966 - INFO - Epoch [1] Batch [4230/7250] loss: nan 
2025-12-13 14:44:48,306 - INFO - Epoch [1] Batch [4240/7250] loss: nan 
2025-12-13 14:44:48,647 - INFO - Epoch [1] Batch [4250/7250] loss: nan 
2025-12-13 14:44:49,000 - INFO - Epoch [1] Batch [4260/7250] loss: nan 
2025-12-13 14:44:49,359 - INFO - Epoch [1] Batch [4270/7250] loss: nan 
2025-12-13 14:44:49,701 - INFO - Epoch [1] Batch [4280/7250] loss: nan 
2025-12-13 14:44:50,039 - INFO - Epoch [1] Batch [4290/7250] loss: nan 
2025-12-13 14:44:50,390 - INFO - Epoch [1] Batch [4300/7250] loss: nan 
2025-12-13 14:44:50,766 - INFO - Epoch [1] Batch [4310/7250] loss: nan 
2025-12-13 14:44:51,103 - INFO - Epoch [1] Batch [4320/7250] loss: nan 
2025-12-13 14:44:51,457 - INFO - Epoch [1] Batch [4330/7250] loss: nan 
2025-12-13 14:44:51,796 - INFO - Epoch [1] Batch [4340/7250] loss: nan 
2025-12-13 14:44:52,131 - INFO - Epoch [1] Batch [4350/7250] loss: nan 
2025-12-13 14:44:52,471 - INFO - Epoch [1] Batch [4360/7250] loss: nan 
2025-12-13 14:44:52,805 - INFO - Epoch [1] Batch [4370/7250] loss: nan 
2025-12-13 14:44:53,151 - INFO - Epoch [1] Batch [4380/7250] loss: nan 
2025-12-13 14:44:53,500 - INFO - Epoch [1] Batch [4390/7250] loss: nan 
2025-12-13 14:44:53,855 - INFO - Epoch [1] Batch [4400/7250] loss: nan 
2025-12-13 14:44:54,202 - INFO - Epoch [1] Batch [4410/7250] loss: nan 
2025-12-13 14:44:54,548 - INFO - Epoch [1] Batch [4420/7250] loss: nan 
2025-12-13 14:44:54,887 - INFO - Epoch [1] Batch [4430/7250] loss: nan 
2025-12-13 14:44:55,228 - INFO - Epoch [1] Batch [4440/7250] loss: nan 
2025-12-13 14:44:55,572 - INFO - Epoch [1] Batch [4450/7250] loss: nan 
2025-12-13 14:44:55,918 - INFO - Epoch [1] Batch [4460/7250] loss: nan 
2025-12-13 14:44:56,268 - INFO - Epoch [1] Batch [4470/7250] loss: nan 
2025-12-13 14:44:56,607 - INFO - Epoch [1] Batch [4480/7250] loss: nan 
2025-12-13 14:44:56,948 - INFO - Epoch [1] Batch [4490/7250] loss: nan 
2025-12-13 14:44:57,283 - INFO - Epoch [1] Batch [4500/7250] loss: nan 
2025-12-13 14:44:57,634 - INFO - Epoch [1] Batch [4510/7250] loss: nan 
2025-12-13 14:44:57,979 - INFO - Epoch [1] Batch [4520/7250] loss: nan 
2025-12-13 14:44:58,320 - INFO - Epoch [1] Batch [4530/7250] loss: nan 
2025-12-13 14:44:58,670 - INFO - Epoch [1] Batch [4540/7250] loss: nan 
2025-12-13 14:44:59,015 - INFO - Epoch [1] Batch [4550/7250] loss: nan 
2025-12-13 14:44:59,353 - INFO - Epoch [1] Batch [4560/7250] loss: nan 
2025-12-13 14:44:59,691 - INFO - Epoch [1] Batch [4570/7250] loss: nan 
2025-12-13 14:45:00,028 - INFO - Epoch [1] Batch [4580/7250] loss: nan 
2025-12-13 14:45:00,383 - INFO - Epoch [1] Batch [4590/7250] loss: nan 
2025-12-13 14:45:00,722 - INFO - Epoch [1] Batch [4600/7250] loss: nan 
2025-12-13 14:45:01,087 - INFO - Epoch [1] Batch [4610/7250] loss: nan 
2025-12-13 14:45:01,435 - INFO - Epoch [1] Batch [4620/7250] loss: nan 
2025-12-13 14:45:01,790 - INFO - Epoch [1] Batch [4630/7250] loss: nan 
2025-12-13 14:45:02,124 - INFO - Epoch [1] Batch [4640/7250] loss: nan 
2025-12-13 14:45:02,457 - INFO - Epoch [1] Batch [4650/7250] loss: nan 
2025-12-13 14:45:02,811 - INFO - Epoch [1] Batch [4660/7250] loss: nan 
2025-12-13 14:45:03,162 - INFO - Epoch [1] Batch [4670/7250] loss: nan 
2025-12-13 14:45:03,504 - INFO - Epoch [1] Batch [4680/7250] loss: nan 
2025-12-13 14:45:03,858 - INFO - Epoch [1] Batch [4690/7250] loss: nan 
2025-12-13 14:45:04,231 - INFO - Epoch [1] Batch [4700/7250] loss: nan 
2025-12-13 14:45:04,565 - INFO - Epoch [1] Batch [4710/7250] loss: nan 
2025-12-13 14:45:04,908 - INFO - Epoch [1] Batch [4720/7250] loss: nan 
2025-12-13 14:45:05,245 - INFO - Epoch [1] Batch [4730/7250] loss: nan 
2025-12-13 14:45:05,578 - INFO - Epoch [1] Batch [4740/7250] loss: nan 
2025-12-13 14:45:05,929 - INFO - Epoch [1] Batch [4750/7250] loss: nan 
2025-12-13 14:45:06,269 - INFO - Epoch [1] Batch [4760/7250] loss: nan 
2025-12-13 14:45:06,614 - INFO - Epoch [1] Batch [4770/7250] loss: nan 
2025-12-13 14:45:06,946 - INFO - Epoch [1] Batch [4780/7250] loss: nan 
2025-12-13 14:45:07,306 - INFO - Epoch [1] Batch [4790/7250] loss: nan 
2025-12-13 14:45:07,655 - INFO - Epoch [1] Batch [4800/7250] loss: nan 
2025-12-13 14:45:07,998 - INFO - Epoch [1] Batch [4810/7250] loss: nan 
2025-12-13 14:45:08,338 - INFO - Epoch [1] Batch [4820/7250] loss: nan 
2025-12-13 14:45:08,673 - INFO - Epoch [1] Batch [4830/7250] loss: nan 
2025-12-13 14:45:09,013 - INFO - Epoch [1] Batch [4840/7250] loss: nan 
2025-12-13 14:45:09,355 - INFO - Epoch [1] Batch [4850/7250] loss: nan 
2025-12-13 14:45:09,698 - INFO - Epoch [1] Batch [4860/7250] loss: nan 
2025-12-13 14:45:10,028 - INFO - Epoch [1] Batch [4870/7250] loss: nan 
2025-12-13 14:45:10,371 - INFO - Epoch [1] Batch [4880/7250] loss: nan 
2025-12-13 14:45:10,710 - INFO - Epoch [1] Batch [4890/7250] loss: nan 
2025-12-13 14:45:11,058 - INFO - Epoch [1] Batch [4900/7250] loss: nan 
2025-12-13 14:45:11,417 - INFO - Epoch [1] Batch [4910/7250] loss: nan 
2025-12-13 14:45:11,765 - INFO - Epoch [1] Batch [4920/7250] loss: nan 
2025-12-13 14:45:12,102 - INFO - Epoch [1] Batch [4930/7250] loss: nan 
2025-12-13 14:45:12,452 - INFO - Epoch [1] Batch [4940/7250] loss: nan 
2025-12-13 14:45:12,799 - INFO - Epoch [1] Batch [4950/7250] loss: nan 
2025-12-13 14:45:13,148 - INFO - Epoch [1] Batch [4960/7250] loss: nan 
2025-12-13 14:45:13,504 - INFO - Epoch [1] Batch [4970/7250] loss: nan 
2025-12-13 14:45:13,859 - INFO - Epoch [1] Batch [4980/7250] loss: nan 
2025-12-13 14:45:14,215 - INFO - Epoch [1] Batch [4990/7250] loss: nan 
2025-12-13 14:45:14,565 - INFO - Epoch [1] Batch [5000/7250] loss: nan 
2025-12-13 14:45:14,897 - INFO - Epoch [1] Batch [5010/7250] loss: nan 
2025-12-13 14:45:15,251 - INFO - Epoch [1] Batch [5020/7250] loss: nan 
2025-12-13 14:45:15,593 - INFO - Epoch [1] Batch [5030/7250] loss: nan 
2025-12-13 14:45:15,951 - INFO - Epoch [1] Batch [5040/7250] loss: nan 
2025-12-13 14:45:16,289 - INFO - Epoch [1] Batch [5050/7250] loss: nan 
2025-12-13 14:45:16,640 - INFO - Epoch [1] Batch [5060/7250] loss: nan 
2025-12-13 14:45:16,989 - INFO - Epoch [1] Batch [5070/7250] loss: nan 
2025-12-13 14:45:17,328 - INFO - Epoch [1] Batch [5080/7250] loss: nan 
2025-12-13 14:45:17,680 - INFO - Epoch [1] Batch [5090/7250] loss: nan 
2025-12-13 14:45:18,029 - INFO - Epoch [1] Batch [5100/7250] loss: nan 
2025-12-13 14:45:18,388 - INFO - Epoch [1] Batch [5110/7250] loss: nan 
2025-12-13 14:45:18,739 - INFO - Epoch [1] Batch [5120/7250] loss: nan 
2025-12-13 14:45:19,089 - INFO - Epoch [1] Batch [5130/7250] loss: nan 
2025-12-13 14:45:19,431 - INFO - Epoch [1] Batch [5140/7250] loss: nan 
2025-12-13 14:45:19,775 - INFO - Epoch [1] Batch [5150/7250] loss: nan 
2025-12-13 14:45:20,117 - INFO - Epoch [1] Batch [5160/7250] loss: nan 
2025-12-13 14:45:20,467 - INFO - Epoch [1] Batch [5170/7250] loss: nan 
2025-12-13 14:45:20,799 - INFO - Epoch [1] Batch [5180/7250] loss: nan 
2025-12-13 14:45:21,134 - INFO - Epoch [1] Batch [5190/7250] loss: nan 
2025-12-13 14:45:21,468 - INFO - Epoch [1] Batch [5200/7250] loss: nan 
2025-12-13 14:45:21,807 - INFO - Epoch [1] Batch [5210/7250] loss: nan 
2025-12-13 14:45:22,164 - INFO - Epoch [1] Batch [5220/7250] loss: nan 
2025-12-13 14:45:22,510 - INFO - Epoch [1] Batch [5230/7250] loss: nan 
2025-12-13 14:45:22,872 - INFO - Epoch [1] Batch [5240/7250] loss: nan 
2025-12-13 14:45:23,209 - INFO - Epoch [1] Batch [5250/7250] loss: nan 
2025-12-13 14:45:23,562 - INFO - Epoch [1] Batch [5260/7250] loss: nan 
2025-12-13 14:45:23,903 - INFO - Epoch [1] Batch [5270/7250] loss: nan 
2025-12-13 14:45:24,239 - INFO - Epoch [1] Batch [5280/7250] loss: nan 
2025-12-13 14:45:24,598 - INFO - Epoch [1] Batch [5290/7250] loss: nan 
2025-12-13 14:45:24,947 - INFO - Epoch [1] Batch [5300/7250] loss: nan 
2025-12-13 14:45:25,286 - INFO - Epoch [1] Batch [5310/7250] loss: nan 
2025-12-13 14:45:25,636 - INFO - Epoch [1] Batch [5320/7250] loss: nan 
2025-12-13 14:45:25,974 - INFO - Epoch [1] Batch [5330/7250] loss: nan 
2025-12-13 14:45:26,314 - INFO - Epoch [1] Batch [5340/7250] loss: nan 
2025-12-13 14:45:26,660 - INFO - Epoch [1] Batch [5350/7250] loss: nan 
2025-12-13 14:45:26,996 - INFO - Epoch [1] Batch [5360/7250] loss: nan 
2025-12-13 14:45:27,342 - INFO - Epoch [1] Batch [5370/7250] loss: nan 
2025-12-13 14:45:27,681 - INFO - Epoch [1] Batch [5380/7250] loss: nan 
2025-12-13 14:45:28,020 - INFO - Epoch [1] Batch [5390/7250] loss: nan 
2025-12-13 14:45:28,395 - INFO - Epoch [1] Batch [5400/7250] loss: nan 
2025-12-13 14:45:28,744 - INFO - Epoch [1] Batch [5410/7250] loss: nan 
2025-12-13 14:45:29,075 - INFO - Epoch [1] Batch [5420/7250] loss: nan 
2025-12-13 14:45:29,427 - INFO - Epoch [1] Batch [5430/7250] loss: nan 
2025-12-13 14:45:29,766 - INFO - Epoch [1] Batch [5440/7250] loss: nan 
2025-12-13 14:45:30,103 - INFO - Epoch [1] Batch [5450/7250] loss: nan 
2025-12-13 14:45:30,444 - INFO - Epoch [1] Batch [5460/7250] loss: nan 
2025-12-13 14:45:30,800 - INFO - Epoch [1] Batch [5470/7250] loss: nan 
2025-12-13 14:45:31,164 - INFO - Epoch [1] Batch [5480/7250] loss: nan 
2025-12-13 14:45:31,535 - INFO - Epoch [1] Batch [5490/7250] loss: nan 
2025-12-13 14:45:31,897 - INFO - Epoch [1] Batch [5500/7250] loss: nan 
2025-12-13 14:45:32,237 - INFO - Epoch [1] Batch [5510/7250] loss: nan 
2025-12-13 14:45:32,595 - INFO - Epoch [1] Batch [5520/7250] loss: nan 
2025-12-13 14:45:32,945 - INFO - Epoch [1] Batch [5530/7250] loss: nan 
2025-12-13 14:45:33,287 - INFO - Epoch [1] Batch [5540/7250] loss: nan 
2025-12-13 14:45:33,642 - INFO - Epoch [1] Batch [5550/7250] loss: nan 
2025-12-13 14:45:33,986 - INFO - Epoch [1] Batch [5560/7250] loss: nan 
2025-12-13 14:45:34,319 - INFO - Epoch [1] Batch [5570/7250] loss: nan 
2025-12-13 14:45:34,674 - INFO - Epoch [1] Batch [5580/7250] loss: nan 
2025-12-13 14:45:35,020 - INFO - Epoch [1] Batch [5590/7250] loss: nan 
2025-12-13 14:45:35,372 - INFO - Epoch [1] Batch [5600/7250] loss: nan 
2025-12-13 14:45:35,714 - INFO - Epoch [1] Batch [5610/7250] loss: nan 
2025-12-13 14:45:36,054 - INFO - Epoch [1] Batch [5620/7250] loss: nan 
2025-12-13 14:45:36,417 - INFO - Epoch [1] Batch [5630/7250] loss: nan 
2025-12-13 14:45:36,755 - INFO - Epoch [1] Batch [5640/7250] loss: nan 
2025-12-13 14:45:37,105 - INFO - Epoch [1] Batch [5650/7250] loss: nan 
2025-12-13 14:45:37,446 - INFO - Epoch [1] Batch [5660/7250] loss: nan 
2025-12-13 14:45:37,798 - INFO - Epoch [1] Batch [5670/7250] loss: nan 
2025-12-13 14:45:38,130 - INFO - Epoch [1] Batch [5680/7250] loss: nan 
2025-12-13 14:45:38,483 - INFO - Epoch [1] Batch [5690/7250] loss: nan 
2025-12-13 14:45:38,817 - INFO - Epoch [1] Batch [5700/7250] loss: nan 
2025-12-13 14:45:39,161 - INFO - Epoch [1] Batch [5710/7250] loss: nan 
2025-12-13 14:45:39,502 - INFO - Epoch [1] Batch [5720/7250] loss: nan 
2025-12-13 14:45:39,853 - INFO - Epoch [1] Batch [5730/7250] loss: nan 
2025-12-13 14:45:40,214 - INFO - Epoch [1] Batch [5740/7250] loss: nan 
2025-12-13 14:45:40,561 - INFO - Epoch [1] Batch [5750/7250] loss: nan 
2025-12-13 14:45:40,900 - INFO - Epoch [1] Batch [5760/7250] loss: nan 
2025-12-13 14:45:41,241 - INFO - Epoch [1] Batch [5770/7250] loss: nan 
2025-12-13 14:45:41,585 - INFO - Epoch [1] Batch [5780/7250] loss: nan 
2025-12-13 14:45:41,918 - INFO - Epoch [1] Batch [5790/7250] loss: nan 
2025-12-13 14:45:42,279 - INFO - Epoch [1] Batch [5800/7250] loss: nan 
2025-12-13 14:45:42,616 - INFO - Epoch [1] Batch [5810/7250] loss: nan 
2025-12-13 14:45:42,963 - INFO - Epoch [1] Batch [5820/7250] loss: nan 
2025-12-13 14:45:43,313 - INFO - Epoch [1] Batch [5830/7250] loss: nan 
2025-12-13 14:45:43,667 - INFO - Epoch [1] Batch [5840/7250] loss: nan 
2025-12-13 14:45:44,004 - INFO - Epoch [1] Batch [5850/7250] loss: nan 
2025-12-13 14:45:44,337 - INFO - Epoch [1] Batch [5860/7250] loss: nan 
2025-12-13 14:45:44,695 - INFO - Epoch [1] Batch [5870/7250] loss: nan 
2025-12-13 14:45:45,060 - INFO - Epoch [1] Batch [5880/7250] loss: nan 
2025-12-13 14:45:45,400 - INFO - Epoch [1] Batch [5890/7250] loss: nan 
2025-12-13 14:45:45,752 - INFO - Epoch [1] Batch [5900/7250] loss: nan 
2025-12-13 14:45:46,102 - INFO - Epoch [1] Batch [5910/7250] loss: nan 
2025-12-13 14:45:46,443 - INFO - Epoch [1] Batch [5920/7250] loss: nan 
2025-12-13 14:45:46,793 - INFO - Epoch [1] Batch [5930/7250] loss: nan 
2025-12-13 14:45:47,136 - INFO - Epoch [1] Batch [5940/7250] loss: nan 
2025-12-13 14:45:47,468 - INFO - Epoch [1] Batch [5950/7250] loss: nan 
2025-12-13 14:45:47,802 - INFO - Epoch [1] Batch [5960/7250] loss: nan 
2025-12-13 14:45:48,148 - INFO - Epoch [1] Batch [5970/7250] loss: nan 
2025-12-13 14:45:48,484 - INFO - Epoch [1] Batch [5980/7250] loss: nan 
2025-12-13 14:45:48,819 - INFO - Epoch [1] Batch [5990/7250] loss: nan 
2025-12-13 14:45:49,162 - INFO - Epoch [1] Batch [6000/7250] loss: nan 
2025-12-13 14:45:49,502 - INFO - Epoch [1] Batch [6010/7250] loss: nan 
2025-12-13 14:45:49,841 - INFO - Epoch [1] Batch [6020/7250] loss: nan 
2025-12-13 14:45:50,179 - INFO - Epoch [1] Batch [6030/7250] loss: nan 
2025-12-13 14:45:50,521 - INFO - Epoch [1] Batch [6040/7250] loss: nan 
2025-12-13 14:45:50,864 - INFO - Epoch [1] Batch [6050/7250] loss: nan 
2025-12-13 14:45:51,209 - INFO - Epoch [1] Batch [6060/7250] loss: nan 
2025-12-13 14:45:51,544 - INFO - Epoch [1] Batch [6070/7250] loss: nan 
2025-12-13 14:45:51,893 - INFO - Epoch [1] Batch [6080/7250] loss: nan 
2025-12-13 14:45:52,229 - INFO - Epoch [1] Batch [6090/7250] loss: nan 
2025-12-13 14:45:52,567 - INFO - Epoch [1] Batch [6100/7250] loss: nan 
2025-12-13 14:45:52,913 - INFO - Epoch [1] Batch [6110/7250] loss: nan 
2025-12-13 14:45:53,252 - INFO - Epoch [1] Batch [6120/7250] loss: nan 
2025-12-13 14:45:53,609 - INFO - Epoch [1] Batch [6130/7250] loss: nan 
2025-12-13 14:45:53,938 - INFO - Epoch [1] Batch [6140/7250] loss: nan 
2025-12-13 14:45:54,280 - INFO - Epoch [1] Batch [6150/7250] loss: nan 
2025-12-13 14:45:54,630 - INFO - Epoch [1] Batch [6160/7250] loss: nan 
2025-12-13 14:45:54,977 - INFO - Epoch [1] Batch [6170/7250] loss: nan 
2025-12-13 14:45:55,301 - INFO - Epoch [1] Batch [6180/7250] loss: nan 
2025-12-13 14:45:55,680 - INFO - Epoch [1] Batch [6190/7250] loss: nan 
2025-12-13 14:45:56,036 - INFO - Epoch [1] Batch [6200/7250] loss: nan 
2025-12-13 14:45:56,377 - INFO - Epoch [1] Batch [6210/7250] loss: nan 
2025-12-13 14:45:56,711 - INFO - Epoch [1] Batch [6220/7250] loss: nan 
2025-12-13 14:45:57,050 - INFO - Epoch [1] Batch [6230/7250] loss: nan 
2025-12-13 14:45:57,399 - INFO - Epoch [1] Batch [6240/7250] loss: nan 
2025-12-13 14:45:57,737 - INFO - Epoch [1] Batch [6250/7250] loss: nan 
2025-12-13 14:45:58,095 - INFO - Epoch [1] Batch [6260/7250] loss: nan 
2025-12-13 14:45:58,438 - INFO - Epoch [1] Batch [6270/7250] loss: nan 
2025-12-13 14:45:58,785 - INFO - Epoch [1] Batch [6280/7250] loss: nan 
2025-12-13 14:45:59,126 - INFO - Epoch [1] Batch [6290/7250] loss: nan 
2025-12-13 14:45:59,460 - INFO - Epoch [1] Batch [6300/7250] loss: nan 
2025-12-13 14:45:59,810 - INFO - Epoch [1] Batch [6310/7250] loss: nan 
2025-12-13 14:46:00,143 - INFO - Epoch [1] Batch [6320/7250] loss: nan 
2025-12-13 14:46:00,494 - INFO - Epoch [1] Batch [6330/7250] loss: nan 
2025-12-13 14:46:00,854 - INFO - Epoch [1] Batch [6340/7250] loss: nan 
2025-12-13 14:46:01,209 - INFO - Epoch [1] Batch [6350/7250] loss: nan 
2025-12-13 14:46:01,551 - INFO - Epoch [1] Batch [6360/7250] loss: nan 
2025-12-13 14:46:01,886 - INFO - Epoch [1] Batch [6370/7250] loss: nan 
2025-12-13 14:46:02,235 - INFO - Epoch [1] Batch [6380/7250] loss: nan 
2025-12-13 14:46:02,614 - INFO - Epoch [1] Batch [6390/7250] loss: nan 
2025-12-13 14:46:02,969 - INFO - Epoch [1] Batch [6400/7250] loss: nan 
2025-12-13 14:46:03,327 - INFO - Epoch [1] Batch [6410/7250] loss: nan 
2025-12-13 14:46:03,691 - INFO - Epoch [1] Batch [6420/7250] loss: nan 
2025-12-13 14:46:04,046 - INFO - Epoch [1] Batch [6430/7250] loss: nan 
2025-12-13 14:46:04,411 - INFO - Epoch [1] Batch [6440/7250] loss: nan 
2025-12-13 14:46:04,768 - INFO - Epoch [1] Batch [6450/7250] loss: nan 
2025-12-13 14:46:05,130 - INFO - Epoch [1] Batch [6460/7250] loss: nan 
2025-12-13 14:46:05,487 - INFO - Epoch [1] Batch [6470/7250] loss: nan 
2025-12-13 14:46:05,841 - INFO - Epoch [1] Batch [6480/7250] loss: nan 
2025-12-13 14:46:06,215 - INFO - Epoch [1] Batch [6490/7250] loss: nan 
2025-12-13 14:46:06,578 - INFO - Epoch [1] Batch [6500/7250] loss: nan 
2025-12-13 14:46:06,918 - INFO - Epoch [1] Batch [6510/7250] loss: nan 
2025-12-13 14:46:07,282 - INFO - Epoch [1] Batch [6520/7250] loss: nan 
2025-12-13 14:46:07,629 - INFO - Epoch [1] Batch [6530/7250] loss: nan 
2025-12-13 14:46:07,996 - INFO - Epoch [1] Batch [6540/7250] loss: nan 
2025-12-13 14:46:08,358 - INFO - Epoch [1] Batch [6550/7250] loss: nan 
2025-12-13 14:46:08,712 - INFO - Epoch [1] Batch [6560/7250] loss: nan 
2025-12-13 14:46:09,078 - INFO - Epoch [1] Batch [6570/7250] loss: nan 
2025-12-13 14:46:09,474 - INFO - Epoch [1] Batch [6580/7250] loss: nan 
2025-12-13 14:46:09,835 - INFO - Epoch [1] Batch [6590/7250] loss: nan 
2025-12-13 14:46:10,197 - INFO - Epoch [1] Batch [6600/7250] loss: nan 
2025-12-13 14:46:10,552 - INFO - Epoch [1] Batch [6610/7250] loss: nan 
2025-12-13 14:46:10,904 - INFO - Epoch [1] Batch [6620/7250] loss: nan 
2025-12-13 14:46:11,250 - INFO - Epoch [1] Batch [6630/7250] loss: nan 
2025-12-13 14:46:11,599 - INFO - Epoch [1] Batch [6640/7250] loss: nan 
2025-12-13 14:46:11,969 - INFO - Epoch [1] Batch [6650/7250] loss: nan 
2025-12-13 14:46:12,332 - INFO - Epoch [1] Batch [6660/7250] loss: nan 
2025-12-13 14:46:12,685 - INFO - Epoch [1] Batch [6670/7250] loss: nan 
2025-12-13 14:46:13,031 - INFO - Epoch [1] Batch [6680/7250] loss: nan 
2025-12-13 14:46:13,392 - INFO - Epoch [1] Batch [6690/7250] loss: nan 
2025-12-13 14:46:13,737 - INFO - Epoch [1] Batch [6700/7250] loss: nan 
2025-12-13 14:46:14,095 - INFO - Epoch [1] Batch [6710/7250] loss: nan 
2025-12-13 14:46:14,468 - INFO - Epoch [1] Batch [6720/7250] loss: nan 
2025-12-13 14:46:14,814 - INFO - Epoch [1] Batch [6730/7250] loss: nan 
2025-12-13 14:46:15,171 - INFO - Epoch [1] Batch [6740/7250] loss: nan 
2025-12-13 14:46:15,537 - INFO - Epoch [1] Batch [6750/7250] loss: nan 
2025-12-13 14:46:15,891 - INFO - Epoch [1] Batch [6760/7250] loss: nan 
2025-12-13 14:46:16,239 - INFO - Epoch [1] Batch [6770/7250] loss: nan 
2025-12-13 14:46:16,593 - INFO - Epoch [1] Batch [6780/7250] loss: nan 
2025-12-13 14:46:16,940 - INFO - Epoch [1] Batch [6790/7250] loss: nan 
2025-12-13 14:46:17,286 - INFO - Epoch [1] Batch [6800/7250] loss: nan 
2025-12-13 14:46:17,641 - INFO - Epoch [1] Batch [6810/7250] loss: nan 
2025-12-13 14:46:17,996 - INFO - Epoch [1] Batch [6820/7250] loss: nan 
2025-12-13 14:46:18,356 - INFO - Epoch [1] Batch [6830/7250] loss: nan 
2025-12-13 14:46:18,702 - INFO - Epoch [1] Batch [6840/7250] loss: nan 
2025-12-13 14:46:19,052 - INFO - Epoch [1] Batch [6850/7250] loss: nan 
2025-12-13 14:46:19,402 - INFO - Epoch [1] Batch [6860/7250] loss: nan 
2025-12-13 14:46:19,794 - INFO - Epoch [1] Batch [6870/7250] loss: nan 
2025-12-13 14:46:20,168 - INFO - Epoch [1] Batch [6880/7250] loss: nan 
2025-12-13 14:46:20,614 - INFO - Epoch [1] Batch [6890/7250] loss: nan 
2025-12-13 14:46:21,065 - INFO - Epoch [1] Batch [6900/7250] loss: nan 
2025-12-13 14:46:21,485 - INFO - Epoch [1] Batch [6910/7250] loss: nan 
2025-12-13 14:46:21,908 - INFO - Epoch [1] Batch [6920/7250] loss: nan 
2025-12-13 14:46:22,322 - INFO - Epoch [1] Batch [6930/7250] loss: nan 
2025-12-13 14:46:22,747 - INFO - Epoch [1] Batch [6940/7250] loss: nan 
2025-12-13 14:46:23,180 - INFO - Epoch [1] Batch [6950/7250] loss: nan 
2025-12-13 14:46:23,614 - INFO - Epoch [1] Batch [6960/7250] loss: nan 
2025-12-13 14:46:24,073 - INFO - Epoch [1] Batch [6970/7250] loss: nan 
2025-12-13 14:46:24,526 - INFO - Epoch [1] Batch [6980/7250] loss: nan 
2025-12-13 14:46:25,009 - INFO - Epoch [1] Batch [6990/7250] loss: nan 
2025-12-13 14:46:25,519 - INFO - Epoch [1] Batch [7000/7250] loss: nan 
2025-12-13 14:46:25,998 - INFO - Epoch [1] Batch [7010/7250] loss: nan 
2025-12-13 14:46:26,459 - INFO - Epoch [1] Batch [7020/7250] loss: nan 
2025-12-13 14:46:26,921 - INFO - Epoch [1] Batch [7030/7250] loss: nan 
2025-12-13 14:46:27,385 - INFO - Epoch [1] Batch [7040/7250] loss: nan 
2025-12-13 14:46:27,860 - INFO - Epoch [1] Batch [7050/7250] loss: nan 
2025-12-13 14:46:28,232 - INFO - Epoch [1] Batch [7060/7250] loss: nan 
2025-12-13 14:46:28,605 - INFO - Epoch [1] Batch [7070/7250] loss: nan 
2025-12-13 14:46:28,966 - INFO - Epoch [1] Batch [7080/7250] loss: nan 
2025-12-13 14:46:29,365 - INFO - Epoch [1] Batch [7090/7250] loss: nan 
2025-12-13 14:46:29,803 - INFO - Epoch [1] Batch [7100/7250] loss: nan 
2025-12-13 14:46:30,214 - INFO - Epoch [1] Batch [7110/7250] loss: nan 
2025-12-13 14:46:30,592 - INFO - Epoch [1] Batch [7120/7250] loss: nan 
2025-12-13 14:46:30,968 - INFO - Epoch [1] Batch [7130/7250] loss: nan 
2025-12-13 14:46:31,343 - INFO - Epoch [1] Batch [7140/7250] loss: nan 
2025-12-13 14:46:31,718 - INFO - Epoch [1] Batch [7150/7250] loss: nan 
2025-12-13 14:46:32,095 - INFO - Epoch [1] Batch [7160/7250] loss: nan 
2025-12-13 14:46:32,481 - INFO - Epoch [1] Batch [7170/7250] loss: nan 
2025-12-13 14:46:32,872 - INFO - Epoch [1] Batch [7180/7250] loss: nan 
2025-12-13 14:46:33,251 - INFO - Epoch [1] Batch [7190/7250] loss: nan 
2025-12-13 14:46:33,620 - INFO - Epoch [1] Batch [7200/7250] loss: nan 
2025-12-13 14:46:33,995 - INFO - Epoch [1] Batch [7210/7250] loss: nan 
2025-12-13 14:46:34,363 - INFO - Epoch [1] Batch [7220/7250] loss: nan 
2025-12-13 14:46:34,746 - INFO - Epoch [1] Batch [7230/7250] loss: nan 
2025-12-13 14:46:35,122 - INFO - Epoch [1] Batch [7240/7250] loss: nan 
2025-12-13 14:47:01,149 - INFO - Epoch [1] Batch [0/254] loss: nan 
2025-12-13 14:47:01,374 - INFO - Epoch [1] Batch [10/254] loss: nan 
2025-12-13 14:47:01,586 - INFO - Epoch [1] Batch [20/254] loss: nan 
2025-12-13 14:47:01,812 - INFO - Epoch [1] Batch [30/254] loss: nan 
2025-12-13 14:47:02,026 - INFO - Epoch [1] Batch [40/254] loss: nan 
2025-12-13 14:47:02,220 - INFO - Epoch [1] Batch [50/254] loss: nan 
2025-12-13 14:47:02,436 - INFO - Epoch [1] Batch [60/254] loss: nan 
2025-12-13 14:47:02,644 - INFO - Epoch [1] Batch [70/254] loss: nan 
2025-12-13 14:47:02,847 - INFO - Epoch [1] Batch [80/254] loss: nan 
2025-12-13 14:47:03,060 - INFO - Epoch [1] Batch [90/254] loss: nan 
2025-12-13 14:47:03,285 - INFO - Epoch [1] Batch [100/254] loss: nan 
2025-12-13 14:47:03,494 - INFO - Epoch [1] Batch [110/254] loss: nan 
2025-12-13 14:47:03,706 - INFO - Epoch [1] Batch [120/254] loss: nan 
2025-12-13 14:47:03,908 - INFO - Epoch [1] Batch [130/254] loss: nan 
2025-12-13 14:47:04,131 - INFO - Epoch [1] Batch [140/254] loss: nan 
2025-12-13 14:47:04,339 - INFO - Epoch [1] Batch [150/254] loss: nan 
2025-12-13 14:47:04,558 - INFO - Epoch [1] Batch [160/254] loss: nan 
2025-12-13 14:47:04,769 - INFO - Epoch [1] Batch [170/254] loss: nan 
2025-12-13 14:47:04,970 - INFO - Epoch [1] Batch [180/254] loss: nan 
2025-12-13 14:47:05,199 - INFO - Epoch [1] Batch [190/254] loss: nan 
2025-12-13 14:47:05,392 - INFO - Epoch [1] Batch [200/254] loss: nan 
2025-12-13 14:47:05,594 - INFO - Epoch [1] Batch [210/254] loss: nan 
2025-12-13 14:47:05,793 - INFO - Epoch [1] Batch [220/254] loss: nan 
2025-12-13 14:47:05,988 - INFO - Epoch [1] Batch [230/254] loss: nan 
2025-12-13 14:47:06,191 - INFO - Epoch [1] Batch [240/254] loss: nan 
2025-12-13 14:47:06,379 - INFO - Epoch [1] Batch [250/254] loss: nan 
2025-12-13 14:47:07,158 - INFO - Epoch: [1/10]Train loss: nan | Val loss: nan
2025-12-13 14:47:07,158 - INFO - Validation loss did not improve. Current best loss: inf
2025-12-13 14:47:08,069 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 1)
2025-12-13 14:47:29,957 - INFO - Epoch [2] Batch [0/7250] loss: nan 
2025-12-13 14:47:30,390 - INFO - Epoch [2] Batch [10/7250] loss: nan 
2025-12-13 14:47:30,823 - INFO - Epoch [2] Batch [20/7250] loss: nan 
2025-12-13 14:47:31,261 - INFO - Epoch [2] Batch [30/7250] loss: nan 
2025-12-13 14:47:31,708 - INFO - Epoch [2] Batch [40/7250] loss: nan 
2025-12-13 14:47:32,143 - INFO - Epoch [2] Batch [50/7250] loss: nan 
2025-12-13 14:47:32,579 - INFO - Epoch [2] Batch [60/7250] loss: nan 
2025-12-13 14:47:33,011 - INFO - Epoch [2] Batch [70/7250] loss: nan 
2025-12-13 14:47:33,443 - INFO - Epoch [2] Batch [80/7250] loss: nan 
2025-12-13 14:47:33,875 - INFO - Epoch [2] Batch [90/7250] loss: nan 
2025-12-13 14:47:34,295 - INFO - Epoch [2] Batch [100/7250] loss: nan 
2025-12-13 14:47:34,734 - INFO - Epoch [2] Batch [110/7250] loss: nan 
2025-12-13 14:47:35,161 - INFO - Epoch [2] Batch [120/7250] loss: nan 
2025-12-13 14:47:35,581 - INFO - Epoch [2] Batch [130/7250] loss: nan 
2025-12-13 14:47:36,010 - INFO - Epoch [2] Batch [140/7250] loss: nan 
2025-12-13 14:47:36,431 - INFO - Epoch [2] Batch [150/7250] loss: nan 
2025-12-13 14:47:36,859 - INFO - Epoch [2] Batch [160/7250] loss: nan 
2025-12-13 14:47:37,295 - INFO - Epoch [2] Batch [170/7250] loss: nan 
2025-12-13 14:47:37,720 - INFO - Epoch [2] Batch [180/7250] loss: nan 
2025-12-13 14:47:38,160 - INFO - Epoch [2] Batch [190/7250] loss: nan 
2025-12-13 14:47:38,655 - INFO - Epoch [2] Batch [200/7250] loss: nan 
2025-12-13 14:47:39,120 - INFO - Epoch [2] Batch [210/7250] loss: nan 
2025-12-13 14:47:39,583 - INFO - Epoch [2] Batch [220/7250] loss: nan 
2025-12-13 14:47:40,093 - INFO - Epoch [2] Batch [230/7250] loss: nan 
2025-12-13 14:47:40,544 - INFO - Epoch [2] Batch [240/7250] loss: nan 
2025-12-13 14:47:41,018 - INFO - Epoch [2] Batch [250/7250] loss: nan 
2025-12-13 14:47:41,483 - INFO - Epoch [2] Batch [260/7250] loss: nan 
2025-12-13 14:47:41,958 - INFO - Epoch [2] Batch [270/7250] loss: nan 
2025-12-13 14:47:42,404 - INFO - Epoch [2] Batch [280/7250] loss: nan 
2025-12-13 14:47:42,841 - INFO - Epoch [2] Batch [290/7250] loss: nan 
2025-12-13 14:47:43,269 - INFO - Epoch [2] Batch [300/7250] loss: nan 
2025-12-13 14:47:43,715 - INFO - Epoch [2] Batch [310/7250] loss: nan 
2025-12-13 14:47:44,157 - INFO - Epoch [2] Batch [320/7250] loss: nan 
2025-12-13 14:47:44,604 - INFO - Epoch [2] Batch [330/7250] loss: nan 
2025-12-13 14:47:45,039 - INFO - Epoch [2] Batch [340/7250] loss: nan 
2025-12-13 14:47:45,487 - INFO - Epoch [2] Batch [350/7250] loss: nan 
2025-12-13 14:47:45,904 - INFO - Epoch [2] Batch [360/7250] loss: nan 
2025-12-13 14:47:46,343 - INFO - Epoch [2] Batch [370/7250] loss: nan 
2025-12-13 14:47:46,791 - INFO - Epoch [2] Batch [380/7250] loss: nan 
2025-12-13 14:47:47,249 - INFO - Epoch [2] Batch [390/7250] loss: nan 
2025-12-13 14:47:47,672 - INFO - Epoch [2] Batch [400/7250] loss: nan 
2025-12-13 14:47:48,123 - INFO - Epoch [2] Batch [410/7250] loss: nan 
2025-12-13 14:47:48,552 - INFO - Epoch [2] Batch [420/7250] loss: nan 
2025-12-13 14:47:48,989 - INFO - Epoch [2] Batch [430/7250] loss: nan 
2025-12-13 14:47:49,458 - INFO - Epoch [2] Batch [440/7250] loss: nan 
2025-12-13 14:47:49,889 - INFO - Epoch [2] Batch [450/7250] loss: nan 
2025-12-13 14:47:50,319 - INFO - Epoch [2] Batch [460/7250] loss: nan 
2025-12-13 14:47:50,754 - INFO - Epoch [2] Batch [470/7250] loss: nan 
2025-12-13 14:47:51,214 - INFO - Epoch [2] Batch [480/7250] loss: nan 
2025-12-13 14:47:51,656 - INFO - Epoch [2] Batch [490/7250] loss: nan 
2025-12-13 14:47:52,083 - INFO - Epoch [2] Batch [500/7250] loss: nan 
2025-12-13 14:47:52,527 - INFO - Epoch [2] Batch [510/7250] loss: nan 
2025-12-13 14:47:52,985 - INFO - Epoch [2] Batch [520/7250] loss: nan 
2025-12-13 14:47:53,441 - INFO - Epoch [2] Batch [530/7250] loss: nan 
2025-12-13 14:47:53,909 - INFO - Epoch [2] Batch [540/7250] loss: nan 
2025-12-13 14:47:54,366 - INFO - Epoch [2] Batch [550/7250] loss: nan 
2025-12-13 14:47:54,831 - INFO - Epoch [2] Batch [560/7250] loss: nan 
2025-12-13 14:47:55,304 - INFO - Epoch [2] Batch [570/7250] loss: nan 
2025-12-13 14:47:55,769 - INFO - Epoch [2] Batch [580/7250] loss: nan 
2025-12-13 14:47:56,238 - INFO - Epoch [2] Batch [590/7250] loss: nan 
2025-12-13 14:47:56,695 - INFO - Epoch [2] Batch [600/7250] loss: nan 
2025-12-13 14:47:57,171 - INFO - Epoch [2] Batch [610/7250] loss: nan 
2025-12-13 14:47:57,629 - INFO - Epoch [2] Batch [620/7250] loss: nan 
2025-12-13 14:47:58,106 - INFO - Epoch [2] Batch [630/7250] loss: nan 
2025-12-13 14:47:58,551 - INFO - Epoch [2] Batch [640/7250] loss: nan 
2025-12-13 14:47:58,996 - INFO - Epoch [2] Batch [650/7250] loss: nan 
2025-12-13 14:47:59,445 - INFO - Epoch [2] Batch [660/7250] loss: nan 
2025-12-13 14:47:59,896 - INFO - Epoch [2] Batch [670/7250] loss: nan 
2025-12-13 14:48:00,363 - INFO - Epoch [2] Batch [680/7250] loss: nan 
2025-12-13 14:48:00,818 - INFO - Epoch [2] Batch [690/7250] loss: nan 
2025-12-13 14:48:01,281 - INFO - Epoch [2] Batch [700/7250] loss: nan 
2025-12-13 14:48:01,749 - INFO - Epoch [2] Batch [710/7250] loss: nan 
2025-12-13 14:48:02,227 - INFO - Epoch [2] Batch [720/7250] loss: nan 
2025-12-13 14:48:02,684 - INFO - Epoch [2] Batch [730/7250] loss: nan 
2025-12-13 14:48:03,146 - INFO - Epoch [2] Batch [740/7250] loss: nan 
2025-12-13 14:48:03,584 - INFO - Epoch [2] Batch [750/7250] loss: nan 
2025-12-13 14:48:04,022 - INFO - Epoch [2] Batch [760/7250] loss: nan 
2025-12-13 14:48:04,473 - INFO - Epoch [2] Batch [770/7250] loss: nan 
2025-12-13 14:48:04,954 - INFO - Epoch [2] Batch [780/7250] loss: nan 
2025-12-13 14:48:05,420 - INFO - Epoch [2] Batch [790/7250] loss: nan 
2025-12-13 14:48:05,864 - INFO - Epoch [2] Batch [800/7250] loss: nan 
2025-12-13 14:48:06,305 - INFO - Epoch [2] Batch [810/7250] loss: nan 
2025-12-13 14:48:06,742 - INFO - Epoch [2] Batch [820/7250] loss: nan 
2025-12-13 14:48:07,174 - INFO - Epoch [2] Batch [830/7250] loss: nan 
2025-12-13 14:48:07,599 - INFO - Epoch [2] Batch [840/7250] loss: nan 
2025-12-13 14:48:08,052 - INFO - Epoch [2] Batch [850/7250] loss: nan 
2025-12-13 14:48:08,511 - INFO - Epoch [2] Batch [860/7250] loss: nan 
2025-12-13 14:48:08,950 - INFO - Epoch [2] Batch [870/7250] loss: nan 
2025-12-13 14:48:09,390 - INFO - Epoch [2] Batch [880/7250] loss: nan 
2025-12-13 14:48:09,831 - INFO - Epoch [2] Batch [890/7250] loss: nan 
2025-12-13 14:48:10,274 - INFO - Epoch [2] Batch [900/7250] loss: nan 
2025-12-13 14:48:10,724 - INFO - Epoch [2] Batch [910/7250] loss: nan 
2025-12-13 14:48:11,206 - INFO - Epoch [2] Batch [920/7250] loss: nan 
2025-12-13 14:48:11,614 - INFO - Epoch [2] Batch [930/7250] loss: nan 
2025-12-13 14:48:11,985 - INFO - Epoch [2] Batch [940/7250] loss: nan 
2025-12-13 14:48:12,356 - INFO - Epoch [2] Batch [950/7250] loss: nan 
2025-12-13 14:48:12,725 - INFO - Epoch [2] Batch [960/7250] loss: nan 
2025-12-13 14:48:13,092 - INFO - Epoch [2] Batch [970/7250] loss: nan 
2025-12-13 14:49:03,676 - INFO - Logging initialize successfully
2025-12-13 14:49:05,981 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 14:49:06,158 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 14:49:08,177 - INFO - Built 29000 samples from annotaions
2025-12-13 14:49:08,179 - INFO - Total train samples: 29000
2025-12-13 14:49:08,237 - INFO - Built 1014 samples from annotaions
2025-12-13 14:49:08,237 - INFO - Total val samples: 1014
2025-12-13 14:49:08,330 - INFO - Built 29000 samples for story 
2025-12-13 14:49:08,330 - INFO - Built 1014 samples for story 
2025-12-13 14:49:08,542 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 14:49:08,552 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 14:49:12,497 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming at epoch 1. (Scheduler restored: True, Scaler restored: True)
2025-12-13 14:49:12,497 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming training from epoch 1, best loss tracked: inf
2025-12-13 14:49:37,228 - INFO - Epoch [1] Batch [0/7250] loss: nan 
2025-12-13 14:49:40,609 - INFO - Epoch [1] Batch [100/7250] loss: nan 
2025-12-13 14:49:43,975 - INFO - Epoch [1] Batch [200/7250] loss: nan 
2025-12-13 14:49:47,320 - INFO - Epoch [1] Batch [300/7250] loss: nan 
2025-12-13 14:53:43,550 - INFO - Logging initialize successfully
2025-12-13 14:53:45,842 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 14:53:45,973 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 14:53:47,370 - INFO - Built 29000 samples from annotaions
2025-12-13 14:53:47,371 - INFO - Total train samples: 29000
2025-12-13 14:53:47,406 - INFO - Built 1014 samples from annotaions
2025-12-13 14:53:47,406 - INFO - Total val samples: 1014
2025-12-13 14:53:47,467 - INFO - Built 29000 samples for story 
2025-12-13 14:53:47,467 - INFO - Built 1014 samples for story 
2025-12-13 14:53:47,606 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 14:53:47,626 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 14:53:51,844 - INFO - No valid checkpoint found or resume disabled. Starting from scratch.
2025-12-13 14:54:10,694 - INFO - Epoch [1] Batch [0/7250] loss: 10.9993 
2025-12-13 14:54:16,537 - INFO - Epoch [1] Batch [100/7250] loss: 6.3941 
2025-12-13 14:54:22,516 - INFO - Epoch [1] Batch [200/7250] loss: 6.3114 
2025-12-13 14:54:28,429 - INFO - Epoch [1] Batch [300/7250] loss: 5.9478 
2025-12-13 14:54:34,356 - INFO - Epoch [1] Batch [400/7250] loss: 6.3597 
2025-12-13 14:54:40,266 - INFO - Epoch [1] Batch [500/7250] loss: 6.2408 
2025-12-13 14:54:46,287 - INFO - Epoch [1] Batch [600/7250] loss: 5.8349 
2025-12-13 14:54:49,456 - WARNING - NaN/Inf loss detected at batch 662, skipping
2025-12-13 14:54:49,724 - WARNING - NaN/Inf loss detected at batch 668, skipping
2025-12-13 14:54:49,745 - WARNING - NaN/Inf loss detected at batch 669, skipping
2025-12-13 14:54:49,768 - WARNING - NaN/Inf loss detected at batch 670, skipping
2025-12-13 14:54:49,861 - WARNING - NaN/Inf loss detected at batch 673, skipping
2025-12-13 14:54:50,000 - WARNING - NaN/Inf loss detected at batch 677, skipping
2025-12-13 14:54:50,124 - WARNING - NaN/Inf loss detected at batch 680, skipping
2025-12-13 14:54:50,280 - WARNING - NaN/Inf loss detected at batch 684, skipping
2025-12-13 14:54:50,302 - WARNING - NaN/Inf loss detected at batch 685, skipping
2025-12-13 14:54:50,531 - WARNING - NaN/Inf loss detected at batch 691, skipping
2025-12-13 14:54:50,551 - WARNING - NaN/Inf loss detected at batch 692, skipping
2025-12-13 14:54:50,646 - WARNING - NaN/Inf loss detected at batch 695, skipping
2025-12-13 14:54:50,667 - WARNING - NaN/Inf loss detected at batch 696, skipping
2025-12-13 14:54:50,726 - WARNING - NaN/Inf loss detected at batch 698, skipping
2025-12-13 14:54:50,747 - WARNING - NaN/Inf loss detected at batch 699, skipping
2025-12-13 14:54:50,784 - INFO - Epoch [1] Batch [700/7250] loss: 5.9019 
2025-12-13 14:54:50,916 - WARNING - NaN/Inf loss detected at batch 704, skipping
2025-12-13 14:54:51,011 - WARNING - NaN/Inf loss detected at batch 707, skipping
2025-12-13 14:54:51,193 - WARNING - NaN/Inf loss detected at batch 712, skipping
2025-12-13 14:54:51,271 - WARNING - NaN/Inf loss detected at batch 714, skipping
2025-12-13 14:54:51,416 - WARNING - NaN/Inf loss detected at batch 718, skipping
2025-12-13 14:54:51,512 - WARNING - NaN/Inf loss detected at batch 721, skipping
2025-12-13 14:54:51,539 - WARNING - NaN/Inf loss detected at batch 722, skipping
2025-12-13 14:54:51,595 - WARNING - NaN/Inf loss detected at batch 724, skipping
2025-12-13 14:54:51,617 - WARNING - NaN/Inf loss detected at batch 725, skipping
2025-12-13 14:54:51,735 - WARNING - NaN/Inf loss detected at batch 728, skipping
2025-12-13 14:54:51,795 - WARNING - NaN/Inf loss detected at batch 730, skipping
2025-12-13 14:54:51,817 - WARNING - NaN/Inf loss detected at batch 731, skipping
2025-12-13 14:54:51,839 - WARNING - NaN/Inf loss detected at batch 732, skipping
2025-12-13 14:54:51,861 - WARNING - NaN/Inf loss detected at batch 733, skipping
2025-12-13 14:54:51,884 - WARNING - NaN/Inf loss detected at batch 734, skipping
2025-12-13 14:54:51,907 - WARNING - NaN/Inf loss detected at batch 735, skipping
2025-12-13 14:54:51,927 - WARNING - NaN/Inf loss detected at batch 736, skipping
2025-12-13 14:54:51,949 - WARNING - NaN/Inf loss detected at batch 737, skipping
2025-12-13 14:54:51,969 - WARNING - NaN/Inf loss detected at batch 738, skipping
2025-12-13 14:54:51,990 - WARNING - NaN/Inf loss detected at batch 739, skipping
2025-12-13 14:54:52,013 - WARNING - NaN/Inf loss detected at batch 740, skipping
2025-12-13 14:54:52,036 - WARNING - NaN/Inf loss detected at batch 741, skipping
2025-12-13 14:54:52,059 - WARNING - NaN/Inf loss detected at batch 742, skipping
2025-12-13 14:54:52,122 - WARNING - NaN/Inf loss detected at batch 744, skipping
2025-12-13 14:54:52,183 - WARNING - NaN/Inf loss detected at batch 746, skipping
2025-12-13 14:54:52,206 - WARNING - NaN/Inf loss detected at batch 747, skipping
2025-12-13 14:54:52,425 - WARNING - NaN/Inf loss detected at batch 752, skipping
2025-12-13 14:54:52,448 - WARNING - NaN/Inf loss detected at batch 753, skipping
2025-12-13 14:54:52,473 - WARNING - NaN/Inf loss detected at batch 754, skipping
2025-12-13 14:54:52,496 - WARNING - NaN/Inf loss detected at batch 755, skipping
2025-12-13 14:54:52,519 - WARNING - NaN/Inf loss detected at batch 756, skipping
2025-12-13 14:54:52,540 - WARNING - NaN/Inf loss detected at batch 757, skipping
2025-12-13 14:54:52,563 - WARNING - NaN/Inf loss detected at batch 758, skipping
2025-12-13 14:54:52,585 - WARNING - NaN/Inf loss detected at batch 759, skipping
2025-12-13 14:54:52,607 - WARNING - NaN/Inf loss detected at batch 760, skipping
2025-12-13 14:54:52,630 - WARNING - NaN/Inf loss detected at batch 761, skipping
2025-12-13 14:54:52,651 - WARNING - NaN/Inf loss detected at batch 762, skipping
2025-12-13 14:54:52,715 - WARNING - NaN/Inf loss detected at batch 764, skipping
2025-12-13 14:54:52,741 - WARNING - NaN/Inf loss detected at batch 765, skipping
2025-12-13 14:54:52,830 - WARNING - NaN/Inf loss detected at batch 767, skipping
2025-12-13 14:54:52,852 - WARNING - NaN/Inf loss detected at batch 768, skipping
2025-12-13 14:54:52,878 - WARNING - NaN/Inf loss detected at batch 769, skipping
2025-12-13 14:54:52,902 - WARNING - NaN/Inf loss detected at batch 770, skipping
2025-12-13 14:54:52,923 - WARNING - NaN/Inf loss detected at batch 771, skipping
2025-12-13 14:54:52,946 - WARNING - NaN/Inf loss detected at batch 772, skipping
2025-12-13 14:54:53,007 - WARNING - NaN/Inf loss detected at batch 774, skipping
2025-12-13 14:54:53,032 - WARNING - NaN/Inf loss detected at batch 775, skipping
2025-12-13 14:54:53,054 - WARNING - NaN/Inf loss detected at batch 776, skipping
2025-12-13 14:54:53,076 - WARNING - NaN/Inf loss detected at batch 777, skipping
2025-12-13 14:54:53,098 - WARNING - NaN/Inf loss detected at batch 778, skipping
2025-12-13 14:54:53,121 - WARNING - NaN/Inf loss detected at batch 779, skipping
2025-12-13 14:54:53,147 - WARNING - NaN/Inf loss detected at batch 780, skipping
2025-12-13 14:54:53,168 - WARNING - NaN/Inf loss detected at batch 781, skipping
2025-12-13 14:54:53,190 - WARNING - NaN/Inf loss detected at batch 782, skipping
2025-12-13 14:54:53,212 - WARNING - NaN/Inf loss detected at batch 783, skipping
2025-12-13 14:54:53,234 - WARNING - NaN/Inf loss detected at batch 784, skipping
2025-12-13 14:54:53,256 - WARNING - NaN/Inf loss detected at batch 785, skipping
2025-12-13 14:54:53,275 - WARNING - NaN/Inf loss detected at batch 786, skipping
2025-12-13 14:54:53,301 - WARNING - NaN/Inf loss detected at batch 787, skipping
2025-12-13 14:54:53,327 - WARNING - NaN/Inf loss detected at batch 788, skipping
2025-12-13 14:54:53,412 - WARNING - NaN/Inf loss detected at batch 790, skipping
2025-12-13 14:54:53,509 - WARNING - NaN/Inf loss detected at batch 793, skipping
2025-12-13 14:54:53,531 - WARNING - NaN/Inf loss detected at batch 794, skipping
2025-12-13 14:54:53,554 - WARNING - NaN/Inf loss detected at batch 795, skipping
2025-12-13 14:54:53,575 - WARNING - NaN/Inf loss detected at batch 796, skipping
2025-12-13 14:54:53,636 - WARNING - NaN/Inf loss detected at batch 798, skipping
2025-12-13 14:54:53,656 - WARNING - NaN/Inf loss detected at batch 799, skipping
2025-12-13 14:54:53,682 - WARNING - NaN/Inf loss detected at batch 800, skipping
2025-12-13 14:54:53,705 - WARNING - NaN/Inf loss detected at batch 801, skipping
2025-12-13 14:54:53,728 - WARNING - NaN/Inf loss detected at batch 802, skipping
2025-12-13 14:54:53,754 - WARNING - NaN/Inf loss detected at batch 803, skipping
2025-12-13 14:54:53,774 - WARNING - NaN/Inf loss detected at batch 804, skipping
2025-12-13 14:54:53,804 - WARNING - NaN/Inf loss detected at batch 805, skipping
2025-12-13 14:54:53,864 - WARNING - NaN/Inf loss detected at batch 807, skipping
2025-12-13 14:54:53,884 - WARNING - NaN/Inf loss detected at batch 808, skipping
2025-12-13 14:54:53,907 - WARNING - NaN/Inf loss detected at batch 809, skipping
2025-12-13 14:54:53,928 - WARNING - NaN/Inf loss detected at batch 810, skipping
2025-12-13 14:54:53,950 - WARNING - NaN/Inf loss detected at batch 811, skipping
2025-12-13 14:54:53,973 - WARNING - NaN/Inf loss detected at batch 812, skipping
2025-12-13 14:54:54,033 - WARNING - NaN/Inf loss detected at batch 814, skipping
2025-12-13 14:54:54,056 - WARNING - NaN/Inf loss detected at batch 815, skipping
2025-12-13 14:54:54,076 - WARNING - NaN/Inf loss detected at batch 816, skipping
2025-12-13 14:54:54,177 - WARNING - NaN/Inf loss detected at batch 819, skipping
2025-12-13 14:54:54,200 - WARNING - NaN/Inf loss detected at batch 820, skipping
2025-12-13 14:54:54,224 - WARNING - NaN/Inf loss detected at batch 821, skipping
2025-12-13 14:54:54,247 - WARNING - NaN/Inf loss detected at batch 822, skipping
2025-12-13 14:54:54,269 - WARNING - NaN/Inf loss detected at batch 823, skipping
2025-12-13 14:54:54,369 - WARNING - NaN/Inf loss detected at batch 826, skipping
2025-12-13 14:54:54,393 - WARNING - NaN/Inf loss detected at batch 827, skipping
2025-12-13 14:54:54,417 - WARNING - NaN/Inf loss detected at batch 828, skipping
2025-12-13 14:54:54,520 - WARNING - NaN/Inf loss detected at batch 831, skipping
2025-12-13 14:54:54,542 - WARNING - NaN/Inf loss detected at batch 832, skipping
2025-12-13 14:54:54,565 - WARNING - NaN/Inf loss detected at batch 833, skipping
2025-12-13 14:54:54,588 - WARNING - NaN/Inf loss detected at batch 834, skipping
2025-12-13 14:54:54,612 - WARNING - NaN/Inf loss detected at batch 835, skipping
2025-12-13 14:54:54,637 - WARNING - NaN/Inf loss detected at batch 836, skipping
2025-12-13 14:54:54,660 - WARNING - NaN/Inf loss detected at batch 837, skipping
2025-12-13 14:54:54,721 - WARNING - NaN/Inf loss detected at batch 839, skipping
2025-12-13 14:54:54,821 - WARNING - NaN/Inf loss detected at batch 842, skipping
2025-12-13 14:54:54,844 - WARNING - NaN/Inf loss detected at batch 843, skipping
2025-12-13 14:54:54,866 - WARNING - NaN/Inf loss detected at batch 844, skipping
2025-12-13 14:54:54,894 - WARNING - NaN/Inf loss detected at batch 845, skipping
2025-12-13 14:54:54,918 - WARNING - NaN/Inf loss detected at batch 846, skipping
2025-12-13 14:54:54,946 - WARNING - NaN/Inf loss detected at batch 847, skipping
2025-12-13 14:54:54,969 - WARNING - NaN/Inf loss detected at batch 848, skipping
2025-12-13 14:54:54,994 - WARNING - NaN/Inf loss detected at batch 849, skipping
2025-12-13 14:54:55,020 - WARNING - NaN/Inf loss detected at batch 850, skipping
2025-12-13 14:54:55,041 - WARNING - NaN/Inf loss detected at batch 851, skipping
2025-12-13 14:54:55,064 - WARNING - NaN/Inf loss detected at batch 852, skipping
2025-12-13 14:54:55,088 - WARNING - NaN/Inf loss detected at batch 853, skipping
2025-12-13 14:54:55,111 - WARNING - NaN/Inf loss detected at batch 854, skipping
2025-12-13 14:54:55,133 - WARNING - NaN/Inf loss detected at batch 855, skipping
2025-12-13 14:54:55,155 - WARNING - NaN/Inf loss detected at batch 856, skipping
2025-12-13 14:54:55,277 - WARNING - NaN/Inf loss detected at batch 859, skipping
2025-12-13 14:54:55,300 - WARNING - NaN/Inf loss detected at batch 860, skipping
2025-12-13 14:54:55,322 - WARNING - NaN/Inf loss detected at batch 861, skipping
2025-12-13 14:54:55,343 - WARNING - NaN/Inf loss detected at batch 862, skipping
2025-12-13 14:54:55,366 - WARNING - NaN/Inf loss detected at batch 863, skipping
2025-12-13 14:54:55,390 - WARNING - NaN/Inf loss detected at batch 864, skipping
2025-12-13 14:54:55,427 - WARNING - NaN/Inf loss detected at batch 865, skipping
2025-12-13 14:54:55,451 - WARNING - NaN/Inf loss detected at batch 866, skipping
2025-12-13 14:54:55,512 - WARNING - NaN/Inf loss detected at batch 868, skipping
2025-12-13 14:54:55,534 - WARNING - NaN/Inf loss detected at batch 869, skipping
2025-12-13 14:54:55,556 - WARNING - NaN/Inf loss detected at batch 870, skipping
2025-12-13 14:54:55,580 - WARNING - NaN/Inf loss detected at batch 871, skipping
2025-12-13 14:54:55,605 - WARNING - NaN/Inf loss detected at batch 872, skipping
2025-12-13 14:54:55,622 - WARNING - NaN/Inf loss detected at batch 873, skipping
2025-12-13 14:54:55,647 - WARNING - NaN/Inf loss detected at batch 874, skipping
2025-12-13 14:54:55,670 - WARNING - NaN/Inf loss detected at batch 875, skipping
2025-12-13 14:54:55,692 - WARNING - NaN/Inf loss detected at batch 876, skipping
2025-12-13 14:54:55,715 - WARNING - NaN/Inf loss detected at batch 877, skipping
2025-12-13 14:54:55,738 - WARNING - NaN/Inf loss detected at batch 878, skipping
2025-12-13 14:54:55,761 - WARNING - NaN/Inf loss detected at batch 879, skipping
2025-12-13 14:54:55,781 - WARNING - NaN/Inf loss detected at batch 880, skipping
2025-12-13 14:54:55,803 - WARNING - NaN/Inf loss detected at batch 881, skipping
2025-12-13 14:54:55,823 - WARNING - NaN/Inf loss detected at batch 882, skipping
2025-12-13 14:54:55,846 - WARNING - NaN/Inf loss detected at batch 883, skipping
2025-12-13 14:54:55,868 - WARNING - NaN/Inf loss detected at batch 884, skipping
2025-12-13 14:54:55,887 - WARNING - NaN/Inf loss detected at batch 885, skipping
2025-12-13 14:54:55,910 - WARNING - NaN/Inf loss detected at batch 886, skipping
2025-12-13 14:54:55,931 - WARNING - NaN/Inf loss detected at batch 887, skipping
2025-12-13 14:54:55,953 - WARNING - NaN/Inf loss detected at batch 888, skipping
2025-12-13 14:54:56,017 - WARNING - NaN/Inf loss detected at batch 890, skipping
2025-12-13 14:54:56,046 - WARNING - NaN/Inf loss detected at batch 891, skipping
2025-12-13 14:54:56,070 - WARNING - NaN/Inf loss detected at batch 892, skipping
2025-12-13 14:54:56,091 - WARNING - NaN/Inf loss detected at batch 893, skipping
2025-12-13 14:54:56,116 - WARNING - NaN/Inf loss detected at batch 894, skipping
2025-12-13 14:54:56,137 - WARNING - NaN/Inf loss detected at batch 895, skipping
2025-12-13 14:54:56,159 - WARNING - NaN/Inf loss detected at batch 896, skipping
2025-12-13 14:54:56,182 - WARNING - NaN/Inf loss detected at batch 897, skipping
2025-12-13 14:54:56,204 - WARNING - NaN/Inf loss detected at batch 898, skipping
2025-12-13 14:54:56,227 - WARNING - NaN/Inf loss detected at batch 899, skipping
2025-12-13 14:54:56,268 - INFO - Epoch [1] Batch [900/7250] loss: 6.1776 
2025-12-13 14:54:56,290 - WARNING - NaN/Inf loss detected at batch 901, skipping
2025-12-13 14:54:56,309 - WARNING - NaN/Inf loss detected at batch 902, skipping
2025-12-13 14:54:56,330 - WARNING - NaN/Inf loss detected at batch 903, skipping
2025-12-13 14:54:56,351 - WARNING - NaN/Inf loss detected at batch 904, skipping
2025-12-13 14:54:56,376 - WARNING - NaN/Inf loss detected at batch 905, skipping
2025-12-13 14:54:56,399 - WARNING - NaN/Inf loss detected at batch 906, skipping
2025-12-13 14:54:56,422 - WARNING - NaN/Inf loss detected at batch 907, skipping
2025-12-13 14:54:56,442 - WARNING - NaN/Inf loss detected at batch 908, skipping
2025-12-13 14:54:56,467 - WARNING - NaN/Inf loss detected at batch 909, skipping
2025-12-13 14:54:56,493 - WARNING - NaN/Inf loss detected at batch 910, skipping
2025-12-13 14:54:56,514 - WARNING - NaN/Inf loss detected at batch 911, skipping
2025-12-13 14:54:56,536 - WARNING - NaN/Inf loss detected at batch 912, skipping
2025-12-13 14:54:56,558 - WARNING - NaN/Inf loss detected at batch 913, skipping
2025-12-13 14:54:56,582 - WARNING - NaN/Inf loss detected at batch 914, skipping
2025-12-13 14:54:56,610 - WARNING - NaN/Inf loss detected at batch 915, skipping
2025-12-13 14:54:56,636 - WARNING - NaN/Inf loss detected at batch 916, skipping
2025-12-13 14:54:56,657 - WARNING - NaN/Inf loss detected at batch 917, skipping
2025-12-13 14:54:56,679 - WARNING - NaN/Inf loss detected at batch 918, skipping
2025-12-13 14:54:56,701 - WARNING - NaN/Inf loss detected at batch 919, skipping
2025-12-13 14:54:56,725 - WARNING - NaN/Inf loss detected at batch 920, skipping
2025-12-13 14:54:56,747 - WARNING - NaN/Inf loss detected at batch 921, skipping
2025-12-13 14:54:56,769 - WARNING - NaN/Inf loss detected at batch 922, skipping
2025-12-13 14:54:56,833 - WARNING - NaN/Inf loss detected at batch 924, skipping
2025-12-13 14:54:56,929 - WARNING - NaN/Inf loss detected at batch 927, skipping
2025-12-13 14:54:56,952 - WARNING - NaN/Inf loss detected at batch 928, skipping
2025-12-13 14:54:56,975 - WARNING - NaN/Inf loss detected at batch 929, skipping
2025-12-13 14:54:56,998 - WARNING - NaN/Inf loss detected at batch 930, skipping
2025-12-13 14:54:57,023 - WARNING - NaN/Inf loss detected at batch 931, skipping
2025-12-13 14:54:57,043 - WARNING - NaN/Inf loss detected at batch 932, skipping
2025-12-13 14:54:57,108 - WARNING - NaN/Inf loss detected at batch 934, skipping
2025-12-13 14:54:57,128 - WARNING - NaN/Inf loss detected at batch 935, skipping
2025-12-13 14:54:57,151 - WARNING - NaN/Inf loss detected at batch 936, skipping
2025-12-13 14:54:57,172 - WARNING - NaN/Inf loss detected at batch 937, skipping
2025-12-13 14:54:57,195 - WARNING - NaN/Inf loss detected at batch 938, skipping
2025-12-13 14:54:57,221 - WARNING - NaN/Inf loss detected at batch 939, skipping
2025-12-13 14:54:57,240 - WARNING - NaN/Inf loss detected at batch 940, skipping
2025-12-13 14:54:57,265 - WARNING - NaN/Inf loss detected at batch 941, skipping
2025-12-13 14:54:57,284 - WARNING - NaN/Inf loss detected at batch 942, skipping
2025-12-13 14:54:57,307 - WARNING - NaN/Inf loss detected at batch 943, skipping
2025-12-13 14:54:57,333 - WARNING - NaN/Inf loss detected at batch 944, skipping
2025-12-13 14:54:57,356 - WARNING - NaN/Inf loss detected at batch 945, skipping
2025-12-13 14:54:57,378 - WARNING - NaN/Inf loss detected at batch 946, skipping
2025-12-13 14:54:57,401 - WARNING - NaN/Inf loss detected at batch 947, skipping
2025-12-13 14:54:57,422 - WARNING - NaN/Inf loss detected at batch 948, skipping
2025-12-13 14:54:57,445 - WARNING - NaN/Inf loss detected at batch 949, skipping
2025-12-13 14:54:57,466 - WARNING - NaN/Inf loss detected at batch 950, skipping
2025-12-13 14:54:57,490 - WARNING - NaN/Inf loss detected at batch 951, skipping
2025-12-13 14:54:57,511 - WARNING - NaN/Inf loss detected at batch 952, skipping
2025-12-13 14:54:57,535 - WARNING - NaN/Inf loss detected at batch 953, skipping
2025-12-13 14:54:57,558 - WARNING - NaN/Inf loss detected at batch 954, skipping
2025-12-13 14:54:57,580 - WARNING - NaN/Inf loss detected at batch 955, skipping
2025-12-13 14:54:57,603 - WARNING - NaN/Inf loss detected at batch 956, skipping
2025-12-13 14:54:57,624 - WARNING - NaN/Inf loss detected at batch 957, skipping
2025-12-13 14:54:57,688 - WARNING - NaN/Inf loss detected at batch 959, skipping
2025-12-13 14:54:57,710 - WARNING - NaN/Inf loss detected at batch 960, skipping
2025-12-13 14:54:57,737 - WARNING - NaN/Inf loss detected at batch 961, skipping
2025-12-13 14:54:57,756 - WARNING - NaN/Inf loss detected at batch 962, skipping
2025-12-13 14:54:57,818 - WARNING - NaN/Inf loss detected at batch 964, skipping
2025-12-13 14:54:57,842 - WARNING - NaN/Inf loss detected at batch 965, skipping
2025-12-13 14:54:57,903 - WARNING - NaN/Inf loss detected at batch 967, skipping
2025-12-13 14:54:57,927 - WARNING - NaN/Inf loss detected at batch 968, skipping
2025-12-13 14:54:57,949 - WARNING - NaN/Inf loss detected at batch 969, skipping
2025-12-13 14:54:57,970 - WARNING - NaN/Inf loss detected at batch 970, skipping
2025-12-13 14:54:57,994 - WARNING - NaN/Inf loss detected at batch 971, skipping
2025-12-13 14:54:58,080 - WARNING - NaN/Inf loss detected at batch 973, skipping
2025-12-13 14:54:58,104 - WARNING - NaN/Inf loss detected at batch 974, skipping
2025-12-13 14:54:58,125 - WARNING - NaN/Inf loss detected at batch 975, skipping
2025-12-13 14:54:58,147 - WARNING - NaN/Inf loss detected at batch 976, skipping
2025-12-13 14:54:58,170 - WARNING - NaN/Inf loss detected at batch 977, skipping
2025-12-13 14:54:58,192 - WARNING - NaN/Inf loss detected at batch 978, skipping
2025-12-13 14:54:58,215 - WARNING - NaN/Inf loss detected at batch 979, skipping
2025-12-13 14:54:58,237 - WARNING - NaN/Inf loss detected at batch 980, skipping
2025-12-13 14:54:58,258 - WARNING - NaN/Inf loss detected at batch 981, skipping
2025-12-13 14:54:58,281 - WARNING - NaN/Inf loss detected at batch 982, skipping
2025-12-13 14:54:58,302 - WARNING - NaN/Inf loss detected at batch 983, skipping
2025-12-13 14:54:58,324 - WARNING - NaN/Inf loss detected at batch 984, skipping
2025-12-13 14:54:58,345 - WARNING - NaN/Inf loss detected at batch 985, skipping
2025-12-13 14:54:58,414 - WARNING - NaN/Inf loss detected at batch 987, skipping
2025-12-13 14:54:58,435 - WARNING - NaN/Inf loss detected at batch 988, skipping
2025-12-13 14:54:58,457 - WARNING - NaN/Inf loss detected at batch 989, skipping
2025-12-13 14:54:58,482 - WARNING - NaN/Inf loss detected at batch 990, skipping
2025-12-13 14:54:58,509 - WARNING - NaN/Inf loss detected at batch 991, skipping
2025-12-13 14:54:58,531 - WARNING - NaN/Inf loss detected at batch 992, skipping
2025-12-13 14:54:58,552 - WARNING - NaN/Inf loss detected at batch 993, skipping
2025-12-13 14:54:58,611 - WARNING - NaN/Inf loss detected at batch 995, skipping
2025-12-13 14:54:58,635 - WARNING - NaN/Inf loss detected at batch 996, skipping
2025-12-13 14:54:58,656 - WARNING - NaN/Inf loss detected at batch 997, skipping
2025-12-13 14:54:58,678 - WARNING - NaN/Inf loss detected at batch 998, skipping
2025-12-13 14:54:58,698 - WARNING - NaN/Inf loss detected at batch 999, skipping
2025-12-13 14:54:58,721 - WARNING - NaN/Inf loss detected at batch 1000, skipping
2025-12-13 14:54:58,743 - WARNING - NaN/Inf loss detected at batch 1001, skipping
2025-12-13 14:54:58,765 - WARNING - NaN/Inf loss detected at batch 1002, skipping
2025-12-13 14:54:58,785 - WARNING - NaN/Inf loss detected at batch 1003, skipping
2025-12-13 14:54:58,809 - WARNING - NaN/Inf loss detected at batch 1004, skipping
2025-12-13 14:54:58,830 - WARNING - NaN/Inf loss detected at batch 1005, skipping
2025-12-13 14:54:58,931 - WARNING - NaN/Inf loss detected at batch 1008, skipping
2025-12-13 14:54:58,955 - WARNING - NaN/Inf loss detected at batch 1009, skipping
2025-12-13 14:54:58,976 - WARNING - NaN/Inf loss detected at batch 1010, skipping
2025-12-13 14:54:59,000 - WARNING - NaN/Inf loss detected at batch 1011, skipping
2025-12-13 14:54:59,024 - WARNING - NaN/Inf loss detected at batch 1012, skipping
2025-12-13 14:54:59,082 - WARNING - NaN/Inf loss detected at batch 1014, skipping
2025-12-13 14:54:59,102 - WARNING - NaN/Inf loss detected at batch 1015, skipping
2025-12-13 14:54:59,124 - WARNING - NaN/Inf loss detected at batch 1016, skipping
2025-12-13 14:54:59,190 - WARNING - NaN/Inf loss detected at batch 1018, skipping
2025-12-13 14:54:59,216 - WARNING - NaN/Inf loss detected at batch 1019, skipping
2025-12-13 14:54:59,239 - WARNING - NaN/Inf loss detected at batch 1020, skipping
2025-12-13 14:54:59,260 - WARNING - NaN/Inf loss detected at batch 1021, skipping
2025-12-13 14:54:59,281 - WARNING - NaN/Inf loss detected at batch 1022, skipping
2025-12-13 14:54:59,302 - WARNING - NaN/Inf loss detected at batch 1023, skipping
2025-12-13 14:54:59,324 - WARNING - NaN/Inf loss detected at batch 1024, skipping
2025-12-13 14:54:59,349 - WARNING - NaN/Inf loss detected at batch 1025, skipping
2025-12-13 14:54:59,379 - WARNING - NaN/Inf loss detected at batch 1026, skipping
2025-12-13 14:54:59,402 - WARNING - NaN/Inf loss detected at batch 1027, skipping
2025-12-13 14:54:59,424 - WARNING - NaN/Inf loss detected at batch 1028, skipping
2025-12-13 14:54:59,447 - WARNING - NaN/Inf loss detected at batch 1029, skipping
2025-12-13 14:54:59,468 - WARNING - NaN/Inf loss detected at batch 1030, skipping
2025-12-13 14:54:59,562 - WARNING - NaN/Inf loss detected at batch 1032, skipping
2025-12-13 14:54:59,585 - WARNING - NaN/Inf loss detected at batch 1033, skipping
2025-12-13 14:54:59,607 - WARNING - NaN/Inf loss detected at batch 1034, skipping
2025-12-13 14:54:59,631 - WARNING - NaN/Inf loss detected at batch 1035, skipping
2025-12-13 14:54:59,652 - WARNING - NaN/Inf loss detected at batch 1036, skipping
2025-12-13 14:54:59,673 - WARNING - NaN/Inf loss detected at batch 1037, skipping
2025-12-13 14:54:59,696 - WARNING - NaN/Inf loss detected at batch 1038, skipping
2025-12-13 14:54:59,718 - WARNING - NaN/Inf loss detected at batch 1039, skipping
2025-12-13 14:54:59,740 - WARNING - NaN/Inf loss detected at batch 1040, skipping
2025-12-13 14:54:59,764 - WARNING - NaN/Inf loss detected at batch 1041, skipping
2025-12-13 14:54:59,785 - WARNING - NaN/Inf loss detected at batch 1042, skipping
2025-12-13 14:54:59,809 - WARNING - NaN/Inf loss detected at batch 1043, skipping
2025-12-13 14:54:59,830 - WARNING - NaN/Inf loss detected at batch 1044, skipping
2025-12-13 14:54:59,852 - WARNING - NaN/Inf loss detected at batch 1045, skipping
2025-12-13 14:54:59,876 - WARNING - NaN/Inf loss detected at batch 1046, skipping
2025-12-13 14:54:59,936 - WARNING - NaN/Inf loss detected at batch 1048, skipping
2025-12-13 14:54:59,958 - WARNING - NaN/Inf loss detected at batch 1049, skipping
2025-12-13 14:54:59,979 - WARNING - NaN/Inf loss detected at batch 1050, skipping
2025-12-13 14:55:00,038 - WARNING - NaN/Inf loss detected at batch 1052, skipping
2025-12-13 14:55:00,059 - WARNING - NaN/Inf loss detected at batch 1053, skipping
2025-12-13 14:55:00,082 - WARNING - NaN/Inf loss detected at batch 1054, skipping
2025-12-13 14:55:00,143 - WARNING - NaN/Inf loss detected at batch 1056, skipping
2025-12-13 14:55:00,177 - WARNING - NaN/Inf loss detected at batch 1057, skipping
2025-12-13 14:55:00,198 - WARNING - NaN/Inf loss detected at batch 1058, skipping
2025-12-13 14:55:00,219 - WARNING - NaN/Inf loss detected at batch 1059, skipping
2025-12-13 14:55:00,240 - WARNING - NaN/Inf loss detected at batch 1060, skipping
2025-12-13 14:55:00,264 - WARNING - NaN/Inf loss detected at batch 1061, skipping
2025-12-13 14:55:00,283 - WARNING - NaN/Inf loss detected at batch 1062, skipping
2025-12-13 14:55:00,309 - WARNING - NaN/Inf loss detected at batch 1063, skipping
2025-12-13 14:55:00,330 - WARNING - NaN/Inf loss detected at batch 1064, skipping
2025-12-13 14:55:00,353 - WARNING - NaN/Inf loss detected at batch 1065, skipping
2025-12-13 14:55:00,377 - WARNING - NaN/Inf loss detected at batch 1066, skipping
2025-12-13 14:55:00,398 - WARNING - NaN/Inf loss detected at batch 1067, skipping
2025-12-13 14:55:00,422 - WARNING - NaN/Inf loss detected at batch 1068, skipping
2025-12-13 14:55:00,444 - WARNING - NaN/Inf loss detected at batch 1069, skipping
2025-12-13 14:55:00,467 - WARNING - NaN/Inf loss detected at batch 1070, skipping
2025-12-13 14:55:00,526 - WARNING - NaN/Inf loss detected at batch 1072, skipping
2025-12-13 14:55:00,548 - WARNING - NaN/Inf loss detected at batch 1073, skipping
2025-12-13 14:55:00,571 - WARNING - NaN/Inf loss detected at batch 1074, skipping
2025-12-13 14:55:00,596 - WARNING - NaN/Inf loss detected at batch 1075, skipping
2025-12-13 14:55:00,618 - WARNING - NaN/Inf loss detected at batch 1076, skipping
2025-12-13 14:55:00,643 - WARNING - NaN/Inf loss detected at batch 1077, skipping
2025-12-13 14:55:00,665 - WARNING - NaN/Inf loss detected at batch 1078, skipping
2025-12-13 14:55:00,694 - WARNING - NaN/Inf loss detected at batch 1079, skipping
2025-12-13 14:55:00,717 - WARNING - NaN/Inf loss detected at batch 1080, skipping
2025-12-13 14:55:00,778 - WARNING - NaN/Inf loss detected at batch 1082, skipping
2025-12-13 14:55:00,798 - WARNING - NaN/Inf loss detected at batch 1083, skipping
2025-12-13 14:55:00,823 - WARNING - NaN/Inf loss detected at batch 1084, skipping
2025-12-13 14:55:00,845 - WARNING - NaN/Inf loss detected at batch 1085, skipping
2025-12-13 14:55:00,869 - WARNING - NaN/Inf loss detected at batch 1086, skipping
2025-12-13 14:55:00,898 - WARNING - NaN/Inf loss detected at batch 1087, skipping
2025-12-13 14:55:00,919 - WARNING - NaN/Inf loss detected at batch 1088, skipping
2025-12-13 14:55:00,943 - WARNING - NaN/Inf loss detected at batch 1089, skipping
2025-12-13 14:55:00,966 - WARNING - NaN/Inf loss detected at batch 1090, skipping
2025-12-13 14:55:00,988 - WARNING - NaN/Inf loss detected at batch 1091, skipping
2025-12-13 14:55:01,011 - WARNING - NaN/Inf loss detected at batch 1092, skipping
2025-12-13 14:55:01,032 - WARNING - NaN/Inf loss detected at batch 1093, skipping
2025-12-13 14:55:01,054 - WARNING - NaN/Inf loss detected at batch 1094, skipping
2025-12-13 14:55:01,077 - WARNING - NaN/Inf loss detected at batch 1095, skipping
2025-12-13 14:55:01,098 - WARNING - NaN/Inf loss detected at batch 1096, skipping
2025-12-13 14:55:01,121 - WARNING - NaN/Inf loss detected at batch 1097, skipping
2025-12-13 14:55:01,142 - WARNING - NaN/Inf loss detected at batch 1098, skipping
2025-12-13 14:55:01,162 - WARNING - NaN/Inf loss detected at batch 1099, skipping
2025-12-13 14:55:01,200 - INFO - Epoch [1] Batch [1100/7250] loss: 6.0016 
2025-12-13 14:55:01,221 - WARNING - NaN/Inf loss detected at batch 1101, skipping
2025-12-13 14:55:01,241 - WARNING - NaN/Inf loss detected at batch 1102, skipping
2025-12-13 14:55:01,263 - WARNING - NaN/Inf loss detected at batch 1103, skipping
2025-12-13 14:55:01,286 - WARNING - NaN/Inf loss detected at batch 1104, skipping
2025-12-13 14:55:01,305 - WARNING - NaN/Inf loss detected at batch 1105, skipping
2025-12-13 14:55:01,327 - WARNING - NaN/Inf loss detected at batch 1106, skipping
2025-12-13 14:55:01,415 - WARNING - NaN/Inf loss detected at batch 1108, skipping
2025-12-13 14:55:01,435 - WARNING - NaN/Inf loss detected at batch 1109, skipping
2025-12-13 14:55:01,540 - WARNING - NaN/Inf loss detected at batch 1112, skipping
2025-12-13 14:55:01,562 - WARNING - NaN/Inf loss detected at batch 1113, skipping
2025-12-13 14:55:01,583 - WARNING - NaN/Inf loss detected at batch 1114, skipping
2025-12-13 14:55:01,605 - WARNING - NaN/Inf loss detected at batch 1115, skipping
2025-12-13 14:55:01,626 - WARNING - NaN/Inf loss detected at batch 1116, skipping
2025-12-13 14:55:01,686 - WARNING - NaN/Inf loss detected at batch 1118, skipping
2025-12-13 14:55:01,706 - WARNING - NaN/Inf loss detected at batch 1119, skipping
2025-12-13 14:55:01,732 - WARNING - NaN/Inf loss detected at batch 1120, skipping
2025-12-13 14:55:01,754 - WARNING - NaN/Inf loss detected at batch 1121, skipping
2025-12-13 14:55:01,779 - WARNING - NaN/Inf loss detected at batch 1122, skipping
2025-12-13 14:55:01,799 - WARNING - NaN/Inf loss detected at batch 1123, skipping
2025-12-13 14:55:01,820 - WARNING - NaN/Inf loss detected at batch 1124, skipping
2025-12-13 14:55:01,843 - WARNING - NaN/Inf loss detected at batch 1125, skipping
2025-12-13 14:55:01,866 - WARNING - NaN/Inf loss detected at batch 1126, skipping
2025-12-13 14:55:01,928 - WARNING - NaN/Inf loss detected at batch 1128, skipping
2025-12-13 14:55:01,949 - WARNING - NaN/Inf loss detected at batch 1129, skipping
2025-12-13 14:55:01,972 - WARNING - NaN/Inf loss detected at batch 1130, skipping
2025-12-13 14:55:02,029 - WARNING - NaN/Inf loss detected at batch 1132, skipping
2025-12-13 14:55:02,051 - WARNING - NaN/Inf loss detected at batch 1133, skipping
2025-12-13 14:55:02,074 - WARNING - NaN/Inf loss detected at batch 1134, skipping
2025-12-13 14:55:02,096 - WARNING - NaN/Inf loss detected at batch 1135, skipping
2025-12-13 14:55:02,116 - WARNING - NaN/Inf loss detected at batch 1136, skipping
2025-12-13 14:55:02,137 - WARNING - NaN/Inf loss detected at batch 1137, skipping
2025-12-13 14:55:02,160 - WARNING - NaN/Inf loss detected at batch 1138, skipping
2025-12-13 14:55:02,181 - WARNING - NaN/Inf loss detected at batch 1139, skipping
2025-12-13 14:55:02,205 - WARNING - NaN/Inf loss detected at batch 1140, skipping
2025-12-13 14:55:02,227 - WARNING - NaN/Inf loss detected at batch 1141, skipping
2025-12-13 14:55:02,248 - WARNING - NaN/Inf loss detected at batch 1142, skipping
2025-12-13 14:55:02,311 - WARNING - NaN/Inf loss detected at batch 1144, skipping
2025-12-13 14:55:02,334 - WARNING - NaN/Inf loss detected at batch 1145, skipping
2025-12-13 14:55:02,355 - WARNING - NaN/Inf loss detected at batch 1146, skipping
2025-12-13 14:55:02,377 - WARNING - NaN/Inf loss detected at batch 1147, skipping
2025-12-13 14:55:02,405 - WARNING - NaN/Inf loss detected at batch 1148, skipping
2025-12-13 14:55:02,426 - WARNING - NaN/Inf loss detected at batch 1149, skipping
2025-12-13 14:55:02,447 - WARNING - NaN/Inf loss detected at batch 1150, skipping
2025-12-13 14:55:02,511 - WARNING - NaN/Inf loss detected at batch 1152, skipping
2025-12-13 14:55:02,531 - WARNING - NaN/Inf loss detected at batch 1153, skipping
2025-12-13 14:55:02,552 - WARNING - NaN/Inf loss detected at batch 1154, skipping
2025-12-13 14:55:02,575 - WARNING - NaN/Inf loss detected at batch 1155, skipping
2025-12-13 14:55:02,595 - WARNING - NaN/Inf loss detected at batch 1156, skipping
2025-12-13 14:55:02,619 - WARNING - NaN/Inf loss detected at batch 1157, skipping
2025-12-13 14:55:02,642 - WARNING - NaN/Inf loss detected at batch 1158, skipping
2025-12-13 14:55:02,662 - WARNING - NaN/Inf loss detected at batch 1159, skipping
2025-12-13 14:55:02,684 - WARNING - NaN/Inf loss detected at batch 1160, skipping
2025-12-13 14:55:02,709 - WARNING - NaN/Inf loss detected at batch 1161, skipping
2025-12-13 14:55:02,733 - WARNING - NaN/Inf loss detected at batch 1162, skipping
2025-12-13 14:55:02,761 - WARNING - NaN/Inf loss detected at batch 1163, skipping
2025-12-13 14:55:02,782 - WARNING - NaN/Inf loss detected at batch 1164, skipping
2025-12-13 14:55:02,804 - WARNING - NaN/Inf loss detected at batch 1165, skipping
2025-12-13 14:55:02,828 - WARNING - NaN/Inf loss detected at batch 1166, skipping
2025-12-13 14:55:02,851 - WARNING - NaN/Inf loss detected at batch 1167, skipping
2025-12-13 14:55:02,873 - WARNING - NaN/Inf loss detected at batch 1168, skipping
2025-12-13 14:55:02,893 - WARNING - NaN/Inf loss detected at batch 1169, skipping
2025-12-13 14:55:02,920 - WARNING - NaN/Inf loss detected at batch 1170, skipping
2025-12-13 14:55:02,942 - WARNING - NaN/Inf loss detected at batch 1171, skipping
2025-12-13 14:55:02,968 - WARNING - NaN/Inf loss detected at batch 1172, skipping
2025-12-13 14:55:02,990 - WARNING - NaN/Inf loss detected at batch 1173, skipping
2025-12-13 14:55:03,009 - WARNING - NaN/Inf loss detected at batch 1174, skipping
2025-12-13 14:55:03,032 - WARNING - NaN/Inf loss detected at batch 1175, skipping
2025-12-13 14:55:03,054 - WARNING - NaN/Inf loss detected at batch 1176, skipping
2025-12-13 14:55:03,076 - WARNING - NaN/Inf loss detected at batch 1177, skipping
2025-12-13 14:55:03,109 - WARNING - NaN/Inf loss detected at batch 1178, skipping
2025-12-13 14:55:03,209 - WARNING - NaN/Inf loss detected at batch 1181, skipping
2025-12-13 14:55:03,231 - WARNING - NaN/Inf loss detected at batch 1182, skipping
2025-12-13 14:55:03,253 - WARNING - NaN/Inf loss detected at batch 1183, skipping
2025-12-13 14:55:03,277 - WARNING - NaN/Inf loss detected at batch 1184, skipping
2025-12-13 14:55:03,298 - WARNING - NaN/Inf loss detected at batch 1185, skipping
2025-12-13 14:55:03,320 - WARNING - NaN/Inf loss detected at batch 1186, skipping
2025-12-13 14:55:03,376 - WARNING - NaN/Inf loss detected at batch 1188, skipping
2025-12-13 14:55:03,398 - WARNING - NaN/Inf loss detected at batch 1189, skipping
2025-12-13 14:55:03,455 - WARNING - NaN/Inf loss detected at batch 1191, skipping
2025-12-13 14:55:03,478 - WARNING - NaN/Inf loss detected at batch 1192, skipping
2025-12-13 14:55:03,500 - WARNING - NaN/Inf loss detected at batch 1193, skipping
2025-12-13 14:55:03,523 - WARNING - NaN/Inf loss detected at batch 1194, skipping
2025-12-13 14:55:03,544 - WARNING - NaN/Inf loss detected at batch 1195, skipping
2025-12-13 14:55:03,566 - WARNING - NaN/Inf loss detected at batch 1196, skipping
2025-12-13 14:55:03,657 - WARNING - NaN/Inf loss detected at batch 1198, skipping
2025-12-13 14:55:03,679 - WARNING - NaN/Inf loss detected at batch 1199, skipping
2025-12-13 14:55:03,702 - WARNING - NaN/Inf loss detected at batch 1200, skipping
2025-12-13 14:55:03,758 - WARNING - NaN/Inf loss detected at batch 1202, skipping
2025-12-13 14:55:03,779 - WARNING - NaN/Inf loss detected at batch 1203, skipping
2025-12-13 14:55:03,801 - WARNING - NaN/Inf loss detected at batch 1204, skipping
2025-12-13 14:55:03,823 - WARNING - NaN/Inf loss detected at batch 1205, skipping
2025-12-13 14:55:03,883 - WARNING - NaN/Inf loss detected at batch 1207, skipping
2025-12-13 14:55:03,905 - WARNING - NaN/Inf loss detected at batch 1208, skipping
2025-12-13 14:55:03,926 - WARNING - NaN/Inf loss detected at batch 1209, skipping
2025-12-13 14:55:03,946 - WARNING - NaN/Inf loss detected at batch 1210, skipping
2025-12-13 14:55:03,972 - WARNING - NaN/Inf loss detected at batch 1211, skipping
2025-12-13 14:55:03,994 - WARNING - NaN/Inf loss detected at batch 1212, skipping
2025-12-13 14:55:04,017 - WARNING - NaN/Inf loss detected at batch 1213, skipping
2025-12-13 14:55:04,043 - WARNING - NaN/Inf loss detected at batch 1214, skipping
2025-12-13 14:55:04,062 - WARNING - NaN/Inf loss detected at batch 1215, skipping
2025-12-13 14:55:04,086 - WARNING - NaN/Inf loss detected at batch 1216, skipping
2025-12-13 14:55:04,108 - WARNING - NaN/Inf loss detected at batch 1217, skipping
2025-12-13 14:55:04,130 - WARNING - NaN/Inf loss detected at batch 1218, skipping
2025-12-13 14:55:04,210 - WARNING - NaN/Inf loss detected at batch 1220, skipping
2025-12-13 14:55:04,232 - WARNING - NaN/Inf loss detected at batch 1221, skipping
2025-12-13 14:55:04,254 - WARNING - NaN/Inf loss detected at batch 1222, skipping
2025-12-13 14:55:04,279 - WARNING - NaN/Inf loss detected at batch 1223, skipping
2025-12-13 14:55:04,301 - WARNING - NaN/Inf loss detected at batch 1224, skipping
2025-12-13 14:55:04,320 - WARNING - NaN/Inf loss detected at batch 1225, skipping
2025-12-13 14:55:04,342 - WARNING - NaN/Inf loss detected at batch 1226, skipping
2025-12-13 14:55:04,363 - WARNING - NaN/Inf loss detected at batch 1227, skipping
2025-12-13 14:55:04,383 - WARNING - NaN/Inf loss detected at batch 1228, skipping
2025-12-13 14:55:04,405 - WARNING - NaN/Inf loss detected at batch 1229, skipping
2025-12-13 14:55:04,427 - WARNING - NaN/Inf loss detected at batch 1230, skipping
2025-12-13 14:55:04,450 - WARNING - NaN/Inf loss detected at batch 1231, skipping
2025-12-13 14:55:04,470 - WARNING - NaN/Inf loss detected at batch 1232, skipping
2025-12-13 14:55:04,527 - WARNING - NaN/Inf loss detected at batch 1234, skipping
2025-12-13 14:55:04,548 - WARNING - NaN/Inf loss detected at batch 1235, skipping
2025-12-13 14:55:04,570 - WARNING - NaN/Inf loss detected at batch 1236, skipping
2025-12-13 14:55:04,591 - WARNING - NaN/Inf loss detected at batch 1237, skipping
2025-12-13 14:55:04,613 - WARNING - NaN/Inf loss detected at batch 1238, skipping
2025-12-13 14:55:04,635 - WARNING - NaN/Inf loss detected at batch 1239, skipping
2025-12-13 14:55:04,657 - WARNING - NaN/Inf loss detected at batch 1240, skipping
2025-12-13 14:55:04,680 - WARNING - NaN/Inf loss detected at batch 1241, skipping
2025-12-13 14:55:04,741 - WARNING - NaN/Inf loss detected at batch 1243, skipping
2025-12-13 14:55:04,763 - WARNING - NaN/Inf loss detected at batch 1244, skipping
2025-12-13 14:55:04,786 - WARNING - NaN/Inf loss detected at batch 1245, skipping
2025-12-13 14:55:04,805 - WARNING - NaN/Inf loss detected at batch 1246, skipping
2025-12-13 14:55:04,826 - WARNING - NaN/Inf loss detected at batch 1247, skipping
2025-12-13 14:55:04,850 - WARNING - NaN/Inf loss detected at batch 1248, skipping
2025-12-13 14:55:04,872 - WARNING - NaN/Inf loss detected at batch 1249, skipping
2025-12-13 14:55:04,929 - WARNING - NaN/Inf loss detected at batch 1251, skipping
2025-12-13 14:55:04,952 - WARNING - NaN/Inf loss detected at batch 1252, skipping
2025-12-13 14:55:04,982 - WARNING - NaN/Inf loss detected at batch 1253, skipping
2025-12-13 14:55:05,004 - WARNING - NaN/Inf loss detected at batch 1254, skipping
2025-12-13 14:55:05,066 - WARNING - NaN/Inf loss detected at batch 1256, skipping
2025-12-13 14:55:05,090 - WARNING - NaN/Inf loss detected at batch 1257, skipping
2025-12-13 14:55:05,113 - WARNING - NaN/Inf loss detected at batch 1258, skipping
2025-12-13 14:55:05,134 - WARNING - NaN/Inf loss detected at batch 1259, skipping
2025-12-13 14:55:05,156 - WARNING - NaN/Inf loss detected at batch 1260, skipping
2025-12-13 14:55:05,179 - WARNING - NaN/Inf loss detected at batch 1261, skipping
2025-12-13 14:55:05,203 - WARNING - NaN/Inf loss detected at batch 1262, skipping
2025-12-13 14:55:05,227 - WARNING - NaN/Inf loss detected at batch 1263, skipping
2025-12-13 14:55:05,249 - WARNING - NaN/Inf loss detected at batch 1264, skipping
2025-12-13 14:55:05,326 - WARNING - NaN/Inf loss detected at batch 1266, skipping
2025-12-13 14:55:05,347 - WARNING - NaN/Inf loss detected at batch 1267, skipping
2025-12-13 14:55:05,368 - WARNING - NaN/Inf loss detected at batch 1268, skipping
2025-12-13 14:55:05,389 - WARNING - NaN/Inf loss detected at batch 1269, skipping
2025-12-13 14:55:05,412 - WARNING - NaN/Inf loss detected at batch 1270, skipping
2025-12-13 14:55:05,433 - WARNING - NaN/Inf loss detected at batch 1271, skipping
2025-12-13 14:55:05,456 - WARNING - NaN/Inf loss detected at batch 1272, skipping
2025-12-13 14:55:05,477 - WARNING - NaN/Inf loss detected at batch 1273, skipping
2025-12-13 14:55:05,500 - WARNING - NaN/Inf loss detected at batch 1274, skipping
2025-12-13 14:55:05,521 - WARNING - NaN/Inf loss detected at batch 1275, skipping
2025-12-13 14:55:05,543 - WARNING - NaN/Inf loss detected at batch 1276, skipping
2025-12-13 14:55:05,565 - WARNING - NaN/Inf loss detected at batch 1277, skipping
2025-12-13 14:55:05,590 - WARNING - NaN/Inf loss detected at batch 1278, skipping
2025-12-13 14:55:05,612 - WARNING - NaN/Inf loss detected at batch 1279, skipping
2025-12-13 14:55:05,634 - WARNING - NaN/Inf loss detected at batch 1280, skipping
2025-12-13 14:55:05,661 - WARNING - NaN/Inf loss detected at batch 1281, skipping
2025-12-13 14:55:05,771 - WARNING - NaN/Inf loss detected at batch 1284, skipping
2025-12-13 14:55:05,796 - WARNING - NaN/Inf loss detected at batch 1285, skipping
2025-12-13 14:55:05,817 - WARNING - NaN/Inf loss detected at batch 1286, skipping
2025-12-13 14:55:05,839 - WARNING - NaN/Inf loss detected at batch 1287, skipping
2025-12-13 14:55:05,860 - WARNING - NaN/Inf loss detected at batch 1288, skipping
2025-12-13 14:55:05,881 - WARNING - NaN/Inf loss detected at batch 1289, skipping
2025-12-13 14:55:05,907 - WARNING - NaN/Inf loss detected at batch 1290, skipping
2025-12-13 14:55:05,927 - WARNING - NaN/Inf loss detected at batch 1291, skipping
2025-12-13 14:55:05,948 - WARNING - NaN/Inf loss detected at batch 1292, skipping
2025-12-13 14:55:05,969 - WARNING - NaN/Inf loss detected at batch 1293, skipping
2025-12-13 14:55:05,991 - WARNING - NaN/Inf loss detected at batch 1294, skipping
2025-12-13 14:55:06,012 - WARNING - NaN/Inf loss detected at batch 1295, skipping
2025-12-13 14:55:06,035 - WARNING - NaN/Inf loss detected at batch 1296, skipping
2025-12-13 14:55:06,057 - WARNING - NaN/Inf loss detected at batch 1297, skipping
2025-12-13 14:55:06,077 - WARNING - NaN/Inf loss detected at batch 1298, skipping
2025-12-13 14:55:06,100 - WARNING - NaN/Inf loss detected at batch 1299, skipping
2025-12-13 14:55:06,124 - WARNING - NaN/Inf loss detected at batch 1300, skipping
2025-12-13 14:55:06,146 - WARNING - NaN/Inf loss detected at batch 1301, skipping
2025-12-13 14:55:06,167 - WARNING - NaN/Inf loss detected at batch 1302, skipping
2025-12-13 14:55:06,192 - WARNING - NaN/Inf loss detected at batch 1303, skipping
2025-12-13 14:55:06,213 - WARNING - NaN/Inf loss detected at batch 1304, skipping
2025-12-13 14:55:06,234 - WARNING - NaN/Inf loss detected at batch 1305, skipping
2025-12-13 14:55:06,257 - WARNING - NaN/Inf loss detected at batch 1306, skipping
2025-12-13 14:55:06,277 - WARNING - NaN/Inf loss detected at batch 1307, skipping
2025-12-13 14:55:06,360 - WARNING - NaN/Inf loss detected at batch 1309, skipping
2025-12-13 14:55:06,420 - WARNING - NaN/Inf loss detected at batch 1311, skipping
2025-12-13 14:55:06,439 - WARNING - NaN/Inf loss detected at batch 1312, skipping
2025-12-13 14:55:06,460 - WARNING - NaN/Inf loss detected at batch 1313, skipping
2025-12-13 14:55:06,483 - WARNING - NaN/Inf loss detected at batch 1314, skipping
2025-12-13 14:55:06,505 - WARNING - NaN/Inf loss detected at batch 1315, skipping
2025-12-13 14:55:06,528 - WARNING - NaN/Inf loss detected at batch 1316, skipping
2025-12-13 14:55:06,548 - WARNING - NaN/Inf loss detected at batch 1317, skipping
2025-12-13 14:55:06,570 - WARNING - NaN/Inf loss detected at batch 1318, skipping
2025-12-13 14:55:06,589 - WARNING - NaN/Inf loss detected at batch 1319, skipping
2025-12-13 14:55:06,618 - WARNING - NaN/Inf loss detected at batch 1320, skipping
2025-12-13 14:55:06,637 - WARNING - NaN/Inf loss detected at batch 1321, skipping
2025-12-13 14:55:06,658 - WARNING - NaN/Inf loss detected at batch 1322, skipping
2025-12-13 14:55:06,678 - WARNING - NaN/Inf loss detected at batch 1323, skipping
2025-12-13 14:55:06,699 - WARNING - NaN/Inf loss detected at batch 1324, skipping
2025-12-13 14:55:06,721 - WARNING - NaN/Inf loss detected at batch 1325, skipping
2025-12-13 14:55:06,742 - WARNING - NaN/Inf loss detected at batch 1326, skipping
2025-12-13 14:55:06,764 - WARNING - NaN/Inf loss detected at batch 1327, skipping
2025-12-13 14:55:06,785 - WARNING - NaN/Inf loss detected at batch 1328, skipping
2025-12-13 14:55:06,808 - WARNING - NaN/Inf loss detected at batch 1329, skipping
2025-12-13 14:55:06,830 - WARNING - NaN/Inf loss detected at batch 1330, skipping
2025-12-13 14:55:06,852 - WARNING - NaN/Inf loss detected at batch 1331, skipping
2025-12-13 14:55:06,933 - WARNING - NaN/Inf loss detected at batch 1333, skipping
2025-12-13 14:55:06,955 - WARNING - NaN/Inf loss detected at batch 1334, skipping
2025-12-13 14:55:06,978 - WARNING - NaN/Inf loss detected at batch 1335, skipping
2025-12-13 14:55:06,998 - WARNING - NaN/Inf loss detected at batch 1336, skipping
2025-12-13 14:55:07,026 - WARNING - NaN/Inf loss detected at batch 1337, skipping
2025-12-13 14:55:07,049 - WARNING - NaN/Inf loss detected at batch 1338, skipping
2025-12-13 14:55:07,074 - WARNING - NaN/Inf loss detected at batch 1339, skipping
2025-12-13 14:55:07,097 - WARNING - NaN/Inf loss detected at batch 1340, skipping
2025-12-13 14:55:07,120 - WARNING - NaN/Inf loss detected at batch 1341, skipping
2025-12-13 14:55:07,211 - WARNING - NaN/Inf loss detected at batch 1344, skipping
2025-12-13 14:55:07,234 - WARNING - NaN/Inf loss detected at batch 1345, skipping
2025-12-13 14:55:07,256 - WARNING - NaN/Inf loss detected at batch 1346, skipping
2025-12-13 14:55:07,281 - WARNING - NaN/Inf loss detected at batch 1347, skipping
2025-12-13 14:55:07,304 - WARNING - NaN/Inf loss detected at batch 1348, skipping
2025-12-13 14:55:07,325 - WARNING - NaN/Inf loss detected at batch 1349, skipping
2025-12-13 14:55:07,347 - WARNING - NaN/Inf loss detected at batch 1350, skipping
2025-12-13 14:55:07,367 - WARNING - NaN/Inf loss detected at batch 1351, skipping
2025-12-13 14:55:07,427 - WARNING - NaN/Inf loss detected at batch 1353, skipping
2025-12-13 14:55:07,448 - WARNING - NaN/Inf loss detected at batch 1354, skipping
2025-12-13 14:55:07,469 - WARNING - NaN/Inf loss detected at batch 1355, skipping
2025-12-13 14:55:07,491 - WARNING - NaN/Inf loss detected at batch 1356, skipping
2025-12-13 14:55:07,511 - WARNING - NaN/Inf loss detected at batch 1357, skipping
2025-12-13 14:55:07,533 - WARNING - NaN/Inf loss detected at batch 1358, skipping
2025-12-13 14:55:07,556 - WARNING - NaN/Inf loss detected at batch 1359, skipping
2025-12-13 14:55:07,578 - WARNING - NaN/Inf loss detected at batch 1360, skipping
2025-12-13 14:55:07,600 - WARNING - NaN/Inf loss detected at batch 1361, skipping
2025-12-13 14:55:07,623 - WARNING - NaN/Inf loss detected at batch 1362, skipping
2025-12-13 14:55:07,646 - WARNING - NaN/Inf loss detected at batch 1363, skipping
2025-12-13 14:55:07,669 - WARNING - NaN/Inf loss detected at batch 1364, skipping
2025-12-13 14:55:07,691 - WARNING - NaN/Inf loss detected at batch 1365, skipping
2025-12-13 14:55:07,712 - WARNING - NaN/Inf loss detected at batch 1366, skipping
2025-12-13 14:55:07,736 - WARNING - NaN/Inf loss detected at batch 1367, skipping
2025-12-13 14:55:07,759 - WARNING - NaN/Inf loss detected at batch 1368, skipping
2025-12-13 14:55:07,779 - WARNING - NaN/Inf loss detected at batch 1369, skipping
2025-12-13 14:55:07,800 - WARNING - NaN/Inf loss detected at batch 1370, skipping
2025-12-13 14:55:07,821 - WARNING - NaN/Inf loss detected at batch 1371, skipping
2025-12-13 14:55:07,842 - WARNING - NaN/Inf loss detected at batch 1372, skipping
2025-12-13 14:55:07,863 - WARNING - NaN/Inf loss detected at batch 1373, skipping
2025-12-13 14:55:07,886 - WARNING - NaN/Inf loss detected at batch 1374, skipping
2025-12-13 14:55:07,909 - WARNING - NaN/Inf loss detected at batch 1375, skipping
2025-12-13 14:55:07,931 - WARNING - NaN/Inf loss detected at batch 1376, skipping
2025-12-13 14:55:07,954 - WARNING - NaN/Inf loss detected at batch 1377, skipping
2025-12-13 14:55:08,073 - WARNING - NaN/Inf loss detected at batch 1380, skipping
2025-12-13 14:55:08,093 - WARNING - NaN/Inf loss detected at batch 1381, skipping
2025-12-13 14:55:08,117 - WARNING - NaN/Inf loss detected at batch 1382, skipping
2025-12-13 14:55:08,174 - WARNING - NaN/Inf loss detected at batch 1384, skipping
2025-12-13 14:55:08,195 - WARNING - NaN/Inf loss detected at batch 1385, skipping
2025-12-13 14:55:08,217 - WARNING - NaN/Inf loss detected at batch 1386, skipping
2025-12-13 14:55:08,242 - WARNING - NaN/Inf loss detected at batch 1387, skipping
2025-12-13 14:55:08,261 - WARNING - NaN/Inf loss detected at batch 1388, skipping
2025-12-13 14:55:08,284 - WARNING - NaN/Inf loss detected at batch 1389, skipping
2025-12-13 14:55:08,305 - WARNING - NaN/Inf loss detected at batch 1390, skipping
2025-12-13 14:55:08,325 - WARNING - NaN/Inf loss detected at batch 1391, skipping
2025-12-13 14:55:08,349 - WARNING - NaN/Inf loss detected at batch 1392, skipping
2025-12-13 14:55:08,371 - WARNING - NaN/Inf loss detected at batch 1393, skipping
2025-12-13 14:55:08,392 - WARNING - NaN/Inf loss detected at batch 1394, skipping
2025-12-13 14:55:08,449 - WARNING - NaN/Inf loss detected at batch 1396, skipping
2025-12-13 14:55:08,470 - WARNING - NaN/Inf loss detected at batch 1397, skipping
2025-12-13 14:55:08,491 - WARNING - NaN/Inf loss detected at batch 1398, skipping
2025-12-13 14:55:08,521 - WARNING - NaN/Inf loss detected at batch 1399, skipping
2025-12-13 14:55:08,541 - WARNING - NaN/Inf loss detected at batch 1400, skipping
2025-12-13 14:55:08,564 - WARNING - NaN/Inf loss detected at batch 1401, skipping
2025-12-13 14:55:08,585 - WARNING - NaN/Inf loss detected at batch 1402, skipping
2025-12-13 14:55:08,610 - WARNING - NaN/Inf loss detected at batch 1403, skipping
2025-12-13 14:55:08,630 - WARNING - NaN/Inf loss detected at batch 1404, skipping
2025-12-13 14:55:08,651 - WARNING - NaN/Inf loss detected at batch 1405, skipping
2025-12-13 14:55:08,673 - WARNING - NaN/Inf loss detected at batch 1406, skipping
2025-12-13 14:55:08,694 - WARNING - NaN/Inf loss detected at batch 1407, skipping
2025-12-13 14:55:08,716 - WARNING - NaN/Inf loss detected at batch 1408, skipping
2025-12-13 14:55:08,738 - WARNING - NaN/Inf loss detected at batch 1409, skipping
2025-12-13 14:55:08,760 - WARNING - NaN/Inf loss detected at batch 1410, skipping
2025-12-13 14:55:08,783 - WARNING - NaN/Inf loss detected at batch 1411, skipping
2025-12-13 14:55:08,805 - WARNING - NaN/Inf loss detected at batch 1412, skipping
2025-12-13 14:55:08,828 - WARNING - NaN/Inf loss detected at batch 1413, skipping
2025-12-13 14:55:08,847 - WARNING - NaN/Inf loss detected at batch 1414, skipping
2025-12-13 14:55:08,872 - WARNING - NaN/Inf loss detected at batch 1415, skipping
2025-12-13 14:55:08,893 - WARNING - NaN/Inf loss detected at batch 1416, skipping
2025-12-13 14:55:08,914 - WARNING - NaN/Inf loss detected at batch 1417, skipping
2025-12-13 14:55:08,939 - WARNING - NaN/Inf loss detected at batch 1418, skipping
2025-12-13 14:55:08,960 - WARNING - NaN/Inf loss detected at batch 1419, skipping
2025-12-13 14:55:08,983 - WARNING - NaN/Inf loss detected at batch 1420, skipping
2025-12-13 14:55:09,006 - WARNING - NaN/Inf loss detected at batch 1421, skipping
2025-12-13 14:55:09,029 - WARNING - NaN/Inf loss detected at batch 1422, skipping
2025-12-13 14:55:09,053 - WARNING - NaN/Inf loss detected at batch 1423, skipping
2025-12-13 14:55:09,076 - WARNING - NaN/Inf loss detected at batch 1424, skipping
2025-12-13 14:55:09,098 - WARNING - NaN/Inf loss detected at batch 1425, skipping
2025-12-13 14:55:09,118 - WARNING - NaN/Inf loss detected at batch 1426, skipping
2025-12-13 14:55:09,138 - WARNING - NaN/Inf loss detected at batch 1427, skipping
2025-12-13 14:55:09,158 - WARNING - NaN/Inf loss detected at batch 1428, skipping
2025-12-13 14:55:09,184 - WARNING - NaN/Inf loss detected at batch 1429, skipping
2025-12-13 14:55:09,205 - WARNING - NaN/Inf loss detected at batch 1430, skipping
2025-12-13 14:55:09,392 - WARNING - NaN/Inf loss detected at batch 1435, skipping
2025-12-13 14:55:09,472 - WARNING - NaN/Inf loss detected at batch 1437, skipping
2025-12-13 14:55:09,494 - WARNING - NaN/Inf loss detected at batch 1438, skipping
2025-12-13 14:55:09,518 - WARNING - NaN/Inf loss detected at batch 1439, skipping
2025-12-13 14:55:09,540 - WARNING - NaN/Inf loss detected at batch 1440, skipping
2025-12-13 14:55:09,570 - WARNING - NaN/Inf loss detected at batch 1441, skipping
2025-12-13 14:55:09,592 - WARNING - NaN/Inf loss detected at batch 1442, skipping
2025-12-13 14:55:09,613 - WARNING - NaN/Inf loss detected at batch 1443, skipping
2025-12-13 14:55:09,635 - WARNING - NaN/Inf loss detected at batch 1444, skipping
2025-12-13 14:55:09,657 - WARNING - NaN/Inf loss detected at batch 1445, skipping
2025-12-13 14:55:09,716 - WARNING - NaN/Inf loss detected at batch 1447, skipping
2025-12-13 14:55:09,737 - WARNING - NaN/Inf loss detected at batch 1448, skipping
2025-12-13 14:55:09,759 - WARNING - NaN/Inf loss detected at batch 1449, skipping
2025-12-13 14:55:09,783 - WARNING - NaN/Inf loss detected at batch 1450, skipping
2025-12-13 14:55:09,801 - WARNING - NaN/Inf loss detected at batch 1451, skipping
2025-12-13 14:55:09,824 - WARNING - NaN/Inf loss detected at batch 1452, skipping
2025-12-13 14:55:09,846 - WARNING - NaN/Inf loss detected at batch 1453, skipping
2025-12-13 14:55:09,869 - WARNING - NaN/Inf loss detected at batch 1454, skipping
2025-12-13 14:55:09,893 - WARNING - NaN/Inf loss detected at batch 1455, skipping
2025-12-13 14:55:09,917 - WARNING - NaN/Inf loss detected at batch 1456, skipping
2025-12-13 14:55:09,937 - WARNING - NaN/Inf loss detected at batch 1457, skipping
2025-12-13 14:55:09,996 - WARNING - NaN/Inf loss detected at batch 1459, skipping
2025-12-13 14:55:10,020 - WARNING - NaN/Inf loss detected at batch 1460, skipping
2025-12-13 14:55:10,042 - WARNING - NaN/Inf loss detected at batch 1461, skipping
2025-12-13 14:55:10,059 - WARNING - NaN/Inf loss detected at batch 1462, skipping
2025-12-13 14:55:10,086 - WARNING - NaN/Inf loss detected at batch 1463, skipping
2025-12-13 14:55:10,105 - WARNING - NaN/Inf loss detected at batch 1464, skipping
2025-12-13 14:55:10,129 - WARNING - NaN/Inf loss detected at batch 1465, skipping
2025-12-13 14:55:10,149 - WARNING - NaN/Inf loss detected at batch 1466, skipping
2025-12-13 14:55:10,209 - WARNING - NaN/Inf loss detected at batch 1468, skipping
2025-12-13 14:55:10,231 - WARNING - NaN/Inf loss detected at batch 1469, skipping
2025-12-13 14:55:10,255 - WARNING - NaN/Inf loss detected at batch 1470, skipping
2025-12-13 14:55:10,280 - WARNING - NaN/Inf loss detected at batch 1471, skipping
2025-12-13 14:55:10,301 - WARNING - NaN/Inf loss detected at batch 1472, skipping
2025-12-13 14:55:10,323 - WARNING - NaN/Inf loss detected at batch 1473, skipping
2025-12-13 14:55:10,343 - WARNING - NaN/Inf loss detected at batch 1474, skipping
2025-12-13 14:55:10,367 - WARNING - NaN/Inf loss detected at batch 1475, skipping
2025-12-13 14:55:10,388 - WARNING - NaN/Inf loss detected at batch 1476, skipping
2025-12-13 14:55:10,409 - WARNING - NaN/Inf loss detected at batch 1477, skipping
2025-12-13 14:55:10,431 - WARNING - NaN/Inf loss detected at batch 1478, skipping
2025-12-13 14:55:10,454 - WARNING - NaN/Inf loss detected at batch 1479, skipping
2025-12-13 14:55:10,475 - WARNING - NaN/Inf loss detected at batch 1480, skipping
2025-12-13 14:55:10,497 - WARNING - NaN/Inf loss detected at batch 1481, skipping
2025-12-13 14:55:10,520 - WARNING - NaN/Inf loss detected at batch 1482, skipping
2025-12-13 14:55:10,540 - WARNING - NaN/Inf loss detected at batch 1483, skipping
2025-12-13 14:55:10,562 - WARNING - NaN/Inf loss detected at batch 1484, skipping
2025-12-13 14:55:10,582 - WARNING - NaN/Inf loss detected at batch 1485, skipping
2025-12-13 14:55:10,605 - WARNING - NaN/Inf loss detected at batch 1486, skipping
2025-12-13 14:55:10,630 - WARNING - NaN/Inf loss detected at batch 1487, skipping
2025-12-13 14:55:10,653 - WARNING - NaN/Inf loss detected at batch 1488, skipping
2025-12-13 14:55:10,713 - WARNING - NaN/Inf loss detected at batch 1490, skipping
2025-12-13 14:55:10,735 - WARNING - NaN/Inf loss detected at batch 1491, skipping
2025-12-13 14:55:10,755 - WARNING - NaN/Inf loss detected at batch 1492, skipping
2025-12-13 14:55:10,776 - WARNING - NaN/Inf loss detected at batch 1493, skipping
2025-12-13 14:55:10,842 - WARNING - NaN/Inf loss detected at batch 1495, skipping
2025-12-13 14:55:10,865 - WARNING - NaN/Inf loss detected at batch 1496, skipping
2025-12-13 14:55:10,932 - WARNING - NaN/Inf loss detected at batch 1498, skipping
2025-12-13 14:55:10,949 - WARNING - NaN/Inf loss detected at batch 1499, skipping
2025-12-13 14:55:10,971 - WARNING - NaN/Inf loss detected at batch 1500, skipping
2025-12-13 14:55:10,997 - WARNING - NaN/Inf loss detected at batch 1501, skipping
2025-12-13 14:55:11,019 - WARNING - NaN/Inf loss detected at batch 1502, skipping
2025-12-13 14:55:11,044 - WARNING - NaN/Inf loss detected at batch 1503, skipping
2025-12-13 14:55:11,067 - WARNING - NaN/Inf loss detected at batch 1504, skipping
2025-12-13 14:55:11,087 - WARNING - NaN/Inf loss detected at batch 1505, skipping
2025-12-13 14:55:11,108 - WARNING - NaN/Inf loss detected at batch 1506, skipping
2025-12-13 14:55:11,130 - WARNING - NaN/Inf loss detected at batch 1507, skipping
2025-12-13 14:55:11,152 - WARNING - NaN/Inf loss detected at batch 1508, skipping
2025-12-13 14:55:11,170 - WARNING - NaN/Inf loss detected at batch 1509, skipping
2025-12-13 14:55:11,195 - WARNING - NaN/Inf loss detected at batch 1510, skipping
2025-12-13 14:55:11,217 - WARNING - NaN/Inf loss detected at batch 1511, skipping
2025-12-13 14:55:11,238 - WARNING - NaN/Inf loss detected at batch 1512, skipping
2025-12-13 14:55:11,260 - WARNING - NaN/Inf loss detected at batch 1513, skipping
2025-12-13 14:55:11,283 - WARNING - NaN/Inf loss detected at batch 1514, skipping
2025-12-13 14:55:11,303 - WARNING - NaN/Inf loss detected at batch 1515, skipping
2025-12-13 14:55:11,324 - WARNING - NaN/Inf loss detected at batch 1516, skipping
2025-12-13 14:55:11,344 - WARNING - NaN/Inf loss detected at batch 1517, skipping
2025-12-13 14:55:11,370 - WARNING - NaN/Inf loss detected at batch 1518, skipping
2025-12-13 14:55:11,391 - WARNING - NaN/Inf loss detected at batch 1519, skipping
2025-12-13 14:55:11,415 - WARNING - NaN/Inf loss detected at batch 1520, skipping
2025-12-13 14:55:11,436 - WARNING - NaN/Inf loss detected at batch 1521, skipping
2025-12-13 14:55:11,457 - WARNING - NaN/Inf loss detected at batch 1522, skipping
2025-12-13 14:55:11,480 - WARNING - NaN/Inf loss detected at batch 1523, skipping
2025-12-13 14:55:11,498 - WARNING - NaN/Inf loss detected at batch 1524, skipping
2025-12-13 14:55:11,520 - WARNING - NaN/Inf loss detected at batch 1525, skipping
2025-12-13 14:55:11,578 - WARNING - NaN/Inf loss detected at batch 1527, skipping
2025-12-13 14:55:11,633 - WARNING - NaN/Inf loss detected at batch 1529, skipping
2025-12-13 14:55:11,662 - WARNING - NaN/Inf loss detected at batch 1530, skipping
2025-12-13 14:55:11,683 - WARNING - NaN/Inf loss detected at batch 1531, skipping
2025-12-13 14:55:11,707 - WARNING - NaN/Inf loss detected at batch 1532, skipping
2025-12-13 14:55:11,729 - WARNING - NaN/Inf loss detected at batch 1533, skipping
2025-12-13 14:55:11,751 - WARNING - NaN/Inf loss detected at batch 1534, skipping
2025-12-13 14:55:11,775 - WARNING - NaN/Inf loss detected at batch 1535, skipping
2025-12-13 14:55:11,800 - WARNING - NaN/Inf loss detected at batch 1536, skipping
2025-12-13 14:55:11,822 - WARNING - NaN/Inf loss detected at batch 1537, skipping
2025-12-13 14:55:11,846 - WARNING - NaN/Inf loss detected at batch 1538, skipping
2025-12-13 14:55:11,866 - WARNING - NaN/Inf loss detected at batch 1539, skipping
2025-12-13 14:55:11,887 - WARNING - NaN/Inf loss detected at batch 1540, skipping
2025-12-13 14:55:11,951 - WARNING - NaN/Inf loss detected at batch 1542, skipping
2025-12-13 14:55:11,971 - WARNING - NaN/Inf loss detected at batch 1543, skipping
2025-12-13 14:55:11,993 - WARNING - NaN/Inf loss detected at batch 1544, skipping
2025-12-13 14:55:12,015 - WARNING - NaN/Inf loss detected at batch 1545, skipping
2025-12-13 14:55:12,038 - WARNING - NaN/Inf loss detected at batch 1546, skipping
2025-12-13 14:55:12,059 - WARNING - NaN/Inf loss detected at batch 1547, skipping
2025-12-13 14:55:12,081 - WARNING - NaN/Inf loss detected at batch 1548, skipping
2025-12-13 14:55:12,103 - WARNING - NaN/Inf loss detected at batch 1549, skipping
2025-12-13 14:55:12,125 - WARNING - NaN/Inf loss detected at batch 1550, skipping
2025-12-13 14:55:12,147 - WARNING - NaN/Inf loss detected at batch 1551, skipping
2025-12-13 14:55:12,169 - WARNING - NaN/Inf loss detected at batch 1552, skipping
2025-12-13 14:55:12,190 - WARNING - NaN/Inf loss detected at batch 1553, skipping
2025-12-13 14:55:12,247 - WARNING - NaN/Inf loss detected at batch 1555, skipping
2025-12-13 14:55:12,303 - WARNING - NaN/Inf loss detected at batch 1557, skipping
2025-12-13 14:55:12,325 - WARNING - NaN/Inf loss detected at batch 1558, skipping
2025-12-13 14:55:12,346 - WARNING - NaN/Inf loss detected at batch 1559, skipping
2025-12-13 14:55:12,366 - WARNING - NaN/Inf loss detected at batch 1560, skipping
2025-12-13 14:55:12,446 - WARNING - NaN/Inf loss detected at batch 1562, skipping
2025-12-13 14:55:12,472 - WARNING - NaN/Inf loss detected at batch 1563, skipping
2025-12-13 14:55:12,492 - WARNING - NaN/Inf loss detected at batch 1564, skipping
2025-12-13 14:55:12,514 - WARNING - NaN/Inf loss detected at batch 1565, skipping
2025-12-13 14:55:12,535 - WARNING - NaN/Inf loss detected at batch 1566, skipping
2025-12-13 14:55:12,559 - WARNING - NaN/Inf loss detected at batch 1567, skipping
2025-12-13 14:55:12,590 - WARNING - NaN/Inf loss detected at batch 1568, skipping
2025-12-13 14:55:12,611 - WARNING - NaN/Inf loss detected at batch 1569, skipping
2025-12-13 14:55:12,632 - WARNING - NaN/Inf loss detected at batch 1570, skipping
2025-12-13 14:55:12,651 - WARNING - NaN/Inf loss detected at batch 1571, skipping
2025-12-13 14:55:12,674 - WARNING - NaN/Inf loss detected at batch 1572, skipping
2025-12-13 14:55:12,695 - WARNING - NaN/Inf loss detected at batch 1573, skipping
2025-12-13 14:55:12,715 - WARNING - NaN/Inf loss detected at batch 1574, skipping
2025-12-13 14:55:12,735 - WARNING - NaN/Inf loss detected at batch 1575, skipping
2025-12-13 14:55:12,757 - WARNING - NaN/Inf loss detected at batch 1576, skipping
2025-12-13 14:55:12,779 - WARNING - NaN/Inf loss detected at batch 1577, skipping
2025-12-13 14:55:12,802 - WARNING - NaN/Inf loss detected at batch 1578, skipping
2025-12-13 14:55:12,823 - WARNING - NaN/Inf loss detected at batch 1579, skipping
2025-12-13 14:55:12,851 - WARNING - NaN/Inf loss detected at batch 1580, skipping
2025-12-13 14:55:12,871 - WARNING - NaN/Inf loss detected at batch 1581, skipping
2025-12-13 14:55:12,894 - WARNING - NaN/Inf loss detected at batch 1582, skipping
2025-12-13 14:55:12,917 - WARNING - NaN/Inf loss detected at batch 1583, skipping
2025-12-13 14:55:12,939 - WARNING - NaN/Inf loss detected at batch 1584, skipping
2025-12-13 14:55:12,961 - WARNING - NaN/Inf loss detected at batch 1585, skipping
2025-12-13 14:55:12,981 - WARNING - NaN/Inf loss detected at batch 1586, skipping
2025-12-13 14:55:13,004 - WARNING - NaN/Inf loss detected at batch 1587, skipping
2025-12-13 14:55:13,027 - WARNING - NaN/Inf loss detected at batch 1588, skipping
2025-12-13 14:55:13,045 - WARNING - NaN/Inf loss detected at batch 1589, skipping
2025-12-13 14:55:13,066 - WARNING - NaN/Inf loss detected at batch 1590, skipping
2025-12-13 14:55:13,088 - WARNING - NaN/Inf loss detected at batch 1591, skipping
2025-12-13 14:55:13,109 - WARNING - NaN/Inf loss detected at batch 1592, skipping
2025-12-13 14:55:13,132 - WARNING - NaN/Inf loss detected at batch 1593, skipping
2025-12-13 14:55:13,155 - WARNING - NaN/Inf loss detected at batch 1594, skipping
2025-12-13 14:55:13,176 - WARNING - NaN/Inf loss detected at batch 1595, skipping
2025-12-13 14:55:13,199 - WARNING - NaN/Inf loss detected at batch 1596, skipping
2025-12-13 14:55:13,221 - WARNING - NaN/Inf loss detected at batch 1597, skipping
2025-12-13 14:55:13,240 - WARNING - NaN/Inf loss detected at batch 1598, skipping
2025-12-13 14:55:13,261 - WARNING - NaN/Inf loss detected at batch 1599, skipping
2025-12-13 14:55:13,283 - WARNING - NaN/Inf loss detected at batch 1600, skipping
2025-12-13 14:55:13,305 - WARNING - NaN/Inf loss detected at batch 1601, skipping
2025-12-13 14:55:13,325 - WARNING - NaN/Inf loss detected at batch 1602, skipping
2025-12-13 14:55:13,345 - WARNING - NaN/Inf loss detected at batch 1603, skipping
2025-12-13 14:56:18,524 - INFO - Logging initialize successfully
2025-12-13 14:56:20,488 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 14:56:20,596 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 14:56:22,007 - INFO - Built 29000 samples from annotaions
2025-12-13 14:56:22,008 - INFO - Total train samples: 29000
2025-12-13 14:56:22,041 - INFO - Built 1014 samples from annotaions
2025-12-13 14:56:22,041 - INFO - Total val samples: 1014
2025-12-13 14:56:22,110 - INFO - Built 29000 samples for story 
2025-12-13 14:56:22,110 - INFO - Built 1014 samples for story 
2025-12-13 14:56:22,263 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 14:56:22,270 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 14:56:25,817 - INFO - No valid checkpoint found or resume disabled. Starting from scratch.
2025-12-13 14:56:44,464 - INFO - Epoch [1] Batch [0/7250] loss: 10.9749 
2025-12-13 14:56:50,342 - INFO - Epoch [1] Batch [100/7250] loss: 6.3066 
2025-12-13 14:56:56,195 - INFO - Epoch [1] Batch [200/7250] loss: 5.9105 
2025-12-13 14:57:02,050 - INFO - Epoch [1] Batch [300/7250] loss: 5.7892 
2025-12-13 14:57:07,905 - INFO - Epoch [1] Batch [400/7250] loss: 6.1210 
2025-12-13 14:57:13,831 - INFO - Epoch [1] Batch [500/7250] loss: 5.8152 
2025-12-13 14:57:19,664 - INFO - Epoch [1] Batch [600/7250] loss: 6.1324 
2025-12-13 14:57:25,528 - INFO - Epoch [1] Batch [700/7250] loss: 5.9224 
2025-12-13 14:57:31,578 - INFO - Epoch [1] Batch [800/7250] loss: 5.7549 
2025-12-13 14:57:37,504 - INFO - Epoch [1] Batch [900/7250] loss: 6.0890 
2025-12-13 14:57:39,814 - WARNING - NaN/Inf loss detected at batch 939, skipping
2025-12-13 14:57:39,896 - WARNING - NaN/Inf loss detected at batch 941, skipping
2025-12-13 14:57:39,922 - WARNING - NaN/Inf loss detected at batch 942, skipping
2025-12-13 14:57:40,061 - WARNING - NaN/Inf loss detected at batch 945, skipping
2025-12-13 14:57:40,087 - WARNING - NaN/Inf loss detected at batch 946, skipping
2025-12-13 14:57:40,155 - WARNING - NaN/Inf loss detected at batch 948, skipping
2025-12-13 14:57:40,177 - WARNING - NaN/Inf loss detected at batch 949, skipping
2025-12-13 14:57:40,200 - WARNING - NaN/Inf loss detected at batch 950, skipping
2025-12-13 14:57:40,226 - WARNING - NaN/Inf loss detected at batch 951, skipping
2025-12-13 14:57:40,250 - WARNING - NaN/Inf loss detected at batch 952, skipping
2025-12-13 14:57:40,276 - WARNING - NaN/Inf loss detected at batch 953, skipping
2025-12-13 14:57:40,298 - WARNING - NaN/Inf loss detected at batch 954, skipping
2025-12-13 14:57:40,321 - WARNING - NaN/Inf loss detected at batch 955, skipping
2025-12-13 14:57:40,345 - WARNING - NaN/Inf loss detected at batch 956, skipping
2025-12-13 14:57:40,367 - WARNING - NaN/Inf loss detected at batch 957, skipping
2025-12-13 14:57:40,390 - WARNING - NaN/Inf loss detected at batch 958, skipping
2025-12-13 14:57:40,472 - WARNING - NaN/Inf loss detected at batch 960, skipping
2025-12-13 14:57:40,494 - WARNING - NaN/Inf loss detected at batch 961, skipping
2025-12-13 14:57:40,517 - WARNING - NaN/Inf loss detected at batch 962, skipping
2025-12-13 14:57:40,539 - WARNING - NaN/Inf loss detected at batch 963, skipping
2025-12-13 14:57:40,564 - WARNING - NaN/Inf loss detected at batch 964, skipping
2025-12-13 14:57:40,586 - WARNING - NaN/Inf loss detected at batch 965, skipping
2025-12-13 14:57:40,608 - WARNING - NaN/Inf loss detected at batch 966, skipping
2025-12-13 14:57:40,631 - WARNING - NaN/Inf loss detected at batch 967, skipping
2025-12-13 14:57:40,653 - WARNING - NaN/Inf loss detected at batch 968, skipping
2025-12-13 14:57:40,676 - WARNING - NaN/Inf loss detected at batch 969, skipping
2025-12-13 14:57:40,700 - WARNING - NaN/Inf loss detected at batch 970, skipping
2025-12-13 14:57:40,723 - WARNING - NaN/Inf loss detected at batch 971, skipping
2025-12-13 14:57:40,805 - WARNING - NaN/Inf loss detected at batch 973, skipping
2025-12-13 14:57:40,830 - WARNING - NaN/Inf loss detected at batch 974, skipping
2025-12-13 14:57:40,853 - WARNING - NaN/Inf loss detected at batch 975, skipping
2025-12-13 14:57:40,876 - WARNING - NaN/Inf loss detected at batch 976, skipping
2025-12-13 14:57:40,900 - WARNING - NaN/Inf loss detected at batch 977, skipping
2025-12-13 14:57:40,923 - WARNING - NaN/Inf loss detected at batch 978, skipping
2025-12-13 14:57:40,945 - WARNING - NaN/Inf loss detected at batch 979, skipping
2025-12-13 14:57:40,968 - WARNING - NaN/Inf loss detected at batch 980, skipping
2025-12-13 14:57:40,984 - WARNING - NaN/Inf loss detected at batch 981, skipping
2025-12-13 14:57:41,015 - WARNING - NaN/Inf loss detected at batch 982, skipping
2025-12-13 14:57:41,036 - WARNING - NaN/Inf loss detected at batch 983, skipping
2025-12-13 14:57:41,061 - WARNING - NaN/Inf loss detected at batch 984, skipping
2025-12-13 14:57:41,084 - WARNING - NaN/Inf loss detected at batch 985, skipping
2025-12-13 14:57:41,103 - WARNING - NaN/Inf loss detected at batch 986, skipping
2025-12-13 14:57:41,126 - WARNING - NaN/Inf loss detected at batch 987, skipping
2025-12-13 14:57:41,150 - WARNING - NaN/Inf loss detected at batch 988, skipping
2025-12-13 14:57:41,173 - WARNING - NaN/Inf loss detected at batch 989, skipping
2025-12-13 14:57:41,196 - WARNING - NaN/Inf loss detected at batch 990, skipping
2025-12-13 14:57:41,220 - WARNING - NaN/Inf loss detected at batch 991, skipping
2025-12-13 14:57:41,242 - WARNING - NaN/Inf loss detected at batch 992, skipping
2025-12-13 14:57:41,262 - WARNING - NaN/Inf loss detected at batch 993, skipping
2025-12-13 14:57:41,287 - WARNING - NaN/Inf loss detected at batch 994, skipping
2025-12-13 14:57:41,309 - WARNING - NaN/Inf loss detected at batch 995, skipping
2025-12-13 14:57:41,368 - WARNING - NaN/Inf loss detected at batch 997, skipping
2025-12-13 14:57:41,390 - WARNING - NaN/Inf loss detected at batch 998, skipping
2025-12-13 14:57:41,412 - WARNING - NaN/Inf loss detected at batch 999, skipping
2025-12-13 14:57:41,455 - INFO - Epoch [1] Batch [1000/7250] loss: 6.2014 
2025-12-13 14:57:41,527 - WARNING - NaN/Inf loss detected at batch 1002, skipping
2025-12-13 14:57:41,559 - WARNING - NaN/Inf loss detected at batch 1003, skipping
2025-12-13 14:57:41,579 - WARNING - NaN/Inf loss detected at batch 1004, skipping
2025-12-13 14:57:41,603 - WARNING - NaN/Inf loss detected at batch 1005, skipping
2025-12-13 14:57:41,625 - WARNING - NaN/Inf loss detected at batch 1006, skipping
2025-12-13 14:57:41,648 - WARNING - NaN/Inf loss detected at batch 1007, skipping
2025-12-13 14:57:41,672 - WARNING - NaN/Inf loss detected at batch 1008, skipping
2025-12-13 14:57:41,695 - WARNING - NaN/Inf loss detected at batch 1009, skipping
2025-12-13 14:57:41,719 - WARNING - NaN/Inf loss detected at batch 1010, skipping
2025-12-13 14:57:41,741 - WARNING - NaN/Inf loss detected at batch 1011, skipping
2025-12-13 14:57:41,769 - WARNING - NaN/Inf loss detected at batch 1012, skipping
2025-12-13 14:57:41,791 - WARNING - NaN/Inf loss detected at batch 1013, skipping
2025-12-13 14:57:41,814 - WARNING - NaN/Inf loss detected at batch 1014, skipping
2025-12-13 14:57:41,895 - WARNING - NaN/Inf loss detected at batch 1016, skipping
2025-12-13 14:57:41,921 - WARNING - NaN/Inf loss detected at batch 1017, skipping
2025-12-13 14:57:41,944 - WARNING - NaN/Inf loss detected at batch 1018, skipping
2025-12-13 14:57:41,967 - WARNING - NaN/Inf loss detected at batch 1019, skipping
2025-12-13 14:57:41,989 - WARNING - NaN/Inf loss detected at batch 1020, skipping
2025-12-13 14:57:42,012 - WARNING - NaN/Inf loss detected at batch 1021, skipping
2025-12-13 14:57:42,034 - WARNING - NaN/Inf loss detected at batch 1022, skipping
2025-12-13 14:57:42,055 - WARNING - NaN/Inf loss detected at batch 1023, skipping
2025-12-13 14:57:42,080 - WARNING - NaN/Inf loss detected at batch 1024, skipping
2025-12-13 14:57:42,102 - WARNING - NaN/Inf loss detected at batch 1025, skipping
2025-12-13 14:57:42,128 - WARNING - NaN/Inf loss detected at batch 1026, skipping
2025-12-13 14:57:42,151 - WARNING - NaN/Inf loss detected at batch 1027, skipping
2025-12-13 14:57:42,178 - WARNING - NaN/Inf loss detected at batch 1028, skipping
2025-12-13 14:57:42,202 - WARNING - NaN/Inf loss detected at batch 1029, skipping
2025-12-13 14:57:42,226 - WARNING - NaN/Inf loss detected at batch 1030, skipping
2025-12-13 14:57:42,251 - WARNING - NaN/Inf loss detected at batch 1031, skipping
2025-12-13 14:57:42,273 - WARNING - NaN/Inf loss detected at batch 1032, skipping
2025-12-13 14:57:42,294 - WARNING - NaN/Inf loss detected at batch 1033, skipping
2025-12-13 14:57:42,316 - WARNING - NaN/Inf loss detected at batch 1034, skipping
2025-12-13 14:57:42,340 - WARNING - NaN/Inf loss detected at batch 1035, skipping
2025-12-13 14:57:42,363 - WARNING - NaN/Inf loss detected at batch 1036, skipping
2025-12-13 14:57:42,385 - WARNING - NaN/Inf loss detected at batch 1037, skipping
2025-12-13 14:57:42,406 - WARNING - NaN/Inf loss detected at batch 1038, skipping
2025-12-13 14:57:42,428 - WARNING - NaN/Inf loss detected at batch 1039, skipping
2025-12-13 14:57:42,452 - WARNING - NaN/Inf loss detected at batch 1040, skipping
2025-12-13 14:57:42,472 - WARNING - NaN/Inf loss detected at batch 1041, skipping
2025-12-13 14:57:42,495 - WARNING - NaN/Inf loss detected at batch 1042, skipping
2025-12-13 14:57:42,555 - WARNING - NaN/Inf loss detected at batch 1044, skipping
2025-12-13 14:57:42,638 - WARNING - NaN/Inf loss detected at batch 1046, skipping
2025-12-13 14:57:42,664 - WARNING - NaN/Inf loss detected at batch 1047, skipping
2025-12-13 14:57:42,687 - WARNING - NaN/Inf loss detected at batch 1048, skipping
2025-12-13 14:57:42,769 - WARNING - NaN/Inf loss detected at batch 1050, skipping
2025-12-13 14:57:42,793 - WARNING - NaN/Inf loss detected at batch 1051, skipping
2025-12-13 14:57:42,818 - WARNING - NaN/Inf loss detected at batch 1052, skipping
2025-12-13 14:57:42,848 - WARNING - NaN/Inf loss detected at batch 1053, skipping
2025-12-13 14:57:42,871 - WARNING - NaN/Inf loss detected at batch 1054, skipping
2025-12-13 14:57:42,893 - WARNING - NaN/Inf loss detected at batch 1055, skipping
2025-12-13 14:57:42,915 - WARNING - NaN/Inf loss detected at batch 1056, skipping
2025-12-13 14:57:42,937 - WARNING - NaN/Inf loss detected at batch 1057, skipping
2025-12-13 14:57:42,961 - WARNING - NaN/Inf loss detected at batch 1058, skipping
2025-12-13 14:57:42,987 - WARNING - NaN/Inf loss detected at batch 1059, skipping
2025-12-13 14:57:43,007 - WARNING - NaN/Inf loss detected at batch 1060, skipping
2025-12-13 14:57:43,030 - WARNING - NaN/Inf loss detected at batch 1061, skipping
2025-12-13 14:57:43,053 - WARNING - NaN/Inf loss detected at batch 1062, skipping
2025-12-13 14:57:43,075 - WARNING - NaN/Inf loss detected at batch 1063, skipping
2025-12-13 14:57:43,098 - WARNING - NaN/Inf loss detected at batch 1064, skipping
2025-12-13 14:57:43,117 - WARNING - NaN/Inf loss detected at batch 1065, skipping
2025-12-13 14:57:43,138 - WARNING - NaN/Inf loss detected at batch 1066, skipping
2025-12-13 14:57:43,165 - WARNING - NaN/Inf loss detected at batch 1067, skipping
2025-12-13 14:57:43,188 - WARNING - NaN/Inf loss detected at batch 1068, skipping
2025-12-13 14:57:43,209 - WARNING - NaN/Inf loss detected at batch 1069, skipping
2025-12-13 14:57:43,232 - WARNING - NaN/Inf loss detected at batch 1070, skipping
2025-12-13 14:57:43,257 - WARNING - NaN/Inf loss detected at batch 1071, skipping
2025-12-13 14:57:43,278 - WARNING - NaN/Inf loss detected at batch 1072, skipping
2025-12-13 14:57:43,301 - WARNING - NaN/Inf loss detected at batch 1073, skipping
2025-12-13 14:57:43,320 - WARNING - NaN/Inf loss detected at batch 1074, skipping
2025-12-13 14:57:43,346 - WARNING - NaN/Inf loss detected at batch 1075, skipping
2025-12-13 14:57:43,367 - WARNING - NaN/Inf loss detected at batch 1076, skipping
2025-12-13 14:57:43,389 - WARNING - NaN/Inf loss detected at batch 1077, skipping
2025-12-13 14:57:43,410 - WARNING - NaN/Inf loss detected at batch 1078, skipping
2025-12-13 14:57:43,433 - WARNING - NaN/Inf loss detected at batch 1079, skipping
2025-12-13 14:57:43,456 - WARNING - NaN/Inf loss detected at batch 1080, skipping
2025-12-13 14:57:43,484 - WARNING - NaN/Inf loss detected at batch 1081, skipping
2025-12-13 14:57:43,513 - WARNING - NaN/Inf loss detected at batch 1082, skipping
2025-12-13 14:57:43,537 - WARNING - NaN/Inf loss detected at batch 1083, skipping
2025-12-13 14:57:43,560 - WARNING - NaN/Inf loss detected at batch 1084, skipping
2025-12-13 14:57:43,649 - WARNING - NaN/Inf loss detected at batch 1086, skipping
2025-12-13 14:57:43,671 - WARNING - NaN/Inf loss detected at batch 1087, skipping
2025-12-13 14:57:43,693 - WARNING - NaN/Inf loss detected at batch 1088, skipping
2025-12-13 14:57:43,715 - WARNING - NaN/Inf loss detected at batch 1089, skipping
2025-12-13 14:57:43,737 - WARNING - NaN/Inf loss detected at batch 1090, skipping
2025-12-13 14:57:43,760 - WARNING - NaN/Inf loss detected at batch 1091, skipping
2025-12-13 14:57:43,782 - WARNING - NaN/Inf loss detected at batch 1092, skipping
2025-12-13 14:57:43,805 - WARNING - NaN/Inf loss detected at batch 1093, skipping
2025-12-13 14:57:43,827 - WARNING - NaN/Inf loss detected at batch 1094, skipping
2025-12-13 14:57:43,851 - WARNING - NaN/Inf loss detected at batch 1095, skipping
2025-12-13 14:57:43,874 - WARNING - NaN/Inf loss detected at batch 1096, skipping
2025-12-13 14:57:43,900 - WARNING - NaN/Inf loss detected at batch 1097, skipping
2025-12-13 14:57:43,926 - WARNING - NaN/Inf loss detected at batch 1098, skipping
2025-12-13 14:57:43,949 - WARNING - NaN/Inf loss detected at batch 1099, skipping
2025-12-13 14:57:43,974 - WARNING - NaN/Inf loss detected at batch 1100, skipping
2025-12-13 14:57:43,998 - WARNING - NaN/Inf loss detected at batch 1101, skipping
2025-12-13 14:57:44,020 - WARNING - NaN/Inf loss detected at batch 1102, skipping
2025-12-13 14:57:44,043 - WARNING - NaN/Inf loss detected at batch 1103, skipping
2025-12-13 14:57:44,065 - WARNING - NaN/Inf loss detected at batch 1104, skipping
2025-12-13 14:57:44,087 - WARNING - NaN/Inf loss detected at batch 1105, skipping
2025-12-13 14:57:44,111 - WARNING - NaN/Inf loss detected at batch 1106, skipping
2025-12-13 14:57:44,134 - WARNING - NaN/Inf loss detected at batch 1107, skipping
2025-12-13 14:57:44,158 - WARNING - NaN/Inf loss detected at batch 1108, skipping
2025-12-13 14:57:44,180 - WARNING - NaN/Inf loss detected at batch 1109, skipping
2025-12-13 14:57:44,202 - WARNING - NaN/Inf loss detected at batch 1110, skipping
2025-12-13 14:57:44,225 - WARNING - NaN/Inf loss detected at batch 1111, skipping
2025-12-13 14:57:44,255 - WARNING - NaN/Inf loss detected at batch 1112, skipping
2025-12-13 14:57:44,278 - WARNING - NaN/Inf loss detected at batch 1113, skipping
2025-12-13 14:57:44,298 - WARNING - NaN/Inf loss detected at batch 1114, skipping
2025-12-13 14:57:44,321 - WARNING - NaN/Inf loss detected at batch 1115, skipping
2025-12-13 14:57:44,344 - WARNING - NaN/Inf loss detected at batch 1116, skipping
2025-12-13 14:57:44,368 - WARNING - NaN/Inf loss detected at batch 1117, skipping
2025-12-13 14:57:44,393 - WARNING - NaN/Inf loss detected at batch 1118, skipping
2025-12-13 14:57:44,420 - WARNING - NaN/Inf loss detected at batch 1119, skipping
2025-12-13 14:57:44,442 - WARNING - NaN/Inf loss detected at batch 1120, skipping
2025-12-13 14:57:44,464 - WARNING - NaN/Inf loss detected at batch 1121, skipping
2025-12-13 14:57:44,487 - WARNING - NaN/Inf loss detected at batch 1122, skipping
2025-12-13 14:57:44,507 - WARNING - NaN/Inf loss detected at batch 1123, skipping
2025-12-13 14:57:44,531 - WARNING - NaN/Inf loss detected at batch 1124, skipping
2025-12-13 14:57:44,556 - WARNING - NaN/Inf loss detected at batch 1125, skipping
2025-12-13 14:57:44,576 - WARNING - NaN/Inf loss detected at batch 1126, skipping
2025-12-13 14:57:44,600 - WARNING - NaN/Inf loss detected at batch 1127, skipping
2025-12-13 14:57:44,621 - WARNING - NaN/Inf loss detected at batch 1128, skipping
2025-12-13 14:57:44,686 - WARNING - NaN/Inf loss detected at batch 1130, skipping
2025-12-13 14:57:44,710 - WARNING - NaN/Inf loss detected at batch 1131, skipping
2025-12-13 14:57:44,734 - WARNING - NaN/Inf loss detected at batch 1132, skipping
2025-12-13 14:57:44,755 - WARNING - NaN/Inf loss detected at batch 1133, skipping
2025-12-13 14:57:44,779 - WARNING - NaN/Inf loss detected at batch 1134, skipping
2025-12-13 14:57:44,802 - WARNING - NaN/Inf loss detected at batch 1135, skipping
2025-12-13 14:57:44,824 - WARNING - NaN/Inf loss detected at batch 1136, skipping
2025-12-13 14:57:44,846 - WARNING - NaN/Inf loss detected at batch 1137, skipping
2025-12-13 14:57:44,871 - WARNING - NaN/Inf loss detected at batch 1138, skipping
2025-12-13 14:57:44,895 - WARNING - NaN/Inf loss detected at batch 1139, skipping
2025-12-13 14:57:44,916 - WARNING - NaN/Inf loss detected at batch 1140, skipping
2025-12-13 14:57:44,938 - WARNING - NaN/Inf loss detected at batch 1141, skipping
2025-12-13 14:57:44,960 - WARNING - NaN/Inf loss detected at batch 1142, skipping
2025-12-13 14:57:44,984 - WARNING - NaN/Inf loss detected at batch 1143, skipping
2025-12-13 14:57:45,006 - WARNING - NaN/Inf loss detected at batch 1144, skipping
2025-12-13 14:57:45,030 - WARNING - NaN/Inf loss detected at batch 1145, skipping
2025-12-13 14:57:45,053 - WARNING - NaN/Inf loss detected at batch 1146, skipping
2025-12-13 14:57:45,076 - WARNING - NaN/Inf loss detected at batch 1147, skipping
2025-12-13 14:57:45,100 - WARNING - NaN/Inf loss detected at batch 1148, skipping
2025-12-13 14:57:45,121 - WARNING - NaN/Inf loss detected at batch 1149, skipping
2025-12-13 14:57:45,144 - WARNING - NaN/Inf loss detected at batch 1150, skipping
2025-12-13 14:57:45,159 - WARNING - NaN/Inf loss detected at batch 1151, skipping
2025-12-13 14:57:45,187 - WARNING - NaN/Inf loss detected at batch 1152, skipping
2025-12-13 14:57:45,211 - WARNING - NaN/Inf loss detected at batch 1153, skipping
2025-12-13 14:57:45,232 - WARNING - NaN/Inf loss detected at batch 1154, skipping
2025-12-13 14:57:45,255 - WARNING - NaN/Inf loss detected at batch 1155, skipping
2025-12-13 14:57:45,280 - WARNING - NaN/Inf loss detected at batch 1156, skipping
2025-12-13 14:57:45,304 - WARNING - NaN/Inf loss detected at batch 1157, skipping
2025-12-13 14:57:45,322 - WARNING - NaN/Inf loss detected at batch 1158, skipping
2025-12-13 14:57:45,348 - WARNING - NaN/Inf loss detected at batch 1159, skipping
2025-12-13 14:57:45,370 - WARNING - NaN/Inf loss detected at batch 1160, skipping
2025-12-13 14:57:45,381 - WARNING - NaN/Inf loss detected at batch 1161, skipping
2025-12-13 14:57:45,414 - WARNING - NaN/Inf loss detected at batch 1162, skipping
2025-12-13 14:57:45,437 - WARNING - NaN/Inf loss detected at batch 1163, skipping
2025-12-13 14:57:45,459 - WARNING - NaN/Inf loss detected at batch 1164, skipping
2025-12-13 14:57:45,483 - WARNING - NaN/Inf loss detected at batch 1165, skipping
2025-12-13 14:57:45,505 - WARNING - NaN/Inf loss detected at batch 1166, skipping
2025-12-13 14:57:45,527 - WARNING - NaN/Inf loss detected at batch 1167, skipping
2025-12-13 14:57:45,551 - WARNING - NaN/Inf loss detected at batch 1168, skipping
2025-12-13 14:57:45,574 - WARNING - NaN/Inf loss detected at batch 1169, skipping
2025-12-13 14:57:45,595 - WARNING - NaN/Inf loss detected at batch 1170, skipping
2025-12-13 14:57:45,619 - WARNING - NaN/Inf loss detected at batch 1171, skipping
2025-12-13 14:57:45,642 - WARNING - NaN/Inf loss detected at batch 1172, skipping
2025-12-13 14:57:45,665 - WARNING - NaN/Inf loss detected at batch 1173, skipping
2025-12-13 14:57:45,689 - WARNING - NaN/Inf loss detected at batch 1174, skipping
2025-12-13 14:57:45,710 - WARNING - NaN/Inf loss detected at batch 1175, skipping
2025-12-13 14:57:45,732 - WARNING - NaN/Inf loss detected at batch 1176, skipping
2025-12-13 14:57:45,755 - WARNING - NaN/Inf loss detected at batch 1177, skipping
2025-12-13 14:57:45,777 - WARNING - NaN/Inf loss detected at batch 1178, skipping
2025-12-13 14:57:45,801 - WARNING - NaN/Inf loss detected at batch 1179, skipping
2025-12-13 14:57:45,823 - WARNING - NaN/Inf loss detected at batch 1180, skipping
2025-12-13 15:00:07,174 - INFO - Logging initialize successfully
2025-12-13 15:00:09,460 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 15:00:09,585 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 15:00:11,011 - INFO - Built 29000 samples from annotaions
2025-12-13 15:00:11,011 - INFO - Total train samples: 29000
2025-12-13 15:00:11,037 - INFO - Built 1014 samples from annotaions
2025-12-13 15:00:11,037 - INFO - Total val samples: 1014
2025-12-13 15:00:11,113 - INFO - Built 29000 samples for story 
2025-12-13 15:00:11,117 - INFO - Built 1014 samples for story 
2025-12-13 15:00:11,260 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 15:00:11,269 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 15:00:15,140 - INFO - No valid checkpoint found or resume disabled. Starting from scratch.
2025-12-13 15:00:33,629 - INFO - Epoch [1] Batch [0/7250] loss: 11.0633 
2025-12-13 15:00:39,538 - INFO - Epoch [1] Batch [100/7250] loss: 2.9150 
2025-12-13 15:00:45,406 - INFO - Epoch [1] Batch [200/7250] loss: 2.5626 
2025-12-13 15:00:51,307 - INFO - Epoch [1] Batch [300/7250] loss: 2.5759 
2025-12-13 15:00:57,205 - INFO - Epoch [1] Batch [400/7250] loss: 2.1976 
2025-12-13 15:01:03,113 - INFO - Epoch [1] Batch [500/7250] loss: 2.2632 
2025-12-13 15:01:08,960 - INFO - Epoch [1] Batch [600/7250] loss: 1.9096 
2025-12-13 15:01:14,856 - INFO - Epoch [1] Batch [700/7250] loss: 2.1349 
2025-12-13 15:01:20,736 - INFO - Epoch [1] Batch [800/7250] loss: 1.7152 
2025-12-13 15:01:26,629 - INFO - Epoch [1] Batch [900/7250] loss: 1.7579 
2025-12-13 15:01:32,620 - INFO - Epoch [1] Batch [1000/7250] loss: 1.7265 
2025-12-13 15:01:38,684 - INFO - Epoch [1] Batch [1100/7250] loss: 1.6815 
2025-12-13 15:01:44,757 - INFO - Epoch [1] Batch [1200/7250] loss: 1.6690 
2025-12-13 15:01:50,808 - INFO - Epoch [1] Batch [1300/7250] loss: 1.7883 
2025-12-13 15:01:56,868 - INFO - Epoch [1] Batch [1400/7250] loss: 1.7383 
2025-12-13 15:02:02,925 - INFO - Epoch [1] Batch [1500/7250] loss: 1.7726 
2025-12-13 15:02:08,972 - INFO - Epoch [1] Batch [1600/7250] loss: 1.7101 
2025-12-13 15:02:14,979 - INFO - Epoch [1] Batch [1700/7250] loss: 1.5785 
2025-12-13 15:02:20,995 - INFO - Epoch [1] Batch [1800/7250] loss: 1.7049 
2025-12-13 15:02:27,031 - INFO - Epoch [1] Batch [1900/7250] loss: 1.8239 
2025-12-13 15:02:33,030 - INFO - Epoch [1] Batch [2000/7250] loss: 1.9161 
2025-12-13 15:02:39,039 - INFO - Epoch [1] Batch [2100/7250] loss: 1.6421 
2025-12-13 15:02:45,052 - INFO - Epoch [1] Batch [2200/7250] loss: 1.5114 
2025-12-13 15:02:51,048 - INFO - Epoch [1] Batch [2300/7250] loss: 1.7226 
2025-12-13 15:02:57,027 - INFO - Epoch [1] Batch [2400/7250] loss: 1.6274 
2025-12-13 15:03:03,029 - INFO - Epoch [1] Batch [2500/7250] loss: 1.5772 
2025-12-13 15:03:09,023 - INFO - Epoch [1] Batch [2600/7250] loss: 1.8862 
2025-12-13 15:03:15,062 - INFO - Epoch [1] Batch [2700/7250] loss: 1.4261 
2025-12-13 15:03:21,117 - INFO - Epoch [1] Batch [2800/7250] loss: 1.6503 
2025-12-13 15:03:27,142 - INFO - Epoch [1] Batch [2900/7250] loss: 1.4368 
2025-12-13 15:03:33,145 - INFO - Epoch [1] Batch [3000/7250] loss: 1.6739 
2025-12-13 15:03:39,160 - INFO - Epoch [1] Batch [3100/7250] loss: 1.5832 
2025-12-13 15:03:45,201 - INFO - Epoch [1] Batch [3200/7250] loss: 1.6976 
2025-12-13 15:03:51,217 - INFO - Epoch [1] Batch [3300/7250] loss: 1.5096 
2025-12-13 15:03:57,224 - INFO - Epoch [1] Batch [3400/7250] loss: 1.6768 
2025-12-13 15:04:03,216 - INFO - Epoch [1] Batch [3500/7250] loss: 1.4841 
2025-12-13 15:04:09,230 - INFO - Epoch [1] Batch [3600/7250] loss: 1.5994 
2025-12-13 15:04:15,321 - INFO - Epoch [1] Batch [3700/7250] loss: 1.4683 
2025-12-13 15:04:21,332 - INFO - Epoch [1] Batch [3800/7250] loss: 1.5333 
2025-12-13 17:03:12,764 - INFO - Logging initialize successfully
2025-12-13 17:03:15,624 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 17:03:15,747 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 17:03:17,145 - INFO - Built 29000 samples from annotaions
2025-12-13 17:03:17,145 - INFO - Total train samples: 29000
2025-12-13 17:03:17,185 - INFO - Built 1014 samples from annotaions
2025-12-13 17:03:17,185 - INFO - Total val samples: 1014
2025-12-13 17:03:17,254 - INFO - Built 29000 samples for story 
2025-12-13 17:03:17,254 - INFO - Built 1014 samples for story 
2025-12-13 17:03:17,399 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 17:03:17,411 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 17:03:21,340 - INFO - No valid checkpoint found or resume disabled. Starting from scratch.
2025-12-13 17:03:40,515 - INFO - Epoch [1] Batch [0/7250] loss: 11.0421 
2025-12-13 17:03:46,290 - INFO - Epoch [1] Batch [100/7250] loss: 2.7509 
2025-12-13 17:03:52,050 - INFO - Epoch [1] Batch [200/7250] loss: 2.4855 
2025-12-13 17:03:57,853 - INFO - Epoch [1] Batch [300/7250] loss: 2.4163 
2025-12-13 17:04:03,681 - INFO - Epoch [1] Batch [400/7250] loss: 2.2961 
2025-12-13 17:04:09,546 - INFO - Epoch [1] Batch [500/7250] loss: 2.3361 
2025-12-13 17:04:15,293 - INFO - Epoch [1] Batch [600/7250] loss: 1.8647 
2025-12-13 17:04:21,078 - INFO - Epoch [1] Batch [700/7250] loss: 2.6582 
2025-12-13 17:04:26,845 - INFO - Epoch [1] Batch [800/7250] loss: 1.8423 
2025-12-13 17:04:32,737 - INFO - Epoch [1] Batch [900/7250] loss: 2.1075 
2025-12-13 17:04:38,538 - INFO - Epoch [1] Batch [1000/7250] loss: 1.6756 
2025-12-13 17:04:44,308 - INFO - Epoch [1] Batch [1100/7250] loss: 1.6604 
2025-12-13 17:04:50,028 - INFO - Epoch [1] Batch [1200/7250] loss: 1.6560 
2025-12-13 17:04:55,795 - INFO - Epoch [1] Batch [1300/7250] loss: 1.9663 
2025-12-13 17:05:01,554 - INFO - Epoch [1] Batch [1400/7250] loss: 1.6779 
2025-12-13 17:05:07,319 - INFO - Epoch [1] Batch [1500/7250] loss: 1.6400 
2025-12-13 17:05:13,115 - INFO - Epoch [1] Batch [1600/7250] loss: 1.5897 
2025-12-13 17:05:18,885 - INFO - Epoch [1] Batch [1700/7250] loss: 1.6213 
2025-12-13 17:05:24,688 - INFO - Epoch [1] Batch [1800/7250] loss: 1.7954 
2025-12-13 17:05:30,653 - INFO - Epoch [1] Batch [1900/7250] loss: 2.0465 
2025-12-13 17:05:36,575 - INFO - Epoch [1] Batch [2000/7250] loss: 1.7961 
2025-12-13 17:05:42,527 - INFO - Epoch [1] Batch [2100/7250] loss: 1.5064 
2025-12-13 17:05:48,522 - INFO - Epoch [1] Batch [2200/7250] loss: 1.4197 
2025-12-13 17:05:54,401 - INFO - Epoch [1] Batch [2300/7250] loss: 1.6399 
2025-12-13 17:06:00,313 - INFO - Epoch [1] Batch [2400/7250] loss: 1.6476 
2025-12-13 17:06:06,178 - INFO - Epoch [1] Batch [2500/7250] loss: 1.6011 
2025-12-13 17:06:12,056 - INFO - Epoch [1] Batch [2600/7250] loss: 1.5790 
2025-12-13 17:06:17,911 - INFO - Epoch [1] Batch [2700/7250] loss: 1.5510 
2025-12-13 17:06:23,787 - INFO - Epoch [1] Batch [2800/7250] loss: 1.5816 
2025-12-13 17:06:29,654 - INFO - Epoch [1] Batch [2900/7250] loss: 1.4860 
2025-12-13 17:06:35,503 - INFO - Epoch [1] Batch [3000/7250] loss: 1.6386 
2025-12-13 17:06:41,390 - INFO - Epoch [1] Batch [3100/7250] loss: 1.9081 
2025-12-13 17:06:47,233 - INFO - Epoch [1] Batch [3200/7250] loss: 1.4213 
2025-12-13 17:06:53,224 - INFO - Epoch [1] Batch [3300/7250] loss: 1.7621 
2025-12-13 17:06:59,078 - INFO - Epoch [1] Batch [3400/7250] loss: 1.4317 
2025-12-13 17:07:04,966 - INFO - Epoch [1] Batch [3500/7250] loss: 1.5593 
2025-12-13 17:07:10,842 - INFO - Epoch [1] Batch [3600/7250] loss: 1.5151 
2025-12-13 17:07:16,771 - INFO - Epoch [1] Batch [3700/7250] loss: 1.4675 
2025-12-13 17:07:23,116 - INFO - Epoch [1] Batch [3800/7250] loss: 1.5453 
2025-12-13 17:07:29,146 - INFO - Epoch [1] Batch [3900/7250] loss: 1.5814 
2025-12-13 17:07:35,246 - INFO - Epoch [1] Batch [4000/7250] loss: 1.5113 
2025-12-13 17:07:41,364 - INFO - Epoch [1] Batch [4100/7250] loss: 1.4195 
2025-12-13 17:07:47,440 - INFO - Epoch [1] Batch [4200/7250] loss: 1.7613 
2025-12-13 17:07:53,511 - INFO - Epoch [1] Batch [4300/7250] loss: 1.4697 
2025-12-13 17:07:59,637 - INFO - Epoch [1] Batch [4400/7250] loss: 1.5020 
2025-12-13 17:08:05,767 - INFO - Epoch [1] Batch [4500/7250] loss: 1.4691 
2025-12-13 17:08:11,833 - INFO - Epoch [1] Batch [4600/7250] loss: 1.7129 
2025-12-13 17:08:18,010 - INFO - Epoch [1] Batch [4700/7250] loss: 1.6141 
2025-12-13 17:08:24,164 - INFO - Epoch [1] Batch [4800/7250] loss: 1.4968 
2025-12-13 17:08:30,567 - INFO - Epoch [1] Batch [4900/7250] loss: 1.7235 
2025-12-13 17:09:22,049 - INFO - Epoch [1] Batch [5000/7250] loss: 1.5018 
2025-12-13 17:09:27,889 - INFO - Epoch [1] Batch [5100/7250] loss: 1.5010 
2025-12-13 17:09:33,847 - INFO - Epoch [1] Batch [5200/7250] loss: 1.4709 
2025-12-13 17:09:39,890 - INFO - Epoch [1] Batch [5300/7250] loss: 1.5816 
2025-12-13 17:09:45,998 - INFO - Epoch [1] Batch [5400/7250] loss: 1.5672 
2025-12-13 17:09:51,968 - INFO - Epoch [1] Batch [5500/7250] loss: 1.4191 
2025-12-13 17:09:57,883 - INFO - Epoch [1] Batch [5600/7250] loss: 1.4738 
2025-12-13 17:10:03,796 - INFO - Epoch [1] Batch [5700/7250] loss: 1.4177 
2025-12-13 17:10:09,723 - INFO - Epoch [1] Batch [5800/7250] loss: 1.4199 
2025-12-13 17:10:15,903 - INFO - Epoch [1] Batch [5900/7250] loss: 1.5047 
2025-12-13 17:10:22,166 - INFO - Epoch [1] Batch [6000/7250] loss: 1.8087 
2025-12-13 17:10:28,494 - INFO - Epoch [1] Batch [6100/7250] loss: 1.4871 
2025-12-13 17:10:34,618 - INFO - Epoch [1] Batch [6200/7250] loss: 1.4321 
2025-12-13 17:10:40,801 - INFO - Epoch [1] Batch [6300/7250] loss: 1.5000 
2025-12-13 17:10:47,062 - INFO - Epoch [1] Batch [6400/7250] loss: 1.4626 
2025-12-13 17:10:53,180 - INFO - Epoch [1] Batch [6500/7250] loss: 1.4804 
2025-12-13 17:10:59,402 - INFO - Epoch [1] Batch [6600/7250] loss: 1.4700 
2025-12-13 17:11:05,479 - INFO - Epoch [1] Batch [6700/7250] loss: 1.4185 
2025-12-13 17:11:11,450 - INFO - Epoch [1] Batch [6800/7250] loss: 1.5427 
2025-12-13 17:11:17,363 - INFO - Epoch [1] Batch [6900/7250] loss: 1.4179 
2025-12-13 17:11:23,290 - INFO - Epoch [1] Batch [7000/7250] loss: 1.5550 
2025-12-13 17:11:29,271 - INFO - Epoch [1] Batch [7100/7250] loss: 1.6948 
2025-12-13 17:11:35,196 - INFO - Epoch [1] Batch [7200/7250] loss: 1.5362 
2025-12-13 17:11:55,938 - INFO - Epoch [1] Batch [0/254] loss: 1.4289 
2025-12-13 17:11:56,271 - INFO - Epoch [1] Batch [10/254] loss: 1.4223 
2025-12-13 17:11:56,463 - INFO - Epoch [1] Batch [20/254] loss: 1.4468 
2025-12-13 17:11:56,675 - INFO - Epoch [1] Batch [30/254] loss: 1.6530 
2025-12-13 17:11:56,860 - INFO - Epoch [1] Batch [40/254] loss: 1.5636 
2025-12-13 17:11:57,056 - INFO - Epoch [1] Batch [50/254] loss: 1.4243 
2025-12-13 17:11:57,248 - INFO - Epoch [1] Batch [60/254] loss: 1.4727 
2025-12-13 17:11:57,445 - INFO - Epoch [1] Batch [70/254] loss: 1.4719 
2025-12-13 17:11:57,641 - INFO - Epoch [1] Batch [80/254] loss: 1.4177 
2025-12-13 17:11:57,839 - INFO - Epoch [1] Batch [90/254] loss: 1.4174 
2025-12-13 17:11:58,044 - INFO - Epoch [1] Batch [100/254] loss: 1.4993 
2025-12-13 17:11:58,245 - INFO - Epoch [1] Batch [110/254] loss: 1.4862 
2025-12-13 17:11:58,456 - INFO - Epoch [1] Batch [120/254] loss: 1.6136 
2025-12-13 17:11:58,644 - INFO - Epoch [1] Batch [130/254] loss: 1.5060 
2025-12-13 17:11:58,835 - INFO - Epoch [1] Batch [140/254] loss: 1.6773 
2025-12-13 17:11:59,035 - INFO - Epoch [1] Batch [150/254] loss: 1.4372 
2025-12-13 17:11:59,233 - INFO - Epoch [1] Batch [160/254] loss: 1.7358 
2025-12-13 17:11:59,418 - INFO - Epoch [1] Batch [170/254] loss: 1.5561 
2025-12-13 17:11:59,615 - INFO - Epoch [1] Batch [180/254] loss: 1.4568 
2025-12-13 17:11:59,801 - INFO - Epoch [1] Batch [190/254] loss: 1.5316 
2025-12-13 17:12:00,008 - INFO - Epoch [1] Batch [200/254] loss: 1.4948 
2025-12-13 17:12:00,213 - INFO - Epoch [1] Batch [210/254] loss: 1.5469 
2025-12-13 17:12:00,409 - INFO - Epoch [1] Batch [220/254] loss: 1.6692 
2025-12-13 17:12:00,609 - INFO - Epoch [1] Batch [230/254] loss: 1.6189 
2025-12-13 17:12:00,832 - INFO - Epoch [1] Batch [240/254] loss: 1.5010 
2025-12-13 17:12:01,028 - INFO - Epoch [1] Batch [250/254] loss: 1.5679 
2025-12-13 17:12:01,789 - INFO - Epoch: [1/10]Train loss: 1.7083 | Val loss: 1.5089
2025-12-13 17:12:01,789 - INFO - Validation loss improved from inf to 1.5089 Saving Best model.
2025-12-13 17:12:02,603 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 1)
2025-12-13 17:12:03,423 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 1)
2025-12-13 17:12:21,480 - INFO - Epoch [2] Batch [0/7250] loss: 1.4719 
2025-12-13 17:12:27,457 - INFO - Epoch [2] Batch [100/7250] loss: 1.4859 
2025-12-13 17:12:34,393 - INFO - Epoch [2] Batch [200/7250] loss: 1.4424 
2025-12-13 17:12:41,406 - INFO - Epoch [2] Batch [300/7250] loss: 1.4178 
2025-12-13 17:12:48,556 - INFO - Epoch [2] Batch [400/7250] loss: 1.5291 
2025-12-13 17:12:55,011 - INFO - Epoch [2] Batch [500/7250] loss: 1.4299 
2025-12-13 17:13:01,155 - INFO - Epoch [2] Batch [600/7250] loss: 1.4163 
2025-12-13 17:13:08,113 - INFO - Epoch [2] Batch [700/7250] loss: 1.4191 
2025-12-13 17:13:15,149 - INFO - Epoch [2] Batch [800/7250] loss: 1.4766 
2025-12-13 17:13:22,428 - INFO - Epoch [2] Batch [900/7250] loss: 1.4632 
2025-12-13 17:13:29,428 - INFO - Epoch [2] Batch [1000/7250] loss: 1.4742 
2025-12-13 17:13:36,399 - INFO - Epoch [2] Batch [1100/7250] loss: 1.4766 
2025-12-13 17:13:43,343 - INFO - Epoch [2] Batch [1200/7250] loss: 1.5231 
2025-12-13 17:13:50,423 - INFO - Epoch [2] Batch [1300/7250] loss: 1.5584 
2025-12-13 17:13:57,373 - INFO - Epoch [2] Batch [1400/7250] loss: 1.5687 
2025-12-13 17:14:04,291 - INFO - Epoch [2] Batch [1500/7250] loss: 1.4487 
2025-12-13 17:14:11,268 - INFO - Epoch [2] Batch [1600/7250] loss: 1.4507 
2025-12-13 17:14:18,517 - INFO - Epoch [2] Batch [1700/7250] loss: 1.4839 
2025-12-13 17:14:25,279 - INFO - Epoch [2] Batch [1800/7250] loss: 1.4156 
2025-12-13 17:14:31,770 - INFO - Epoch [2] Batch [1900/7250] loss: 1.4906 
2025-12-13 17:14:38,169 - INFO - Epoch [2] Batch [2000/7250] loss: 1.4802 
2025-12-13 17:14:45,408 - INFO - Epoch [2] Batch [2100/7250] loss: 1.4181 
2025-12-13 17:14:52,602 - INFO - Epoch [2] Batch [2200/7250] loss: 1.4148 
2025-12-13 17:14:59,650 - INFO - Epoch [2] Batch [2300/7250] loss: 1.4165 
2025-12-13 17:15:06,590 - INFO - Epoch [2] Batch [2400/7250] loss: 1.4460 
2025-12-13 17:15:13,619 - INFO - Epoch [2] Batch [2500/7250] loss: 1.4608 
2025-12-13 17:15:20,639 - INFO - Epoch [2] Batch [2600/7250] loss: 1.4206 
2025-12-13 17:15:27,535 - INFO - Epoch [2] Batch [2700/7250] loss: 1.4926 
2025-12-13 17:15:34,452 - INFO - Epoch [2] Batch [2800/7250] loss: 1.4270 
2025-12-13 17:15:41,667 - INFO - Epoch [2] Batch [2900/7250] loss: 1.4733 
2025-12-13 17:15:48,672 - INFO - Epoch [2] Batch [3000/7250] loss: 1.4658 
2025-12-13 17:15:55,714 - INFO - Epoch [2] Batch [3100/7250] loss: 1.4498 
2025-12-13 17:16:02,528 - INFO - Epoch [2] Batch [3200/7250] loss: 1.4580 
2025-12-13 17:16:09,499 - INFO - Epoch [2] Batch [3300/7250] loss: 1.4320 
2025-12-13 17:16:16,570 - INFO - Epoch [2] Batch [3400/7250] loss: 1.4170 
2025-12-13 17:16:23,511 - INFO - Epoch [2] Batch [3500/7250] loss: 1.4985 
2025-12-13 17:16:30,463 - INFO - Epoch [2] Batch [3600/7250] loss: 1.4545 
2025-12-13 17:16:37,413 - INFO - Epoch [2] Batch [3700/7250] loss: 1.5065 
2025-12-13 17:16:44,501 - INFO - Epoch [2] Batch [3800/7250] loss: 1.4795 
2025-12-13 17:16:51,476 - INFO - Epoch [2] Batch [3900/7250] loss: 1.5246 
2025-12-13 17:16:58,536 - INFO - Epoch [2] Batch [4000/7250] loss: 1.6052 
2025-12-13 17:17:05,365 - INFO - Epoch [2] Batch [4100/7250] loss: 1.4142 
2025-12-13 17:17:12,278 - INFO - Epoch [2] Batch [4200/7250] loss: 1.4132 
2025-12-13 17:17:19,374 - INFO - Epoch [2] Batch [4300/7250] loss: 1.7327 
2025-12-13 17:17:26,574 - INFO - Epoch [2] Batch [4400/7250] loss: 1.4155 
2025-12-13 17:17:34,094 - INFO - Epoch [2] Batch [4500/7250] loss: 1.4614 
2025-12-13 17:17:41,680 - INFO - Epoch [2] Batch [4600/7250] loss: 1.4678 
2025-12-13 17:17:49,055 - INFO - Epoch [2] Batch [4700/7250] loss: 1.4135 
2025-12-13 17:17:56,360 - INFO - Epoch [2] Batch [4800/7250] loss: 1.5475 
2025-12-13 17:18:03,560 - INFO - Epoch [2] Batch [4900/7250] loss: 1.4992 
2025-12-13 17:18:10,826 - INFO - Epoch [2] Batch [5000/7250] loss: 1.4146 
2025-12-13 17:18:18,090 - INFO - Epoch [2] Batch [5100/7250] loss: 1.4466 
2025-12-13 17:18:25,125 - INFO - Epoch [2] Batch [5200/7250] loss: 1.4875 
2025-12-13 17:18:32,233 - INFO - Epoch [2] Batch [5300/7250] loss: 1.5148 
2025-12-13 17:18:39,344 - INFO - Epoch [2] Batch [5400/7250] loss: 1.4772 
2025-12-13 17:18:45,615 - INFO - Epoch [2] Batch [5500/7250] loss: 1.5564 
2025-12-13 17:18:52,237 - INFO - Epoch [2] Batch [5600/7250] loss: 1.4155 
2025-12-13 17:18:58,861 - INFO - Epoch [2] Batch [5700/7250] loss: 1.4707 
2025-12-13 17:19:05,755 - INFO - Epoch [2] Batch [5800/7250] loss: 1.6158 
2025-12-13 17:19:13,487 - INFO - Epoch [2] Batch [5900/7250] loss: 1.4156 
2025-12-13 17:19:20,665 - INFO - Epoch [2] Batch [6000/7250] loss: 1.5462 
2025-12-13 17:19:27,992 - INFO - Epoch [2] Batch [6100/7250] loss: 1.4129 
2025-12-13 17:19:35,165 - INFO - Epoch [2] Batch [6200/7250] loss: 1.4798 
2025-12-13 17:19:41,706 - INFO - Epoch [2] Batch [6300/7250] loss: 1.4493 
2025-12-13 17:19:48,550 - INFO - Epoch [2] Batch [6400/7250] loss: 1.4152 
2025-12-13 17:19:55,683 - INFO - Epoch [2] Batch [6500/7250] loss: 1.4127 
2025-12-13 17:20:02,488 - INFO - Epoch [2] Batch [6600/7250] loss: 1.4991 
2025-12-13 17:20:09,419 - INFO - Epoch [2] Batch [6700/7250] loss: 1.4751 
2025-12-13 17:20:16,403 - INFO - Epoch [2] Batch [6800/7250] loss: 1.4147 
2025-12-13 17:20:23,418 - INFO - Epoch [2] Batch [6900/7250] loss: 1.4520 
2025-12-13 17:20:30,394 - INFO - Epoch [2] Batch [7000/7250] loss: 1.4282 
2025-12-13 17:20:37,438 - INFO - Epoch [2] Batch [7100/7250] loss: 1.4275 
2025-12-13 17:20:44,246 - INFO - Epoch [2] Batch [7200/7250] loss: 1.5945 
2025-12-13 17:21:12,469 - INFO - Epoch [2] Batch [0/254] loss: 1.4187 
2025-12-13 17:21:12,802 - INFO - Epoch [2] Batch [10/254] loss: 1.4196 
2025-12-13 17:21:13,069 - INFO - Epoch [2] Batch [20/254] loss: 1.4202 
2025-12-13 17:21:13,342 - INFO - Epoch [2] Batch [30/254] loss: 1.5707 
2025-12-13 17:21:13,598 - INFO - Epoch [2] Batch [40/254] loss: 1.5608 
2025-12-13 17:21:13,859 - INFO - Epoch [2] Batch [50/254] loss: 1.4169 
2025-12-13 17:21:14,115 - INFO - Epoch [2] Batch [60/254] loss: 1.4185 
2025-12-13 17:21:14,373 - INFO - Epoch [2] Batch [70/254] loss: 1.4175 
2025-12-13 17:21:14,634 - INFO - Epoch [2] Batch [80/254] loss: 1.4157 
2025-12-13 17:21:14,883 - INFO - Epoch [2] Batch [90/254] loss: 1.4162 
2025-12-13 17:21:15,133 - INFO - Epoch [2] Batch [100/254] loss: 1.4717 
2025-12-13 17:21:15,386 - INFO - Epoch [2] Batch [110/254] loss: 1.4630 
2025-12-13 17:21:15,634 - INFO - Epoch [2] Batch [120/254] loss: 1.4799 
2025-12-13 17:21:15,884 - INFO - Epoch [2] Batch [130/254] loss: 1.4763 
2025-12-13 17:21:16,137 - INFO - Epoch [2] Batch [140/254] loss: 1.6307 
2025-12-13 17:21:16,390 - INFO - Epoch [2] Batch [150/254] loss: 1.4192 
2025-12-13 17:21:16,643 - INFO - Epoch [2] Batch [160/254] loss: 1.6771 
2025-12-13 17:21:16,902 - INFO - Epoch [2] Batch [170/254] loss: 1.5031 
2025-12-13 17:21:17,156 - INFO - Epoch [2] Batch [180/254] loss: 1.4202 
2025-12-13 17:21:17,405 - INFO - Epoch [2] Batch [190/254] loss: 1.5014 
2025-12-13 17:21:17,662 - INFO - Epoch [2] Batch [200/254] loss: 1.4628 
2025-12-13 17:21:17,918 - INFO - Epoch [2] Batch [210/254] loss: 1.4874 
2025-12-13 17:21:18,185 - INFO - Epoch [2] Batch [220/254] loss: 1.5968 
2025-12-13 17:21:18,439 - INFO - Epoch [2] Batch [230/254] loss: 1.6021 
2025-12-13 17:21:18,695 - INFO - Epoch [2] Batch [240/254] loss: 1.4889 
2025-12-13 17:21:18,942 - INFO - Epoch [2] Batch [250/254] loss: 1.4885 
2025-12-13 17:21:19,866 - INFO - Epoch: [2/10]Train loss: 1.4722 | Val loss: 1.4782
2025-12-13 17:21:19,866 - INFO - Validation loss improved from 1.5089 to 1.4782 Saving Best model.
2025-12-13 17:21:20,941 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 2)
2025-12-13 17:21:21,886 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 2)
2025-12-13 17:21:39,939 - INFO - Epoch [3] Batch [0/7250] loss: 1.4288 
2025-12-13 17:21:46,198 - INFO - Epoch [3] Batch [100/7250] loss: 1.4167 
2025-12-13 17:21:52,904 - INFO - Epoch [3] Batch [200/7250] loss: 1.4602 
2025-12-13 17:21:59,022 - INFO - Epoch [3] Batch [300/7250] loss: 1.4405 
2025-12-13 17:22:05,087 - INFO - Epoch [3] Batch [400/7250] loss: 1.4905 
2025-12-13 17:22:11,116 - INFO - Epoch [3] Batch [500/7250] loss: 1.4135 
2025-12-13 17:22:17,116 - INFO - Epoch [3] Batch [600/7250] loss: 1.4129 
2025-12-13 17:22:23,148 - INFO - Epoch [3] Batch [700/7250] loss: 1.4219 
2025-12-13 17:22:30,078 - INFO - Epoch [3] Batch [800/7250] loss: 1.4336 
2025-12-13 17:22:36,155 - INFO - Epoch [3] Batch [900/7250] loss: 1.4128 
2025-12-13 17:22:42,191 - INFO - Epoch [3] Batch [1000/7250] loss: 1.4127 
2025-12-13 17:22:48,261 - INFO - Epoch [3] Batch [1100/7250] loss: 1.4537 
2025-12-13 17:22:54,282 - INFO - Epoch [3] Batch [1200/7250] loss: 1.4947 
2025-12-13 17:23:00,416 - INFO - Epoch [3] Batch [1300/7250] loss: 1.4148 
2025-12-13 17:23:06,436 - INFO - Epoch [3] Batch [1400/7250] loss: 1.4551 
2025-12-13 17:23:12,519 - INFO - Epoch [3] Batch [1500/7250] loss: 1.4534 
2025-12-13 17:23:18,684 - INFO - Epoch [3] Batch [1600/7250] loss: 1.4527 
2025-12-13 17:23:24,929 - INFO - Epoch [3] Batch [1700/7250] loss: 1.4177 
2025-12-13 17:23:31,179 - INFO - Epoch [3] Batch [1800/7250] loss: 1.4842 
2025-12-13 17:23:37,477 - INFO - Epoch [3] Batch [1900/7250] loss: 1.4214 
2025-12-13 17:23:43,712 - INFO - Epoch [3] Batch [2000/7250] loss: 1.4256 
2025-12-13 17:23:50,584 - INFO - Epoch [3] Batch [2100/7250] loss: 1.4165 
2025-12-13 17:23:57,296 - INFO - Epoch [3] Batch [2200/7250] loss: 1.4946 
2025-12-13 17:24:03,973 - INFO - Epoch [3] Batch [2300/7250] loss: 1.4199 
2025-12-13 17:24:10,361 - INFO - Epoch [3] Batch [2400/7250] loss: 1.4160 
2025-12-13 17:24:16,709 - INFO - Epoch [3] Batch [2500/7250] loss: 1.5322 
2025-12-13 17:24:23,048 - INFO - Epoch [3] Batch [2600/7250] loss: 1.4157 
2025-12-13 17:24:29,198 - INFO - Epoch [3] Batch [2700/7250] loss: 1.4297 
2025-12-13 17:24:35,426 - INFO - Epoch [3] Batch [2800/7250] loss: 1.4373 
2025-12-13 17:24:41,643 - INFO - Epoch [3] Batch [2900/7250] loss: 1.4136 
2025-12-13 17:26:43,922 - INFO - Logging initialize successfully
2025-12-13 17:26:46,701 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 17:26:46,803 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 17:26:48,188 - INFO - Built 29000 samples from annotaions
2025-12-13 17:26:48,194 - INFO - Total train samples: 29000
2025-12-13 17:26:48,231 - INFO - Built 1014 samples from annotaions
2025-12-13 17:26:48,232 - INFO - Total val samples: 1014
2025-12-13 17:26:48,287 - INFO - Built 29000 samples for story 
2025-12-13 17:26:48,298 - INFO - Built 1014 samples for story 
2025-12-13 17:26:48,446 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 17:26:48,453 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 17:26:52,159 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming at epoch 2. (Scheduler restored: True, Scaler restored: True)
2025-12-13 17:26:52,159 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming training from epoch 2, best loss tracked: inf
2025-12-13 17:27:10,064 - INFO - Epoch [2] Batch [0/7250] loss: 1.4865 
2025-12-13 17:27:16,072 - INFO - Epoch [2] Batch [100/7250] loss: 1.4259 
2025-12-13 17:27:22,082 - INFO - Epoch [2] Batch [200/7250] loss: 1.4134 
2025-12-13 17:27:28,233 - INFO - Epoch [2] Batch [300/7250] loss: 1.4144 
2025-12-13 17:27:34,476 - INFO - Epoch [2] Batch [400/7250] loss: 1.4169 
2025-12-13 17:27:40,896 - INFO - Epoch [2] Batch [500/7250] loss: 1.4527 
2025-12-13 17:27:47,066 - INFO - Epoch [2] Batch [600/7250] loss: 1.4558 
2025-12-13 17:27:53,424 - INFO - Epoch [2] Batch [700/7250] loss: 1.4237 
2025-12-13 17:27:59,831 - INFO - Epoch [2] Batch [800/7250] loss: 1.4814 
2025-12-13 17:28:06,093 - INFO - Epoch [2] Batch [900/7250] loss: 1.6669 
2025-12-13 17:28:12,426 - INFO - Epoch [2] Batch [1000/7250] loss: 1.4690 
2025-12-13 17:28:18,850 - INFO - Epoch [2] Batch [1100/7250] loss: 1.5909 
2025-12-13 17:28:25,080 - INFO - Epoch [2] Batch [1200/7250] loss: 1.4129 
2025-12-13 17:28:31,618 - INFO - Epoch [2] Batch [1300/7250] loss: 1.4132 
2025-12-13 17:28:37,964 - INFO - Epoch [2] Batch [1400/7250] loss: 1.4124 
2025-12-13 17:28:44,198 - INFO - Epoch [2] Batch [1500/7250] loss: 1.5799 
2025-12-13 17:28:50,547 - INFO - Epoch [2] Batch [1600/7250] loss: 1.4540 
2025-12-13 17:28:56,695 - INFO - Epoch [2] Batch [1700/7250] loss: 1.4116 
2025-12-13 17:29:02,877 - INFO - Epoch [2] Batch [1800/7250] loss: 1.4154 
2025-12-13 17:29:09,284 - INFO - Epoch [2] Batch [1900/7250] loss: 1.4135 
2025-12-13 17:29:15,571 - INFO - Epoch [2] Batch [2000/7250] loss: 1.4123 
2025-12-13 17:29:21,940 - INFO - Epoch [2] Batch [2100/7250] loss: 1.4148 
2025-12-13 17:29:28,394 - INFO - Epoch [2] Batch [2200/7250] loss: 1.4890 
2025-12-13 17:29:35,621 - INFO - Epoch [2] Batch [2300/7250] loss: 1.4949 
2025-12-13 17:29:42,826 - INFO - Epoch [2] Batch [2400/7250] loss: 1.4145 
2025-12-13 17:29:50,033 - INFO - Epoch [2] Batch [2500/7250] loss: 1.4861 
2025-12-13 17:29:57,173 - INFO - Epoch [2] Batch [2600/7250] loss: 1.4134 
2025-12-13 17:30:04,367 - INFO - Epoch [2] Batch [2700/7250] loss: 1.4624 
2025-12-13 17:30:11,756 - INFO - Epoch [2] Batch [2800/7250] loss: 1.4131 
2025-12-13 17:30:18,501 - INFO - Epoch [2] Batch [2900/7250] loss: 1.5316 
2025-12-13 17:30:25,014 - INFO - Epoch [2] Batch [3000/7250] loss: 1.4143 
2025-12-13 17:30:32,234 - INFO - Epoch [2] Batch [3100/7250] loss: 1.4113 
2025-12-13 17:30:40,107 - INFO - Epoch [2] Batch [3200/7250] loss: 1.4153 
2025-12-13 17:30:47,863 - INFO - Epoch [2] Batch [3300/7250] loss: 1.4621 
2025-12-13 17:30:55,575 - INFO - Epoch [2] Batch [3400/7250] loss: 1.4128 
2025-12-13 17:31:02,818 - INFO - Epoch [2] Batch [3500/7250] loss: 1.4872 
2025-12-13 17:31:10,120 - INFO - Epoch [2] Batch [3600/7250] loss: 1.5323 
2025-12-13 17:31:17,399 - INFO - Epoch [2] Batch [3700/7250] loss: 1.4149 
2025-12-13 17:31:24,617 - INFO - Epoch [2] Batch [3800/7250] loss: 1.4955 
2025-12-13 17:31:31,913 - INFO - Epoch [2] Batch [3900/7250] loss: 1.4132 
2025-12-13 17:31:39,209 - INFO - Epoch [2] Batch [4000/7250] loss: 1.5800 
2025-12-13 17:31:46,576 - INFO - Epoch [2] Batch [4100/7250] loss: 1.4508 
2025-12-13 17:31:53,936 - INFO - Epoch [2] Batch [4200/7250] loss: 1.4381 
2025-12-13 17:32:01,275 - INFO - Epoch [2] Batch [4300/7250] loss: 1.4139 
2025-12-13 17:32:07,785 - INFO - Epoch [2] Batch [4400/7250] loss: 1.4133 
2025-12-13 17:32:14,224 - INFO - Epoch [2] Batch [4500/7250] loss: 1.4134 
2025-12-13 17:32:21,539 - INFO - Epoch [2] Batch [4600/7250] loss: 1.4215 
2025-12-13 17:32:28,819 - INFO - Epoch [2] Batch [4700/7250] loss: 1.4114 
2025-12-13 17:32:36,001 - INFO - Epoch [2] Batch [4800/7250] loss: 1.6106 
2025-12-13 17:32:42,733 - INFO - Epoch [2] Batch [4900/7250] loss: 1.4166 
2025-12-13 17:32:48,909 - INFO - Epoch [2] Batch [5000/7250] loss: 1.4793 
2025-12-13 17:32:55,090 - INFO - Epoch [2] Batch [5100/7250] loss: 1.4128 
2025-12-13 17:33:01,356 - INFO - Epoch [2] Batch [5200/7250] loss: 1.4532 
2025-12-13 17:33:07,541 - INFO - Epoch [2] Batch [5300/7250] loss: 1.4112 
2025-12-13 17:33:13,734 - INFO - Epoch [2] Batch [5400/7250] loss: 1.4904 
2025-12-13 17:33:19,981 - INFO - Epoch [2] Batch [5500/7250] loss: 1.5761 
2025-12-13 17:33:26,314 - INFO - Epoch [2] Batch [5600/7250] loss: 1.4156 
2025-12-13 17:33:32,549 - INFO - Epoch [2] Batch [5700/7250] loss: 1.5210 
2025-12-13 17:33:38,941 - INFO - Epoch [2] Batch [5800/7250] loss: 1.4140 
2025-12-13 17:33:45,307 - INFO - Epoch [2] Batch [5900/7250] loss: 1.4167 
2025-12-13 17:33:51,611 - INFO - Epoch [2] Batch [6000/7250] loss: 1.4117 
2025-12-13 17:33:57,921 - INFO - Epoch [2] Batch [6100/7250] loss: 1.4155 
2025-12-13 17:34:04,133 - INFO - Epoch [2] Batch [6200/7250] loss: 1.4117 
2025-12-13 17:34:10,361 - INFO - Epoch [2] Batch [6300/7250] loss: 1.5766 
2025-12-13 17:34:16,622 - INFO - Epoch [2] Batch [6400/7250] loss: 1.4111 
2025-12-13 17:34:22,864 - INFO - Epoch [2] Batch [6500/7250] loss: 1.6682 
2025-12-13 17:34:29,084 - INFO - Epoch [2] Batch [6600/7250] loss: 1.4774 
2025-12-13 17:34:35,388 - INFO - Epoch [2] Batch [6700/7250] loss: 1.4342 
2025-12-13 17:34:41,704 - INFO - Epoch [2] Batch [6800/7250] loss: 1.4122 
2025-12-13 17:34:48,164 - INFO - Epoch [2] Batch [6900/7250] loss: 1.5022 
2025-12-13 17:34:54,656 - INFO - Epoch [2] Batch [7000/7250] loss: 1.4187 
2025-12-13 17:35:01,127 - INFO - Epoch [2] Batch [7100/7250] loss: 1.4145 
2025-12-13 17:35:07,576 - INFO - Epoch [2] Batch [7200/7250] loss: 1.4139 
2025-12-13 17:35:30,866 - INFO - Epoch [2] Batch [0/254] loss: 1.4164 
2025-12-13 17:35:31,089 - INFO - Epoch [2] Batch [10/254] loss: 1.4147 
2025-12-13 17:35:31,296 - INFO - Epoch [2] Batch [20/254] loss: 1.4135 
2025-12-13 17:35:31,512 - INFO - Epoch [2] Batch [30/254] loss: 1.4160 
2025-12-13 17:35:31,715 - INFO - Epoch [2] Batch [40/254] loss: 1.5644 
2025-12-13 17:35:31,920 - INFO - Epoch [2] Batch [50/254] loss: 1.4160 
2025-12-13 17:35:32,138 - INFO - Epoch [2] Batch [60/254] loss: 1.8417 
2025-12-13 17:35:32,345 - INFO - Epoch [2] Batch [70/254] loss: 1.4303 
2025-12-13 17:35:32,553 - INFO - Epoch [2] Batch [80/254] loss: 1.4169 
2025-12-13 17:35:32,764 - INFO - Epoch [2] Batch [90/254] loss: 1.4154 
2025-12-13 17:35:32,962 - INFO - Epoch [2] Batch [100/254] loss: 1.5172 
2025-12-13 17:35:33,167 - INFO - Epoch [2] Batch [110/254] loss: 1.4154 
2025-12-13 17:35:33,427 - INFO - Epoch [2] Batch [120/254] loss: 1.4175 
2025-12-13 17:35:33,685 - INFO - Epoch [2] Batch [130/254] loss: 1.4632 
2025-12-13 17:35:33,920 - INFO - Epoch [2] Batch [140/254] loss: 1.4165 
2025-12-13 17:35:34,129 - INFO - Epoch [2] Batch [150/254] loss: 1.4174 
2025-12-13 17:35:34,332 - INFO - Epoch [2] Batch [160/254] loss: 1.4181 
2025-12-13 17:35:34,545 - INFO - Epoch [2] Batch [170/254] loss: 1.4942 
2025-12-13 17:35:34,761 - INFO - Epoch [2] Batch [180/254] loss: 1.4179 
2025-12-13 17:35:34,968 - INFO - Epoch [2] Batch [190/254] loss: 1.4181 
2025-12-13 17:35:35,171 - INFO - Epoch [2] Batch [200/254] loss: 1.4809 
2025-12-13 17:35:35,373 - INFO - Epoch [2] Batch [210/254] loss: 1.4898 
2025-12-13 17:35:35,572 - INFO - Epoch [2] Batch [220/254] loss: 1.4418 
2025-12-13 17:35:35,771 - INFO - Epoch [2] Batch [230/254] loss: 1.5498 
2025-12-13 17:35:35,976 - INFO - Epoch [2] Batch [240/254] loss: 1.4165 
2025-12-13 17:35:36,184 - INFO - Epoch [2] Batch [250/254] loss: 1.4624 
2025-12-13 17:35:36,957 - INFO - Epoch: [2/10]Train loss: 1.4621 | Val loss: 1.4626
2025-12-13 17:35:38,758 - INFO - Epoch 2 Generated Story: markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings markings
2025-12-13 17:35:38,758 - INFO - Validation loss improved from inf to 1.4626 Saving Best model.
2025-12-13 17:35:39,577 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 2)
2025-12-13 17:35:40,376 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 2)
2025-12-13 17:35:58,123 - INFO - Epoch [3] Batch [0/7250] loss: 1.4123 
2025-12-13 17:37:36,981 - INFO - Logging initialize successfully
2025-12-13 17:37:39,294 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 17:37:39,410 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 17:37:40,790 - INFO - Built 29000 samples from annotaions
2025-12-13 17:37:40,790 - INFO - Total train samples: 29000
2025-12-13 17:37:40,836 - INFO - Built 1014 samples from annotaions
2025-12-13 17:37:40,836 - INFO - Total val samples: 1014
2025-12-13 17:37:40,888 - INFO - Built 29000 samples for story 
2025-12-13 17:37:40,904 - INFO - Built 1014 samples for story 
2025-12-13 17:37:41,045 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 17:37:41,045 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 17:37:44,904 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming at epoch 2. (Scheduler restored: True, Scaler restored: True)
2025-12-13 17:37:44,904 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming training from epoch 2, best loss tracked: inf
2025-12-13 17:38:02,921 - INFO - Epoch [2] Batch [0/7250] loss: 1.4127 
2025-12-13 17:38:18,228 - INFO - Epoch [2] Batch [250/7250] loss: 1.4765 
2025-12-13 17:38:33,972 - INFO - Epoch [2] Batch [500/7250] loss: 1.4120 
2025-12-13 17:38:49,249 - INFO - Epoch [2] Batch [750/7250] loss: 1.4124 
2025-12-13 17:39:04,576 - INFO - Epoch [2] Batch [1000/7250] loss: 1.4131 
2025-12-13 17:39:20,327 - INFO - Epoch [2] Batch [1250/7250] loss: 1.4468 
2025-12-13 17:39:38,204 - INFO - Epoch [2] Batch [1500/7250] loss: 1.4114 
2025-12-13 17:39:56,516 - INFO - Epoch [2] Batch [1750/7250] loss: 1.4786 
2025-12-13 17:40:14,837 - INFO - Epoch [2] Batch [2000/7250] loss: 1.4120 
2025-12-13 17:40:32,836 - INFO - Epoch [2] Batch [2250/7250] loss: 1.4551 
2025-12-13 17:40:50,655 - INFO - Epoch [2] Batch [2500/7250] loss: 1.5294 
2025-12-13 17:41:08,909 - INFO - Epoch [2] Batch [2750/7250] loss: 1.4838 
2025-12-13 17:41:26,351 - INFO - Epoch [2] Batch [3000/7250] loss: 1.4115 
2025-12-13 17:41:44,337 - INFO - Epoch [2] Batch [3250/7250] loss: 1.4127 
2025-12-13 17:42:02,464 - INFO - Epoch [2] Batch [3500/7250] loss: 1.4714 
2025-12-13 17:42:18,830 - INFO - Epoch [2] Batch [3750/7250] loss: 1.5673 
2025-12-13 17:42:35,645 - INFO - Epoch [2] Batch [4000/7250] loss: 1.4464 
2025-12-13 17:42:53,146 - INFO - Epoch [2] Batch [4250/7250] loss: 1.4886 
2025-12-13 17:43:08,614 - INFO - Epoch [2] Batch [4500/7250] loss: 1.4119 
2025-12-13 17:43:24,086 - INFO - Epoch [2] Batch [4750/7250] loss: 1.4123 
2025-12-13 17:43:39,601 - INFO - Epoch [2] Batch [5000/7250] loss: 1.4783 
2025-12-13 17:43:55,033 - INFO - Epoch [2] Batch [5250/7250] loss: 1.4146 
2025-12-13 17:44:10,556 - INFO - Epoch [2] Batch [5500/7250] loss: 1.4217 
2025-12-13 17:44:26,799 - INFO - Epoch [2] Batch [5750/7250] loss: 1.4411 
2025-12-13 17:44:44,540 - INFO - Epoch [2] Batch [6000/7250] loss: 1.6192 
2025-12-13 17:45:00,615 - INFO - Epoch [2] Batch [6250/7250] loss: 1.5461 
2025-12-13 17:45:16,217 - INFO - Epoch [2] Batch [6500/7250] loss: 1.4586 
2025-12-13 17:45:31,791 - INFO - Epoch [2] Batch [6750/7250] loss: 1.4144 
2025-12-13 17:45:47,356 - INFO - Epoch [2] Batch [7000/7250] loss: 1.4116 
2025-12-13 17:46:22,089 - INFO - Epoch [2] Batch [0/254] loss: 1.4141 
2025-12-13 17:46:22,288 - INFO - Epoch [2] Batch [10/254] loss: 1.4152 
2025-12-13 17:46:22,489 - INFO - Epoch [2] Batch [20/254] loss: 1.4832 
2025-12-13 17:46:22,693 - INFO - Epoch [2] Batch [30/254] loss: 1.4612 
2025-12-13 17:46:22,883 - INFO - Epoch [2] Batch [40/254] loss: 1.4910 
2025-12-13 17:46:23,082 - INFO - Epoch [2] Batch [50/254] loss: 1.4456 
2025-12-13 17:46:23,279 - INFO - Epoch [2] Batch [60/254] loss: 1.4606 
2025-12-13 17:46:23,476 - INFO - Epoch [2] Batch [70/254] loss: 1.4184 
2025-12-13 17:46:23,674 - INFO - Epoch [2] Batch [80/254] loss: 1.4124 
2025-12-13 17:46:23,869 - INFO - Epoch [2] Batch [90/254] loss: 1.4137 
2025-12-13 17:46:24,076 - INFO - Epoch [2] Batch [100/254] loss: 1.4227 
2025-12-13 17:46:24,274 - INFO - Epoch [2] Batch [110/254] loss: 1.4154 
2025-12-13 17:46:24,469 - INFO - Epoch [2] Batch [120/254] loss: 1.4181 
2025-12-13 17:46:24,679 - INFO - Epoch [2] Batch [130/254] loss: 1.5775 
2025-12-13 17:46:24,875 - INFO - Epoch [2] Batch [140/254] loss: 1.4183 
2025-12-13 17:46:25,062 - INFO - Epoch [2] Batch [150/254] loss: 1.4155 
2025-12-13 17:46:25,261 - INFO - Epoch [2] Batch [160/254] loss: 1.4155 
2025-12-13 17:46:25,455 - INFO - Epoch [2] Batch [170/254] loss: 1.4136 
2025-12-13 17:46:25,649 - INFO - Epoch [2] Batch [180/254] loss: 1.5030 
2025-12-13 17:46:25,851 - INFO - Epoch [2] Batch [190/254] loss: 1.4179 
2025-12-13 17:46:26,043 - INFO - Epoch [2] Batch [200/254] loss: 1.5276 
2025-12-13 17:46:26,247 - INFO - Epoch [2] Batch [210/254] loss: 1.4807 
2025-12-13 17:46:26,447 - INFO - Epoch [2] Batch [220/254] loss: 1.4643 
2025-12-13 17:46:26,640 - INFO - Epoch [2] Batch [230/254] loss: 1.4168 
2025-12-13 17:46:26,833 - INFO - Epoch [2] Batch [240/254] loss: 1.4687 
2025-12-13 17:46:27,014 - INFO - Epoch [2] Batch [250/254] loss: 1.4131 
2025-12-13 17:46:27,762 - INFO - Epoch: [2/10]Train loss: 1.4519 | Val loss: 1.4582
2025-12-13 17:46:30,003 - INFO - Epoch 2 Generated Story: fightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfightingfighting
2025-12-13 17:46:30,003 - INFO - Validation loss improved from inf to 1.4582 Saving Best model.
2025-12-13 17:46:30,916 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 2)
2025-12-13 17:46:31,740 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 2)
2025-12-13 17:46:49,569 - INFO - Epoch [3] Batch [0/7250] loss: 1.4119 
2025-12-13 17:47:05,642 - INFO - Epoch [3] Batch [250/7250] loss: 1.4099 
2025-12-13 17:47:21,363 - INFO - Epoch [3] Batch [500/7250] loss: 1.4146 
2025-12-13 17:47:37,366 - INFO - Epoch [3] Batch [750/7250] loss: 1.4113 
2025-12-13 17:47:53,683 - INFO - Epoch [3] Batch [1000/7250] loss: 1.4198 
2025-12-13 17:48:09,439 - INFO - Epoch [3] Batch [1250/7250] loss: 1.5100 
2025-12-13 17:48:25,536 - INFO - Epoch [3] Batch [1500/7250] loss: 1.4138 
2025-12-13 17:48:41,961 - INFO - Epoch [3] Batch [1750/7250] loss: 1.4144 
2025-12-13 17:48:57,799 - INFO - Epoch [3] Batch [2000/7250] loss: 1.4125 
2025-12-13 17:49:13,717 - INFO - Epoch [3] Batch [2250/7250] loss: 1.4110 
2025-12-13 17:49:29,887 - INFO - Epoch [3] Batch [2500/7250] loss: 1.4138 
2025-12-13 17:49:45,862 - INFO - Epoch [3] Batch [2750/7250] loss: 1.4568 
2025-12-13 17:50:01,618 - INFO - Epoch [3] Batch [3000/7250] loss: 1.4297 
2025-12-13 17:50:17,349 - INFO - Epoch [3] Batch [3250/7250] loss: 1.4524 
2025-12-13 17:50:33,445 - INFO - Epoch [3] Batch [3500/7250] loss: 1.4223 
2025-12-13 17:50:49,683 - INFO - Epoch [3] Batch [3750/7250] loss: 1.4115 
2025-12-13 17:51:05,835 - INFO - Epoch [3] Batch [4000/7250] loss: 1.4118 
2025-12-13 17:51:21,828 - INFO - Epoch [3] Batch [4250/7250] loss: 1.4728 
2025-12-13 17:51:37,619 - INFO - Epoch [3] Batch [4500/7250] loss: 1.4129 
2025-12-13 17:51:53,239 - INFO - Epoch [3] Batch [4750/7250] loss: 1.4099 
2025-12-13 17:52:08,825 - INFO - Epoch [3] Batch [5000/7250] loss: 1.4336 
2025-12-13 17:52:24,437 - INFO - Epoch [3] Batch [5250/7250] loss: 1.4136 
2025-12-13 17:52:40,110 - INFO - Epoch [3] Batch [5500/7250] loss: 1.4144 
2025-12-13 17:52:55,659 - INFO - Epoch [3] Batch [5750/7250] loss: 1.4405 
2025-12-13 17:53:11,324 - INFO - Epoch [3] Batch [6000/7250] loss: 1.4111 
2025-12-13 17:53:26,944 - INFO - Epoch [3] Batch [6250/7250] loss: 1.4522 
2025-12-13 17:53:42,543 - INFO - Epoch [3] Batch [6500/7250] loss: 1.4099 
2025-12-13 17:53:58,144 - INFO - Epoch [3] Batch [6750/7250] loss: 1.4251 
2025-12-13 17:54:13,741 - INFO - Epoch [3] Batch [7000/7250] loss: 1.4144 
2025-12-13 17:54:49,110 - INFO - Epoch [3] Batch [0/254] loss: 1.4134 
2025-12-13 17:54:49,379 - INFO - Epoch [3] Batch [10/254] loss: 1.4128 
2025-12-13 17:54:49,587 - INFO - Epoch [3] Batch [20/254] loss: 1.4678 
2025-12-13 17:54:49,787 - INFO - Epoch [3] Batch [30/254] loss: 1.4575 
2025-12-13 17:54:49,987 - INFO - Epoch [3] Batch [40/254] loss: 1.4897 
2025-12-13 17:54:50,190 - INFO - Epoch [3] Batch [50/254] loss: 1.4315 
2025-12-13 17:54:50,427 - INFO - Epoch [3] Batch [60/254] loss: 1.4475 
2025-12-13 17:54:50,640 - INFO - Epoch [3] Batch [70/254] loss: 1.4154 
2025-12-13 17:54:50,846 - INFO - Epoch [3] Batch [80/254] loss: 1.4144 
2025-12-13 17:54:51,080 - INFO - Epoch [3] Batch [90/254] loss: 1.4152 
2025-12-13 17:54:51,309 - INFO - Epoch [3] Batch [100/254] loss: 1.4169 
2025-12-13 17:54:51,520 - INFO - Epoch [3] Batch [110/254] loss: 1.4133 
2025-12-13 17:54:51,748 - INFO - Epoch [3] Batch [120/254] loss: 1.4151 
2025-12-13 17:54:51,985 - INFO - Epoch [3] Batch [130/254] loss: 1.5562 
2025-12-13 17:54:52,209 - INFO - Epoch [3] Batch [140/254] loss: 1.4179 
2025-12-13 17:54:52,418 - INFO - Epoch [3] Batch [150/254] loss: 1.4181 
2025-12-13 17:54:52,634 - INFO - Epoch [3] Batch [160/254] loss: 1.4132 
2025-12-13 17:54:52,836 - INFO - Epoch [3] Batch [170/254] loss: 1.4136 
2025-12-13 17:54:53,036 - INFO - Epoch [3] Batch [180/254] loss: 1.4885 
2025-12-13 17:54:53,233 - INFO - Epoch [3] Batch [190/254] loss: 1.4144 
2025-12-13 17:54:53,468 - INFO - Epoch [3] Batch [200/254] loss: 1.5162 
2025-12-13 17:54:53,693 - INFO - Epoch [3] Batch [210/254] loss: 1.4819 
2025-12-13 17:54:53,891 - INFO - Epoch [3] Batch [220/254] loss: 1.4861 
2025-12-13 17:54:54,099 - INFO - Epoch [3] Batch [230/254] loss: 1.4182 
2025-12-13 17:54:54,322 - INFO - Epoch [3] Batch [240/254] loss: 1.4534 
2025-12-13 17:54:54,525 - INFO - Epoch [3] Batch [250/254] loss: 1.4130 
2025-12-13 17:54:55,299 - INFO - Epoch: [3/10]Train loss: 1.4387 | Val loss: 1.4587
2025-12-13 17:54:56,999 - INFO - Epoch 3 Generated Story: grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded grounded
2025-12-13 17:54:57,000 - INFO - Validation loss did not improve. Current best loss: 1.4582
2025-12-13 17:54:57,925 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 3)
2025-12-13 17:55:16,424 - INFO - Epoch [4] Batch [0/7250] loss: 1.5081 
2025-12-13 17:55:32,022 - INFO - Epoch [4] Batch [250/7250] loss: 1.4139 
2025-12-13 17:55:47,415 - INFO - Epoch [4] Batch [500/7250] loss: 1.4294 
2025-12-13 17:56:02,879 - INFO - Epoch [4] Batch [750/7250] loss: 1.4145 
2025-12-13 17:56:18,345 - INFO - Epoch [4] Batch [1000/7250] loss: 1.4095 
2025-12-13 17:56:33,860 - INFO - Epoch [4] Batch [1250/7250] loss: 1.4132 
2025-12-13 17:56:49,303 - INFO - Epoch [4] Batch [1500/7250] loss: 1.4131 
2025-12-13 17:57:04,835 - INFO - Epoch [4] Batch [1750/7250] loss: 1.4115 
2025-12-13 17:57:20,352 - INFO - Epoch [4] Batch [2000/7250] loss: 1.4130 
2025-12-13 17:57:35,943 - INFO - Epoch [4] Batch [2250/7250] loss: 1.4150 
2025-12-13 17:57:51,471 - INFO - Epoch [4] Batch [2500/7250] loss: 1.4412 
2025-12-13 17:58:07,023 - INFO - Epoch [4] Batch [2750/7250] loss: 1.4397 
2025-12-13 17:58:22,581 - INFO - Epoch [4] Batch [3000/7250] loss: 1.4361 
2025-12-13 17:58:38,174 - INFO - Epoch [4] Batch [3250/7250] loss: 1.4133 
2025-12-13 17:58:53,880 - INFO - Epoch [4] Batch [3500/7250] loss: 1.4105 
2025-12-13 17:59:09,448 - INFO - Epoch [4] Batch [3750/7250] loss: 1.5341 
2025-12-13 17:59:25,322 - INFO - Epoch [4] Batch [4000/7250] loss: 1.4106 
2025-12-13 17:59:41,470 - INFO - Epoch [4] Batch [4250/7250] loss: 1.4182 
2025-12-13 17:59:57,656 - INFO - Epoch [4] Batch [4500/7250] loss: 1.4664 
2025-12-13 18:00:13,620 - INFO - Epoch [4] Batch [4750/7250] loss: 1.4132 
2025-12-13 18:00:29,146 - INFO - Epoch [4] Batch [5000/7250] loss: 1.4096 
2025-12-13 18:00:44,611 - INFO - Epoch [4] Batch [5250/7250] loss: 1.4572 
2025-12-13 18:01:00,150 - INFO - Epoch [4] Batch [5500/7250] loss: 1.4378 
2025-12-13 18:01:15,714 - INFO - Epoch [4] Batch [5750/7250] loss: 1.4577 
2025-12-13 18:01:31,545 - INFO - Epoch [4] Batch [6000/7250] loss: 1.5354 
2025-12-13 18:01:47,071 - INFO - Epoch [4] Batch [6250/7250] loss: 1.4113 
2025-12-13 18:02:02,593 - INFO - Epoch [4] Batch [6500/7250] loss: 1.4112 
2025-12-13 18:02:18,153 - INFO - Epoch [4] Batch [6750/7250] loss: 1.4115 
2025-12-13 18:02:33,799 - INFO - Epoch [4] Batch [7000/7250] loss: 1.4594 
2025-12-13 18:03:09,033 - INFO - Epoch [4] Batch [0/254] loss: 1.4126 
2025-12-13 18:03:09,423 - INFO - Epoch [4] Batch [10/254] loss: 1.4115 
2025-12-13 18:03:09,615 - INFO - Epoch [4] Batch [20/254] loss: 1.4484 
2025-12-13 18:03:09,814 - INFO - Epoch [4] Batch [30/254] loss: 1.4459 
2025-12-13 18:03:10,012 - INFO - Epoch [4] Batch [40/254] loss: 1.4906 
2025-12-13 18:03:10,210 - INFO - Epoch [4] Batch [50/254] loss: 1.4194 
2025-12-13 18:03:10,408 - INFO - Epoch [4] Batch [60/254] loss: 1.4325 
2025-12-13 18:03:10,611 - INFO - Epoch [4] Batch [70/254] loss: 1.4146 
2025-12-13 18:03:10,805 - INFO - Epoch [4] Batch [80/254] loss: 1.4137 
2025-12-13 18:03:11,003 - INFO - Epoch [4] Batch [90/254] loss: 1.4136 
2025-12-13 18:03:11,199 - INFO - Epoch [4] Batch [100/254] loss: 1.4156 
2025-12-13 18:03:11,401 - INFO - Epoch [4] Batch [110/254] loss: 1.4136 
2025-12-13 18:03:11,609 - INFO - Epoch [4] Batch [120/254] loss: 1.4150 
2025-12-13 18:03:11,819 - INFO - Epoch [4] Batch [130/254] loss: 1.5475 
2025-12-13 18:03:12,021 - INFO - Epoch [4] Batch [140/254] loss: 1.4163 
2025-12-13 18:03:12,224 - INFO - Epoch [4] Batch [150/254] loss: 1.4168 
2025-12-13 18:03:12,429 - INFO - Epoch [4] Batch [160/254] loss: 1.4136 
2025-12-13 18:03:12,632 - INFO - Epoch [4] Batch [170/254] loss: 1.4129 
2025-12-13 18:03:12,832 - INFO - Epoch [4] Batch [180/254] loss: 1.4791 
2025-12-13 18:03:13,034 - INFO - Epoch [4] Batch [190/254] loss: 1.4140 
2025-12-13 18:03:13,225 - INFO - Epoch [4] Batch [200/254] loss: 1.4998 
2025-12-13 18:03:13,426 - INFO - Epoch [4] Batch [210/254] loss: 1.4782 
2025-12-13 18:03:13,624 - INFO - Epoch [4] Batch [220/254] loss: 1.4871 
2025-12-13 18:03:13,829 - INFO - Epoch [4] Batch [230/254] loss: 1.4142 
2025-12-13 18:03:14,037 - INFO - Epoch [4] Batch [240/254] loss: 1.4448 
2025-12-13 18:03:14,251 - INFO - Epoch [4] Batch [250/254] loss: 1.4129 
2025-12-13 18:03:14,997 - INFO - Epoch: [4/10]Train loss: 1.4310 | Val loss: 1.4568
2025-12-13 18:03:16,644 - INFO - Epoch 4 Generated Story: boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling boiling
2025-12-13 18:03:16,644 - INFO - Validation loss improved from 1.4582 to 1.4568 Saving Best model.
2025-12-13 18:03:17,547 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 4)
2025-12-13 18:03:18,403 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 4)
2025-12-13 18:03:36,446 - INFO - Epoch [5] Batch [0/7250] loss: 1.4362 
2025-12-13 18:03:52,021 - INFO - Epoch [5] Batch [250/7250] loss: 1.4100 
2025-12-13 18:04:07,460 - INFO - Epoch [5] Batch [500/7250] loss: 1.4126 
2025-12-13 18:04:22,943 - INFO - Epoch [5] Batch [750/7250] loss: 1.4116 
2025-12-13 18:04:38,422 - INFO - Epoch [5] Batch [1000/7250] loss: 1.4181 
2025-12-13 18:04:54,046 - INFO - Epoch [5] Batch [1250/7250] loss: 1.4140 
2025-12-13 18:05:09,711 - INFO - Epoch [5] Batch [1500/7250] loss: 1.4133 
2025-12-13 18:05:25,645 - INFO - Epoch [5] Batch [1750/7250] loss: 1.5026 
2025-12-13 18:05:42,158 - INFO - Epoch [5] Batch [2000/7250] loss: 1.4185 
2025-12-13 18:05:58,250 - INFO - Epoch [5] Batch [2250/7250] loss: 1.4328 
2025-12-13 18:06:14,403 - INFO - Epoch [5] Batch [2500/7250] loss: 1.4107 
2025-12-13 18:06:29,920 - INFO - Epoch [5] Batch [2750/7250] loss: 1.4395 
2025-12-13 18:06:45,465 - INFO - Epoch [5] Batch [3000/7250] loss: 1.4111 
2025-12-13 18:07:00,959 - INFO - Epoch [5] Batch [3250/7250] loss: 1.4143 
2025-12-13 18:07:16,458 - INFO - Epoch [5] Batch [3500/7250] loss: 1.4141 
2025-12-13 18:07:31,953 - INFO - Epoch [5] Batch [3750/7250] loss: 1.4399 
2025-12-13 18:07:47,592 - INFO - Epoch [5] Batch [4000/7250] loss: 1.4134 
2025-12-13 18:08:03,364 - INFO - Epoch [5] Batch [4250/7250] loss: 1.4107 
2025-12-13 18:08:18,794 - INFO - Epoch [5] Batch [4500/7250] loss: 1.4107 
2025-12-13 18:08:34,379 - INFO - Epoch [5] Batch [4750/7250] loss: 1.4121 
2025-12-13 18:08:49,926 - INFO - Epoch [5] Batch [5000/7250] loss: 1.4116 
2025-12-13 18:09:05,517 - INFO - Epoch [5] Batch [5250/7250] loss: 1.4338 
2025-12-13 18:09:20,960 - INFO - Epoch [5] Batch [5500/7250] loss: 1.4116 
2025-12-13 18:09:36,437 - INFO - Epoch [5] Batch [5750/7250] loss: 1.4115 
2025-12-13 18:09:52,013 - INFO - Epoch [5] Batch [6000/7250] loss: 1.4108 
2025-12-13 18:10:07,482 - INFO - Epoch [5] Batch [6250/7250] loss: 1.4754 
2025-12-13 18:10:23,122 - INFO - Epoch [5] Batch [6500/7250] loss: 1.4134 
2025-12-13 18:10:38,774 - INFO - Epoch [5] Batch [6750/7250] loss: 1.4109 
2025-12-13 18:10:54,247 - INFO - Epoch [5] Batch [7000/7250] loss: 1.4574 
2025-12-13 18:11:27,762 - INFO - Epoch [5] Batch [0/254] loss: 1.4127 
2025-12-13 18:11:28,030 - INFO - Epoch [5] Batch [10/254] loss: 1.4115 
2025-12-13 18:11:28,218 - INFO - Epoch [5] Batch [20/254] loss: 1.4323 
2025-12-13 18:11:28,424 - INFO - Epoch [5] Batch [30/254] loss: 1.4360 
2025-12-13 18:11:28,620 - INFO - Epoch [5] Batch [40/254] loss: 1.4881 
2025-12-13 18:11:28,816 - INFO - Epoch [5] Batch [50/254] loss: 1.4142 
2025-12-13 18:11:29,010 - INFO - Epoch [5] Batch [60/254] loss: 1.4313 
2025-12-13 18:11:29,208 - INFO - Epoch [5] Batch [70/254] loss: 1.4139 
2025-12-13 18:11:29,404 - INFO - Epoch [5] Batch [80/254] loss: 1.4127 
2025-12-13 18:11:29,604 - INFO - Epoch [5] Batch [90/254] loss: 1.4125 
2025-12-13 18:11:29,795 - INFO - Epoch [5] Batch [100/254] loss: 1.4160 
2025-12-13 18:11:29,986 - INFO - Epoch [5] Batch [110/254] loss: 1.4124 
2025-12-13 18:11:30,181 - INFO - Epoch [5] Batch [120/254] loss: 1.4160 
2025-12-13 18:11:30,378 - INFO - Epoch [5] Batch [130/254] loss: 1.5397 
2025-12-13 18:11:30,573 - INFO - Epoch [5] Batch [140/254] loss: 1.4169 
2025-12-13 18:11:30,784 - INFO - Epoch [5] Batch [150/254] loss: 1.4150 
2025-12-13 18:11:30,990 - INFO - Epoch [5] Batch [160/254] loss: 1.4137 
2025-12-13 18:11:31,186 - INFO - Epoch [5] Batch [170/254] loss: 1.4119 
2025-12-13 18:11:31,378 - INFO - Epoch [5] Batch [180/254] loss: 1.4747 
2025-12-13 18:11:31,579 - INFO - Epoch [5] Batch [190/254] loss: 1.4150 
2025-12-13 18:11:31,775 - INFO - Epoch [5] Batch [200/254] loss: 1.4924 
2025-12-13 18:11:31,975 - INFO - Epoch [5] Batch [210/254] loss: 1.4786 
2025-12-13 18:11:32,183 - INFO - Epoch [5] Batch [220/254] loss: 1.4837 
2025-12-13 18:11:32,378 - INFO - Epoch [5] Batch [230/254] loss: 1.4146 
2025-12-13 18:11:32,574 - INFO - Epoch [5] Batch [240/254] loss: 1.4365 
2025-12-13 18:11:32,773 - INFO - Epoch [5] Batch [250/254] loss: 1.4129 
2025-12-13 18:11:33,504 - INFO - Epoch: [5/10]Train loss: 1.4263 | Val loss: 1.4551
2025-12-13 18:11:35,181 - INFO - Epoch 5 Generated Story: serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv serv
2025-12-13 18:11:35,181 - INFO - Validation loss improved from 1.4568 to 1.4551 Saving Best model.
2025-12-13 18:11:36,098 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 5)
2025-12-13 18:11:37,095 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 5)
2025-12-13 18:11:55,142 - INFO - Epoch [6] Batch [0/7250] loss: 1.4118 
2025-12-13 18:12:10,653 - INFO - Logging initialize successfully
2025-12-13 18:12:13,360 - INFO - Loaded 29000 rows for split='train'.
2025-12-13 18:12:13,491 - INFO - Loaded 1014 rows for split='val'.
2025-12-13 18:12:14,874 - INFO - Built 29000 samples from annotaions
2025-12-13 18:12:14,874 - INFO - Total train samples: 29000
2025-12-13 18:12:14,915 - INFO - Built 1014 samples from annotaions
2025-12-13 18:12:14,915 - INFO - Total val samples: 1014
2025-12-13 18:12:14,971 - INFO - Built 29000 samples for story 
2025-12-13 18:12:14,984 - INFO - Built 1014 samples for story 
2025-12-13 18:12:15,125 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-13 18:12:15,133 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-13 18:12:18,939 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming at epoch 5. (Scheduler restored: True, Scaler restored: True)
2025-12-13 18:12:18,939 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming training from epoch 5, best loss tracked: inf
2025-12-13 18:12:37,277 - INFO - Epoch [5] Batch [0/7250] loss: 1.4138 
2025-12-13 18:12:52,311 - INFO - Epoch [5] Batch [250/7250] loss: 1.4114 
2025-12-13 18:13:07,331 - INFO - Epoch [5] Batch [500/7250] loss: 1.4117 
2025-12-13 18:13:22,583 - INFO - Epoch [5] Batch [750/7250] loss: 1.4103 
2025-12-13 18:13:38,024 - INFO - Epoch [5] Batch [1000/7250] loss: 1.4104 
2025-12-13 18:13:53,555 - INFO - Epoch [5] Batch [1250/7250] loss: 1.4151 
2025-12-13 18:14:08,910 - INFO - Epoch [5] Batch [1500/7250] loss: 1.4103 
2025-12-13 18:14:24,216 - INFO - Epoch [5] Batch [1750/7250] loss: 1.4116 
2025-12-13 18:14:39,604 - INFO - Epoch [5] Batch [2000/7250] loss: 1.4100 
2025-12-13 18:14:54,980 - INFO - Epoch [5] Batch [2250/7250] loss: 1.4119 
2025-12-13 18:15:10,436 - INFO - Epoch [5] Batch [2500/7250] loss: 1.4109 
2025-12-13 18:15:25,750 - INFO - Epoch [5] Batch [2750/7250] loss: 1.4119 
2025-12-13 18:15:41,168 - INFO - Epoch [5] Batch [3000/7250] loss: 1.4131 
2025-12-13 18:15:56,478 - INFO - Epoch [5] Batch [3250/7250] loss: 1.4815 
2025-12-13 18:16:12,094 - INFO - Epoch [5] Batch [3500/7250] loss: 1.4112 
2025-12-13 18:16:28,134 - INFO - Epoch [5] Batch [3750/7250] loss: 1.4101 
2025-12-13 18:16:44,037 - INFO - Epoch [5] Batch [4000/7250] loss: 1.4701 
2025-12-13 18:17:00,001 - INFO - Epoch [5] Batch [4250/7250] loss: 1.4189 
2025-12-13 18:17:16,076 - INFO - Epoch [5] Batch [4500/7250] loss: 1.4206 
2025-12-13 18:17:32,218 - INFO - Epoch [5] Batch [4750/7250] loss: 1.4120 
2025-12-13 18:17:48,359 - INFO - Epoch [5] Batch [5000/7250] loss: 1.4875 
2025-12-13 18:18:04,094 - INFO - Epoch [5] Batch [5250/7250] loss: 1.4412 
2025-12-13 18:18:19,576 - INFO - Epoch [5] Batch [5500/7250] loss: 1.4115 
2025-12-13 18:18:35,044 - INFO - Epoch [5] Batch [5750/7250] loss: 1.4227 
2025-12-13 18:18:50,499 - INFO - Epoch [5] Batch [6000/7250] loss: 1.4099 
2025-12-13 18:19:05,996 - INFO - Epoch [5] Batch [6250/7250] loss: 1.4102 
2025-12-13 18:19:21,429 - INFO - Epoch [5] Batch [6500/7250] loss: 1.5554 
2025-12-13 18:19:36,833 - INFO - Epoch [5] Batch [6750/7250] loss: 1.4736 
2025-12-13 18:19:53,455 - INFO - Epoch [5] Batch [7000/7250] loss: 1.5024 
2025-12-13 18:20:37,285 - INFO - Epoch [5] Batch [0/254] loss: 1.4122 
2025-12-13 18:20:37,541 - INFO - Epoch [5] Batch [10/254] loss: 1.4121 
2025-12-13 18:20:37,800 - INFO - Epoch [5] Batch [20/254] loss: 1.4133 
2025-12-13 18:20:38,053 - INFO - Epoch [5] Batch [30/254] loss: 1.4752 
2025-12-13 18:20:38,314 - INFO - Epoch [5] Batch [40/254] loss: 1.4115 
2025-12-13 18:20:38,574 - INFO - Epoch [5] Batch [50/254] loss: 1.4146 
2025-12-13 18:20:38,828 - INFO - Epoch [5] Batch [60/254] loss: 1.4124 
2025-12-13 18:20:39,094 - INFO - Epoch [5] Batch [70/254] loss: 1.4125 
2025-12-13 18:20:39,352 - INFO - Epoch [5] Batch [80/254] loss: 1.4809 
2025-12-13 18:20:39,624 - INFO - Epoch [5] Batch [90/254] loss: 1.4114 
2025-12-13 18:20:39,889 - INFO - Epoch [5] Batch [100/254] loss: 1.4171 
2025-12-13 18:20:40,150 - INFO - Epoch [5] Batch [110/254] loss: 1.4136 
2025-12-13 18:20:40,403 - INFO - Epoch [5] Batch [120/254] loss: 1.5179 
2025-12-13 18:20:40,662 - INFO - Epoch [5] Batch [130/254] loss: 1.5752 
2025-12-13 18:20:40,923 - INFO - Epoch [5] Batch [140/254] loss: 1.4913 
2025-12-13 18:20:41,176 - INFO - Epoch [5] Batch [150/254] loss: 1.4133 
2025-12-13 18:20:41,426 - INFO - Epoch [5] Batch [160/254] loss: 1.4187 
2025-12-13 18:20:41,684 - INFO - Epoch [5] Batch [170/254] loss: 1.4114 
2025-12-13 18:20:41,951 - INFO - Epoch [5] Batch [180/254] loss: 1.4846 
2025-12-13 18:20:42,221 - INFO - Epoch [5] Batch [190/254] loss: 1.4566 
2025-12-13 18:20:42,482 - INFO - Epoch [5] Batch [200/254] loss: 1.5599 
2025-12-13 18:20:42,772 - INFO - Epoch [5] Batch [210/254] loss: 1.4838 
2025-12-13 18:20:43,029 - INFO - Epoch [5] Batch [220/254] loss: 1.5296 
2025-12-13 18:20:43,290 - INFO - Epoch [5] Batch [230/254] loss: 1.4124 
2025-12-13 18:20:43,549 - INFO - Epoch [5] Batch [240/254] loss: 1.4231 
2025-12-13 18:20:43,802 - INFO - Epoch [5] Batch [250/254] loss: 1.4129 
2025-12-13 18:20:44,737 - INFO - Epoch: [5/10]Train loss: 1.4423 | Val loss: 1.4478
2025-12-13 18:20:46,554 - INFO - Epoch 5 Generated Story: Lac tourists tourists
2025-12-13 18:20:46,554 - INFO - Validation loss improved from inf to 1.4478. Saving Best model.
2025-12-13 18:20:47,660 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 5)
2025-12-13 18:20:48,786 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 5)
2025-12-13 18:21:15,125 - INFO - Epoch [6] Batch [0/7250] loss: 1.4754 
2025-12-13 18:21:31,986 - INFO - Epoch [6] Batch [250/7250] loss: 1.4093 
2025-12-13 18:21:47,070 - INFO - Epoch [6] Batch [500/7250] loss: 1.4841 
2025-12-13 18:22:02,365 - INFO - Epoch [6] Batch [750/7250] loss: 1.4095 
2025-12-13 18:22:18,706 - INFO - Epoch [6] Batch [1000/7250] loss: 1.4111 
2025-12-13 18:22:34,796 - INFO - Epoch [6] Batch [1250/7250] loss: 1.4190 
2025-12-13 18:22:51,734 - INFO - Epoch [6] Batch [1500/7250] loss: 1.4148 
2025-12-13 18:23:10,710 - INFO - Epoch [6] Batch [1750/7250] loss: 1.4121 
2025-12-13 18:23:29,553 - INFO - Epoch [6] Batch [2000/7250] loss: 1.5205 
2025-12-13 18:23:47,931 - INFO - Epoch [6] Batch [2250/7250] loss: 1.4634 
2025-12-13 18:24:04,614 - INFO - Epoch [6] Batch [2500/7250] loss: 1.4112 
2025-12-13 18:24:21,058 - INFO - Epoch [6] Batch [2750/7250] loss: 1.4125 
2025-12-13 18:24:39,498 - INFO - Epoch [6] Batch [3000/7250] loss: 1.4118 
2025-12-13 18:24:57,811 - INFO - Epoch [6] Batch [3250/7250] loss: 1.4124 
2025-12-13 18:25:15,994 - INFO - Epoch [6] Batch [3500/7250] loss: 1.4111 
2025-12-13 18:25:34,378 - INFO - Epoch [6] Batch [3750/7250] loss: 1.4431 
2025-12-13 18:25:53,145 - INFO - Epoch [6] Batch [4000/7250] loss: 1.4117 
2025-12-13 18:26:11,405 - INFO - Epoch [6] Batch [4250/7250] loss: 1.4476 
2025-12-13 18:26:29,833 - INFO - Epoch [6] Batch [4500/7250] loss: 1.5520 
2025-12-13 18:26:48,536 - INFO - Epoch [6] Batch [4750/7250] loss: 1.4128 
2025-12-13 18:27:07,619 - INFO - Epoch [6] Batch [5000/7250] loss: 1.4099 
2025-12-13 18:27:26,123 - INFO - Epoch [6] Batch [5250/7250] loss: 1.4161 
2025-12-13 18:27:45,020 - INFO - Epoch [6] Batch [5500/7250] loss: 1.4103 
2025-12-13 18:28:03,668 - INFO - Epoch [6] Batch [5750/7250] loss: 1.4100 
2025-12-13 18:28:22,101 - INFO - Epoch [6] Batch [6000/7250] loss: 1.4924 
2025-12-13 18:28:40,349 - INFO - Epoch [6] Batch [6250/7250] loss: 1.4093 
2025-12-13 18:28:58,893 - INFO - Epoch [6] Batch [6500/7250] loss: 1.4372 
2025-12-13 18:29:17,022 - INFO - Epoch [6] Batch [6750/7250] loss: 1.4361 
2025-12-13 18:29:35,310 - INFO - Epoch [6] Batch [7000/7250] loss: 1.4988 
2025-12-13 18:30:22,021 - INFO - Epoch [6] Batch [0/254] loss: 1.4125 
2025-12-13 18:30:22,365 - INFO - Epoch [6] Batch [10/254] loss: 1.4105 
2025-12-13 18:30:22,622 - INFO - Epoch [6] Batch [20/254] loss: 1.4136 
2025-12-13 18:30:22,874 - INFO - Epoch [6] Batch [30/254] loss: 1.4730 
2025-12-13 18:30:23,139 - INFO - Epoch [6] Batch [40/254] loss: 1.4115 
2025-12-13 18:30:23,396 - INFO - Epoch [6] Batch [50/254] loss: 1.4137 
2025-12-13 18:30:23,658 - INFO - Epoch [6] Batch [60/254] loss: 1.4113 
2025-12-13 18:30:23,912 - INFO - Epoch [6] Batch [70/254] loss: 1.4106 
2025-12-13 18:30:24,169 - INFO - Epoch [6] Batch [80/254] loss: 1.4820 
2025-12-13 18:30:24,424 - INFO - Epoch [6] Batch [90/254] loss: 1.4125 
2025-12-13 18:30:24,680 - INFO - Epoch [6] Batch [100/254] loss: 1.4771 
2025-12-13 18:30:24,930 - INFO - Epoch [6] Batch [110/254] loss: 1.4114 
2025-12-13 18:30:25,184 - INFO - Epoch [6] Batch [120/254] loss: 1.4685 
2025-12-13 18:30:25,439 - INFO - Epoch [6] Batch [130/254] loss: 1.6218 
2025-12-13 18:30:25,692 - INFO - Epoch [6] Batch [140/254] loss: 1.4906 
2025-12-13 18:30:25,947 - INFO - Epoch [6] Batch [150/254] loss: 1.4124 
2025-12-13 18:30:26,203 - INFO - Epoch [6] Batch [160/254] loss: 1.4203 
2025-12-13 18:30:26,456 - INFO - Epoch [6] Batch [170/254] loss: 1.4117 
2025-12-13 18:30:26,714 - INFO - Epoch [6] Batch [180/254] loss: 1.4865 
2025-12-13 18:30:26,969 - INFO - Epoch [6] Batch [190/254] loss: 1.4268 
2025-12-13 18:30:27,226 - INFO - Epoch [6] Batch [200/254] loss: 1.5639 
2025-12-13 18:30:27,485 - INFO - Epoch [6] Batch [210/254] loss: 1.4902 
2025-12-13 18:30:27,743 - INFO - Epoch [6] Batch [220/254] loss: 1.5166 
2025-12-13 18:30:27,996 - INFO - Epoch [6] Batch [230/254] loss: 1.4119 
2025-12-13 18:30:28,249 - INFO - Epoch [6] Batch [240/254] loss: 1.4139 
2025-12-13 18:30:28,500 - INFO - Epoch [6] Batch [250/254] loss: 1.4142 2025-12-18 20:46:11,930 - INFO - Logging initialize successfully
2025-12-18 20:47:51,685 - INFO - Logging initialize successfully
2025-12-18 20:47:51,837 - INFO - Loaded 29000 rows for split='train'.
2025-12-18 20:47:51,970 - INFO - Loaded 1014 rows for split='val'.
2025-12-18 20:47:53,275 - INFO - Built 29000 samples from annotaions
2025-12-18 20:47:53,281 - INFO - Total train samples: 29000
2025-12-18 20:47:53,317 - INFO - Built 1014 samples from annotaions
2025-12-18 20:47:53,317 - INFO - Total val samples: 1014
2025-12-18 20:47:53,382 - INFO - Built 29000 samples for story 
2025-12-18 20:47:53,389 - INFO - Built 1014 samples for story 
2025-12-18 20:47:53,540 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-18 20:47:53,551 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-18 20:47:59,624 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-12-18 20:51:16,468 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-18 20:51:17,818 - WARNING - Model architecture mismatch. Starting from scratch. Error: Error(s) in loading state_dict for MultimodelGPT2:
	Missing key(s) in state_dict: "learned_bos_embedding", "gpt2.transformer.wte.weight", "gpt2.transformer.wpe.weight", "gpt2.transformer.h.0.ln_1.weight", "gpt2.transformer.h.0.ln_1.bias", "gpt2.transformer.h.0.attn.c_attn.weight", "gpt2.transformer.h.0.attn.c_attn.bias", "gpt2.transformer.h.0.attn.c_proj.weight", "gpt2.transformer.h.0.attn.c_proj.bias", "gpt2.transformer.h.0.ln_2.weight", "gpt2.transformer.h.0.ln_2.bias", "gpt2.transformer.h.0.mlp.c_fc.weight", "gpt2.transformer.h.0.mlp.c_fc.bias", "gpt2.transformer.h.0.mlp.c_proj.weight", "gpt2.transformer.h.0.mlp.c_proj.bias", "gpt2.transformer.h.1.ln_1.weight", "gpt2.transformer.h.1.ln_1.bias", "gpt2.transformer.h.1.attn.c_attn.weight", "gpt2.transformer.h.1.attn.c_attn.bias", "gpt2.transformer.h.1.attn.c_proj.weight", "gpt2.transformer.h.1.attn.c_proj.bias", "gpt2.transformer.h.1.ln_2.weight", "gpt2.transformer.h.1.ln_2.bias", "gpt2.transformer.h.1.mlp.c_fc.weight", "gpt2.transformer.h.1.mlp.c_fc.bias", "gpt2.transformer.h.1.mlp.c_proj.weight", "gpt2.transformer.h.1.mlp.c_proj.bias", "gpt2.transformer.h.2.ln_1.weight", "gpt2.transformer.h.2.ln_1.bias", "gpt2.transformer.h.2.attn.c_attn.weight", "gpt2.transformer.h.2.attn.c_attn.bias", "gpt2.transformer.h.2.attn.c_proj.weight", "gpt2.transformer.h.2.attn.c_proj.bias", "gpt2.transformer.h.2.ln_2.weight", "gpt2.transformer.h.2.ln_2.bias", "gpt2.transformer.h.2.mlp.c_fc.weight", "gpt2.transformer.h.2.mlp.c_fc.bias", "gpt2.transformer.h.2.mlp.c_proj.weight", "gpt2.transformer.h.2.mlp.c_proj.bias", "gpt2.transformer.h.3.ln_1.weight", "gpt2.transformer.h.3.ln_1.bias", "gpt2.transformer.h.3.attn.c_attn.weight", "gpt2.transformer.h.3.attn.c_attn.bias", "gpt2.transformer.h.3.attn.c_proj.weight", "gpt2.transformer.h.3.attn.c_proj.bias", "gpt2.transformer.h.3.ln_2.weight", "gpt2.transformer.h.3.ln_2.bias", "gpt2.transformer.h.3.mlp.c_fc.weight", "gpt2.transformer.h.3.mlp.c_fc.bias", "gpt2.transformer.h.3.mlp.c_proj.weight", "gpt2.transformer.h.3.mlp.c_proj.bias", "gpt2.transformer.h.4.ln_1.weight", "gpt2.transformer.h.4.ln_1.bias", "gpt2.transformer.h.4.attn.c_attn.weight", "gpt2.transformer.h.4.attn.c_attn.bias", "gpt2.transformer.h.4.attn.c_proj.weight", "gpt2.transformer.h.4.attn.c_proj.bias", "gpt2.transformer.h.4.ln_2.weight", "gpt2.transformer.h.4.ln_2.bias", "gpt2.transformer.h.4.mlp.c_fc.weight", "gpt2.transformer.h.4.mlp.c_fc.bias", "gpt2.transformer.h.4.mlp.c_proj.weight", "gpt2.transformer.h.4.mlp.c_proj.bias", "gpt2.transformer.h.5.ln_1.weight", "gpt2.transformer.h.5.ln_1.bias", "gpt2.transformer.h.5.attn.c_attn.weight", "gpt2.transformer.h.5.attn.c_attn.bias", "gpt2.transformer.h.5.attn.c_proj.weight", "gpt2.transformer.h.5.attn.c_proj.bias", "gpt2.transformer.h.5.ln_2.weight", "gpt2.transformer.h.5.ln_2.bias", "gpt2.transformer.h.5.mlp.c_fc.weight", "gpt2.transformer.h.5.mlp.c_fc.bias", "gpt2.transformer.h.5.mlp.c_proj.weight", "gpt2.transformer.h.5.mlp.c_proj.bias", "gpt2.transformer.h.6.ln_1.weight", "gpt2.transformer.h.6.ln_1.bias", "gpt2.transformer.h.6.attn.c_attn.weight", "gpt2.transformer.h.6.attn.c_attn.bias", "gpt2.transformer.h.6.attn.c_proj.weight", "gpt2.transformer.h.6.attn.c_proj.bias", "gpt2.transformer.h.6.ln_2.weight", "gpt2.transformer.h.6.ln_2.bias", "gpt2.transformer.h.6.mlp.c_fc.weight", "gpt2.transformer.h.6.mlp.c_fc.bias", "gpt2.transformer.h.6.mlp.c_proj.weight", "gpt2.transformer.h.6.mlp.c_proj.bias", "gpt2.transformer.h.7.ln_1.weight", "gpt2.transformer.h.7.ln_1.bias", "gpt2.transformer.h.7.attn.c_attn.weight", "gpt2.transformer.h.7.attn.c_attn.bias", "gpt2.transformer.h.7.attn.c_proj.weight", "gpt2.transformer.h.7.attn.c_proj.bias", "gpt2.transformer.h.7.ln_2.weight", "gpt2.transformer.h.7.ln_2.bias", "gpt2.transformer.h.7.mlp.c_fc.weight", "gpt2.transformer.h.7.mlp.c_fc.bias", "gpt2.transformer.h.7.mlp.c_proj.weight", "gpt2.transformer.h.7.mlp.c_proj.bias", "gpt2.transformer.h.8.ln_1.weight", "gpt2.transformer.h.8.ln_1.bias", "gpt2.transformer.h.8.attn.c_attn.weight", "gpt2.transformer.h.8.attn.c_attn.bias", "gpt2.transformer.h.8.attn.c_proj.weight", "gpt2.transformer.h.8.attn.c_proj.bias", "gpt2.transformer.h.8.ln_2.weight", "gpt2.transformer.h.8.ln_2.bias", "gpt2.transformer.h.8.mlp.c_fc.weight", "gpt2.transformer.h.8.mlp.c_fc.bias", "gpt2.transformer.h.8.mlp.c_proj.weight", "gpt2.transformer.h.8.mlp.c_proj.bias", "gpt2.transformer.h.9.ln_1.weight", "gpt2.transformer.h.9.ln_1.bias", "gpt2.transformer.h.9.attn.c_attn.weight", "gpt2.transformer.h.9.attn.c_attn.bias", "gpt2.transformer.h.9.attn.c_proj.weight", "gpt2.transformer.h.9.attn.c_proj.bias", "gpt2.transformer.h.9.ln_2.weight", "gpt2.transformer.h.9.ln_2.bias", "gpt2.transformer.h.9.mlp.c_fc.weight", "gpt2.transformer.h.9.mlp.c_fc.bias", "gpt2.transformer.h.9.mlp.c_proj.weight", "gpt2.transformer.h.9.mlp.c_proj.bias", "gpt2.transformer.h.10.ln_1.weight", "gpt2.transformer.h.10.ln_1.bias", "gpt2.transformer.h.10.attn.c_attn.weight", "gpt2.transformer.h.10.attn.c_attn.bias", "gpt2.transformer.h.10.attn.c_proj.weight", "gpt2.transformer.h.10.attn.c_proj.bias", "gpt2.transformer.h.10.ln_2.weight", "gpt2.transformer.h.10.ln_2.bias", "gpt2.transformer.h.10.mlp.c_fc.weight", "gpt2.transformer.h.10.mlp.c_fc.bias", "gpt2.transformer.h.10.mlp.c_proj.weight", "gpt2.transformer.h.10.mlp.c_proj.bias", "gpt2.transformer.h.11.ln_1.weight", "gpt2.transformer.h.11.ln_1.bias", "gpt2.transformer.h.11.attn.c_attn.weight", "gpt2.transformer.h.11.attn.c_attn.bias", "gpt2.transformer.h.11.attn.c_proj.weight", "gpt2.transformer.h.11.attn.c_proj.bias", "gpt2.transformer.h.11.ln_2.weight", "gpt2.transformer.h.11.ln_2.bias", "gpt2.transformer.h.11.mlp.c_fc.weight", "gpt2.transformer.h.11.mlp.c_fc.bias", "gpt2.transformer.h.11.mlp.c_proj.weight", "gpt2.transformer.h.11.mlp.c_proj.bias", "gpt2.transformer.ln_f.weight", "gpt2.transformer.ln_f.bias", "gpt2.lm_head.weight", "image_projection.0.weight", "image_projection.0.bias", "image_projection.1.weight", "image_projection.1.bias". 
	Unexpected key(s) in state_dict: "token_embedding.weight", "positional_embedding.weight", "lm_head.weight", "lm_head.bias", "transformer.layers.0.self_attn.in_proj_weight", "transformer.layers.0.self_attn.in_proj_bias", "transformer.layers.0.self_attn.out_proj.weight", "transformer.layers.0.self_attn.out_proj.bias", "transformer.layers.0.linear1.weight", "transformer.layers.0.linear1.bias", "transformer.layers.0.linear2.weight", "transformer.layers.0.linear2.bias", "transformer.layers.0.norm1.weight", "transformer.layers.0.norm1.bias", "transformer.layers.0.norm2.weight", "transformer.layers.0.norm2.bias", "transformer.layers.1.self_attn.in_proj_weight", "transformer.layers.1.self_attn.in_proj_bias", "transformer.layers.1.self_attn.out_proj.weight", "transformer.layers.1.self_attn.out_proj.bias", "transformer.layers.1.linear1.weight", "transformer.layers.1.linear1.bias", "transformer.layers.1.linear2.weight", "transformer.layers.1.linear2.bias", "transformer.layers.1.norm1.weight", "transformer.layers.1.norm1.bias", "transformer.layers.1.norm2.weight", "transformer.layers.1.norm2.bias", "transformer.layers.2.self_attn.in_proj_weight", "transformer.layers.2.self_attn.in_proj_bias", "transformer.layers.2.self_attn.out_proj.weight", "transformer.layers.2.self_attn.out_proj.bias", "transformer.layers.2.linear1.weight", "transformer.layers.2.linear1.bias", "transformer.layers.2.linear2.weight", "transformer.layers.2.linear2.bias", "transformer.layers.2.norm1.weight", "transformer.layers.2.norm1.bias", "transformer.layers.2.norm2.weight", "transformer.layers.2.norm2.bias", "image_projection.weight", "image_projection.bias". 
2025-12-18 20:54:23,387 - INFO - Logging initialize successfully
2025-12-18 20:54:23,507 - INFO - Loaded 29000 rows for split='train'.
2025-12-18 20:54:23,623 - INFO - Loaded 1014 rows for split='val'.
2025-12-18 20:54:24,921 - INFO - Built 29000 samples from annotaions
2025-12-18 20:54:24,923 - INFO - Total train samples: 29000
2025-12-18 20:54:24,957 - INFO - Built 1014 samples from annotaions
2025-12-18 20:54:24,957 - INFO - Total val samples: 1014
2025-12-18 20:54:25,026 - INFO - Built 29000 samples for story 
2025-12-18 20:54:25,029 - INFO - Built 1014 samples for story 
2025-12-18 20:54:25,174 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-18 20:54:25,179 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-18 20:54:29,376 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-18 20:54:30,206 - WARNING - Model architecture mismatch. Starting from scratch. Error: Error(s) in loading state_dict for MultimodelGPT2:
	Missing key(s) in state_dict: "learned_bos_embedding", "gpt2.transformer.wte.weight", "gpt2.transformer.wpe.weight", "gpt2.transformer.h.0.ln_1.weight", "gpt2.transformer.h.0.ln_1.bias", "gpt2.transformer.h.0.attn.c_attn.weight", "gpt2.transformer.h.0.attn.c_attn.bias", "gpt2.transformer.h.0.attn.c_proj.weight", "gpt2.transformer.h.0.attn.c_proj.bias", "gpt2.transformer.h.0.ln_2.weight", "gpt2.transformer.h.0.ln_2.bias", "gpt2.transformer.h.0.mlp.c_fc.weight", "gpt2.transformer.h.0.mlp.c_fc.bias", "gpt2.transformer.h.0.mlp.c_proj.weight", "gpt2.transformer.h.0.mlp.c_proj.bias", "gpt2.transformer.h.1.ln_1.weight", "gpt2.transformer.h.1.ln_1.bias", "gpt2.transformer.h.1.attn.c_attn.weight", "gpt2.transformer.h.1.attn.c_attn.bias", "gpt2.transformer.h.1.attn.c_proj.weight", "gpt2.transformer.h.1.attn.c_proj.bias", "gpt2.transformer.h.1.ln_2.weight", "gpt2.transformer.h.1.ln_2.bias", "gpt2.transformer.h.1.mlp.c_fc.weight", "gpt2.transformer.h.1.mlp.c_fc.bias", "gpt2.transformer.h.1.mlp.c_proj.weight", "gpt2.transformer.h.1.mlp.c_proj.bias", "gpt2.transformer.h.2.ln_1.weight", "gpt2.transformer.h.2.ln_1.bias", "gpt2.transformer.h.2.attn.c_attn.weight", "gpt2.transformer.h.2.attn.c_attn.bias", "gpt2.transformer.h.2.attn.c_proj.weight", "gpt2.transformer.h.2.attn.c_proj.bias", "gpt2.transformer.h.2.ln_2.weight", "gpt2.transformer.h.2.ln_2.bias", "gpt2.transformer.h.2.mlp.c_fc.weight", "gpt2.transformer.h.2.mlp.c_fc.bias", "gpt2.transformer.h.2.mlp.c_proj.weight", "gpt2.transformer.h.2.mlp.c_proj.bias", "gpt2.transformer.h.3.ln_1.weight", "gpt2.transformer.h.3.ln_1.bias", "gpt2.transformer.h.3.attn.c_attn.weight", "gpt2.transformer.h.3.attn.c_attn.bias", "gpt2.transformer.h.3.attn.c_proj.weight", "gpt2.transformer.h.3.attn.c_proj.bias", "gpt2.transformer.h.3.ln_2.weight", "gpt2.transformer.h.3.ln_2.bias", "gpt2.transformer.h.3.mlp.c_fc.weight", "gpt2.transformer.h.3.mlp.c_fc.bias", "gpt2.transformer.h.3.mlp.c_proj.weight", "gpt2.transformer.h.3.mlp.c_proj.bias", "gpt2.transformer.h.4.ln_1.weight", "gpt2.transformer.h.4.ln_1.bias", "gpt2.transformer.h.4.attn.c_attn.weight", "gpt2.transformer.h.4.attn.c_attn.bias", "gpt2.transformer.h.4.attn.c_proj.weight", "gpt2.transformer.h.4.attn.c_proj.bias", "gpt2.transformer.h.4.ln_2.weight", "gpt2.transformer.h.4.ln_2.bias", "gpt2.transformer.h.4.mlp.c_fc.weight", "gpt2.transformer.h.4.mlp.c_fc.bias", "gpt2.transformer.h.4.mlp.c_proj.weight", "gpt2.transformer.h.4.mlp.c_proj.bias", "gpt2.transformer.h.5.ln_1.weight", "gpt2.transformer.h.5.ln_1.bias", "gpt2.transformer.h.5.attn.c_attn.weight", "gpt2.transformer.h.5.attn.c_attn.bias", "gpt2.transformer.h.5.attn.c_proj.weight", "gpt2.transformer.h.5.attn.c_proj.bias", "gpt2.transformer.h.5.ln_2.weight", "gpt2.transformer.h.5.ln_2.bias", "gpt2.transformer.h.5.mlp.c_fc.weight", "gpt2.transformer.h.5.mlp.c_fc.bias", "gpt2.transformer.h.5.mlp.c_proj.weight", "gpt2.transformer.h.5.mlp.c_proj.bias", "gpt2.transformer.h.6.ln_1.weight", "gpt2.transformer.h.6.ln_1.bias", "gpt2.transformer.h.6.attn.c_attn.weight", "gpt2.transformer.h.6.attn.c_attn.bias", "gpt2.transformer.h.6.attn.c_proj.weight", "gpt2.transformer.h.6.attn.c_proj.bias", "gpt2.transformer.h.6.ln_2.weight", "gpt2.transformer.h.6.ln_2.bias", "gpt2.transformer.h.6.mlp.c_fc.weight", "gpt2.transformer.h.6.mlp.c_fc.bias", "gpt2.transformer.h.6.mlp.c_proj.weight", "gpt2.transformer.h.6.mlp.c_proj.bias", "gpt2.transformer.h.7.ln_1.weight", "gpt2.transformer.h.7.ln_1.bias", "gpt2.transformer.h.7.attn.c_attn.weight", "gpt2.transformer.h.7.attn.c_attn.bias", "gpt2.transformer.h.7.attn.c_proj.weight", "gpt2.transformer.h.7.attn.c_proj.bias", "gpt2.transformer.h.7.ln_2.weight", "gpt2.transformer.h.7.ln_2.bias", "gpt2.transformer.h.7.mlp.c_fc.weight", "gpt2.transformer.h.7.mlp.c_fc.bias", "gpt2.transformer.h.7.mlp.c_proj.weight", "gpt2.transformer.h.7.mlp.c_proj.bias", "gpt2.transformer.h.8.ln_1.weight", "gpt2.transformer.h.8.ln_1.bias", "gpt2.transformer.h.8.attn.c_attn.weight", "gpt2.transformer.h.8.attn.c_attn.bias", "gpt2.transformer.h.8.attn.c_proj.weight", "gpt2.transformer.h.8.attn.c_proj.bias", "gpt2.transformer.h.8.ln_2.weight", "gpt2.transformer.h.8.ln_2.bias", "gpt2.transformer.h.8.mlp.c_fc.weight", "gpt2.transformer.h.8.mlp.c_fc.bias", "gpt2.transformer.h.8.mlp.c_proj.weight", "gpt2.transformer.h.8.mlp.c_proj.bias", "gpt2.transformer.h.9.ln_1.weight", "gpt2.transformer.h.9.ln_1.bias", "gpt2.transformer.h.9.attn.c_attn.weight", "gpt2.transformer.h.9.attn.c_attn.bias", "gpt2.transformer.h.9.attn.c_proj.weight", "gpt2.transformer.h.9.attn.c_proj.bias", "gpt2.transformer.h.9.ln_2.weight", "gpt2.transformer.h.9.ln_2.bias", "gpt2.transformer.h.9.mlp.c_fc.weight", "gpt2.transformer.h.9.mlp.c_fc.bias", "gpt2.transformer.h.9.mlp.c_proj.weight", "gpt2.transformer.h.9.mlp.c_proj.bias", "gpt2.transformer.h.10.ln_1.weight", "gpt2.transformer.h.10.ln_1.bias", "gpt2.transformer.h.10.attn.c_attn.weight", "gpt2.transformer.h.10.attn.c_attn.bias", "gpt2.transformer.h.10.attn.c_proj.weight", "gpt2.transformer.h.10.attn.c_proj.bias", "gpt2.transformer.h.10.ln_2.weight", "gpt2.transformer.h.10.ln_2.bias", "gpt2.transformer.h.10.mlp.c_fc.weight", "gpt2.transformer.h.10.mlp.c_fc.bias", "gpt2.transformer.h.10.mlp.c_proj.weight", "gpt2.transformer.h.10.mlp.c_proj.bias", "gpt2.transformer.h.11.ln_1.weight", "gpt2.transformer.h.11.ln_1.bias", "gpt2.transformer.h.11.attn.c_attn.weight", "gpt2.transformer.h.11.attn.c_attn.bias", "gpt2.transformer.h.11.attn.c_proj.weight", "gpt2.transformer.h.11.attn.c_proj.bias", "gpt2.transformer.h.11.ln_2.weight", "gpt2.transformer.h.11.ln_2.bias", "gpt2.transformer.h.11.mlp.c_fc.weight", "gpt2.transformer.h.11.mlp.c_fc.bias", "gpt2.transformer.h.11.mlp.c_proj.weight", "gpt2.transformer.h.11.mlp.c_proj.bias", "gpt2.transformer.ln_f.weight", "gpt2.transformer.ln_f.bias", "gpt2.lm_head.weight", "image_projection.0.weight", "image_projection.0.bias", "image_projection.1.weight", "image_projection.1.bias". 
	Unexpected key(s) in state_dict: "token_embedding.weight", "positional_embedding.weight", "lm_head.weight", "lm_head.bias", "transformer.layers.0.self_attn.in_proj_weight", "transformer.layers.0.self_attn.in_proj_bias", "transformer.layers.0.self_attn.out_proj.weight", "transformer.layers.0.self_attn.out_proj.bias", "transformer.layers.0.linear1.weight", "transformer.layers.0.linear1.bias", "transformer.layers.0.linear2.weight", "transformer.layers.0.linear2.bias", "transformer.layers.0.norm1.weight", "transformer.layers.0.norm1.bias", "transformer.layers.0.norm2.weight", "transformer.layers.0.norm2.bias", "transformer.layers.1.self_attn.in_proj_weight", "transformer.layers.1.self_attn.in_proj_bias", "transformer.layers.1.self_attn.out_proj.weight", "transformer.layers.1.self_attn.out_proj.bias", "transformer.layers.1.linear1.weight", "transformer.layers.1.linear1.bias", "transformer.layers.1.linear2.weight", "transformer.layers.1.linear2.bias", "transformer.layers.1.norm1.weight", "transformer.layers.1.norm1.bias", "transformer.layers.1.norm2.weight", "transformer.layers.1.norm2.bias", "transformer.layers.2.self_attn.in_proj_weight", "transformer.layers.2.self_attn.in_proj_bias", "transformer.layers.2.self_attn.out_proj.weight", "transformer.layers.2.self_attn.out_proj.bias", "transformer.layers.2.linear1.weight", "transformer.layers.2.linear1.bias", "transformer.layers.2.linear2.weight", "transformer.layers.2.linear2.bias", "transformer.layers.2.norm1.weight", "transformer.layers.2.norm1.bias", "transformer.layers.2.norm2.weight", "transformer.layers.2.norm2.bias", "image_projection.weight", "image_projection.bias". 
2025-12-18 21:04:53,582 - INFO - Logging initialize successfully
2025-12-18 21:04:53,704 - INFO - Loaded 29000 rows for split='train'.
2025-12-18 21:04:53,824 - INFO - Loaded 1014 rows for split='val'.
2025-12-18 21:04:55,071 - INFO - Built 29000 samples from annotaions
2025-12-18 21:04:55,071 - INFO - Total train samples: 29000
2025-12-18 21:04:55,108 - INFO - Built 1014 samples from annotaions
2025-12-18 21:04:55,108 - INFO - Total val samples: 1014
2025-12-18 21:04:55,162 - INFO - Built 29000 samples for story 
2025-12-18 21:04:55,178 - INFO - Built 1014 samples for story 
2025-12-18 21:04:55,317 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-18 21:04:55,317 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-18 21:05:00,061 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-18 21:05:00,859 - WARNING - Model architecture mismatch. Starting from scratch. Error: Error(s) in loading state_dict for MultimodelGPT2:
	Missing key(s) in state_dict: "learned_bos_embedding", "gpt2.transformer.wte.weight", "gpt2.transformer.wpe.weight", "gpt2.transformer.h.0.ln_1.weight", "gpt2.transformer.h.0.ln_1.bias", "gpt2.transformer.h.0.attn.c_attn.weight", "gpt2.transformer.h.0.attn.c_attn.bias", "gpt2.transformer.h.0.attn.c_proj.weight", "gpt2.transformer.h.0.attn.c_proj.bias", "gpt2.transformer.h.0.ln_2.weight", "gpt2.transformer.h.0.ln_2.bias", "gpt2.transformer.h.0.mlp.c_fc.weight", "gpt2.transformer.h.0.mlp.c_fc.bias", "gpt2.transformer.h.0.mlp.c_proj.weight", "gpt2.transformer.h.0.mlp.c_proj.bias", "gpt2.transformer.h.1.ln_1.weight", "gpt2.transformer.h.1.ln_1.bias", "gpt2.transformer.h.1.attn.c_attn.weight", "gpt2.transformer.h.1.attn.c_attn.bias", "gpt2.transformer.h.1.attn.c_proj.weight", "gpt2.transformer.h.1.attn.c_proj.bias", "gpt2.transformer.h.1.ln_2.weight", "gpt2.transformer.h.1.ln_2.bias", "gpt2.transformer.h.1.mlp.c_fc.weight", "gpt2.transformer.h.1.mlp.c_fc.bias", "gpt2.transformer.h.1.mlp.c_proj.weight", "gpt2.transformer.h.1.mlp.c_proj.bias", "gpt2.transformer.h.2.ln_1.weight", "gpt2.transformer.h.2.ln_1.bias", "gpt2.transformer.h.2.attn.c_attn.weight", "gpt2.transformer.h.2.attn.c_attn.bias", "gpt2.transformer.h.2.attn.c_proj.weight", "gpt2.transformer.h.2.attn.c_proj.bias", "gpt2.transformer.h.2.ln_2.weight", "gpt2.transformer.h.2.ln_2.bias", "gpt2.transformer.h.2.mlp.c_fc.weight", "gpt2.transformer.h.2.mlp.c_fc.bias", "gpt2.transformer.h.2.mlp.c_proj.weight", "gpt2.transformer.h.2.mlp.c_proj.bias", "gpt2.transformer.h.3.ln_1.weight", "gpt2.transformer.h.3.ln_1.bias", "gpt2.transformer.h.3.attn.c_attn.weight", "gpt2.transformer.h.3.attn.c_attn.bias", "gpt2.transformer.h.3.attn.c_proj.weight", "gpt2.transformer.h.3.attn.c_proj.bias", "gpt2.transformer.h.3.ln_2.weight", "gpt2.transformer.h.3.ln_2.bias", "gpt2.transformer.h.3.mlp.c_fc.weight", "gpt2.transformer.h.3.mlp.c_fc.bias", "gpt2.transformer.h.3.mlp.c_proj.weight", "gpt2.transformer.h.3.mlp.c_proj.bias", "gpt2.transformer.h.4.ln_1.weight", "gpt2.transformer.h.4.ln_1.bias", "gpt2.transformer.h.4.attn.c_attn.weight", "gpt2.transformer.h.4.attn.c_attn.bias", "gpt2.transformer.h.4.attn.c_proj.weight", "gpt2.transformer.h.4.attn.c_proj.bias", "gpt2.transformer.h.4.ln_2.weight", "gpt2.transformer.h.4.ln_2.bias", "gpt2.transformer.h.4.mlp.c_fc.weight", "gpt2.transformer.h.4.mlp.c_fc.bias", "gpt2.transformer.h.4.mlp.c_proj.weight", "gpt2.transformer.h.4.mlp.c_proj.bias", "gpt2.transformer.h.5.ln_1.weight", "gpt2.transformer.h.5.ln_1.bias", "gpt2.transformer.h.5.attn.c_attn.weight", "gpt2.transformer.h.5.attn.c_attn.bias", "gpt2.transformer.h.5.attn.c_proj.weight", "gpt2.transformer.h.5.attn.c_proj.bias", "gpt2.transformer.h.5.ln_2.weight", "gpt2.transformer.h.5.ln_2.bias", "gpt2.transformer.h.5.mlp.c_fc.weight", "gpt2.transformer.h.5.mlp.c_fc.bias", "gpt2.transformer.h.5.mlp.c_proj.weight", "gpt2.transformer.h.5.mlp.c_proj.bias", "gpt2.transformer.h.6.ln_1.weight", "gpt2.transformer.h.6.ln_1.bias", "gpt2.transformer.h.6.attn.c_attn.weight", "gpt2.transformer.h.6.attn.c_attn.bias", "gpt2.transformer.h.6.attn.c_proj.weight", "gpt2.transformer.h.6.attn.c_proj.bias", "gpt2.transformer.h.6.ln_2.weight", "gpt2.transformer.h.6.ln_2.bias", "gpt2.transformer.h.6.mlp.c_fc.weight", "gpt2.transformer.h.6.mlp.c_fc.bias", "gpt2.transformer.h.6.mlp.c_proj.weight", "gpt2.transformer.h.6.mlp.c_proj.bias", "gpt2.transformer.h.7.ln_1.weight", "gpt2.transformer.h.7.ln_1.bias", "gpt2.transformer.h.7.attn.c_attn.weight", "gpt2.transformer.h.7.attn.c_attn.bias", "gpt2.transformer.h.7.attn.c_proj.weight", "gpt2.transformer.h.7.attn.c_proj.bias", "gpt2.transformer.h.7.ln_2.weight", "gpt2.transformer.h.7.ln_2.bias", "gpt2.transformer.h.7.mlp.c_fc.weight", "gpt2.transformer.h.7.mlp.c_fc.bias", "gpt2.transformer.h.7.mlp.c_proj.weight", "gpt2.transformer.h.7.mlp.c_proj.bias", "gpt2.transformer.h.8.ln_1.weight", "gpt2.transformer.h.8.ln_1.bias", "gpt2.transformer.h.8.attn.c_attn.weight", "gpt2.transformer.h.8.attn.c_attn.bias", "gpt2.transformer.h.8.attn.c_proj.weight", "gpt2.transformer.h.8.attn.c_proj.bias", "gpt2.transformer.h.8.ln_2.weight", "gpt2.transformer.h.8.ln_2.bias", "gpt2.transformer.h.8.mlp.c_fc.weight", "gpt2.transformer.h.8.mlp.c_fc.bias", "gpt2.transformer.h.8.mlp.c_proj.weight", "gpt2.transformer.h.8.mlp.c_proj.bias", "gpt2.transformer.h.9.ln_1.weight", "gpt2.transformer.h.9.ln_1.bias", "gpt2.transformer.h.9.attn.c_attn.weight", "gpt2.transformer.h.9.attn.c_attn.bias", "gpt2.transformer.h.9.attn.c_proj.weight", "gpt2.transformer.h.9.attn.c_proj.bias", "gpt2.transformer.h.9.ln_2.weight", "gpt2.transformer.h.9.ln_2.bias", "gpt2.transformer.h.9.mlp.c_fc.weight", "gpt2.transformer.h.9.mlp.c_fc.bias", "gpt2.transformer.h.9.mlp.c_proj.weight", "gpt2.transformer.h.9.mlp.c_proj.bias", "gpt2.transformer.h.10.ln_1.weight", "gpt2.transformer.h.10.ln_1.bias", "gpt2.transformer.h.10.attn.c_attn.weight", "gpt2.transformer.h.10.attn.c_attn.bias", "gpt2.transformer.h.10.attn.c_proj.weight", "gpt2.transformer.h.10.attn.c_proj.bias", "gpt2.transformer.h.10.ln_2.weight", "gpt2.transformer.h.10.ln_2.bias", "gpt2.transformer.h.10.mlp.c_fc.weight", "gpt2.transformer.h.10.mlp.c_fc.bias", "gpt2.transformer.h.10.mlp.c_proj.weight", "gpt2.transformer.h.10.mlp.c_proj.bias", "gpt2.transformer.h.11.ln_1.weight", "gpt2.transformer.h.11.ln_1.bias", "gpt2.transformer.h.11.attn.c_attn.weight", "gpt2.transformer.h.11.attn.c_attn.bias", "gpt2.transformer.h.11.attn.c_proj.weight", "gpt2.transformer.h.11.attn.c_proj.bias", "gpt2.transformer.h.11.ln_2.weight", "gpt2.transformer.h.11.ln_2.bias", "gpt2.transformer.h.11.mlp.c_fc.weight", "gpt2.transformer.h.11.mlp.c_fc.bias", "gpt2.transformer.h.11.mlp.c_proj.weight", "gpt2.transformer.h.11.mlp.c_proj.bias", "gpt2.transformer.ln_f.weight", "gpt2.transformer.ln_f.bias", "gpt2.lm_head.weight", "image_projection.0.weight", "image_projection.0.bias", "image_projection.1.weight", "image_projection.1.bias". 
	Unexpected key(s) in state_dict: "token_embedding.weight", "positional_embedding.weight", "lm_head.weight", "lm_head.bias", "transformer.layers.0.self_attn.in_proj_weight", "transformer.layers.0.self_attn.in_proj_bias", "transformer.layers.0.self_attn.out_proj.weight", "transformer.layers.0.self_attn.out_proj.bias", "transformer.layers.0.linear1.weight", "transformer.layers.0.linear1.bias", "transformer.layers.0.linear2.weight", "transformer.layers.0.linear2.bias", "transformer.layers.0.norm1.weight", "transformer.layers.0.norm1.bias", "transformer.layers.0.norm2.weight", "transformer.layers.0.norm2.bias", "transformer.layers.1.self_attn.in_proj_weight", "transformer.layers.1.self_attn.in_proj_bias", "transformer.layers.1.self_attn.out_proj.weight", "transformer.layers.1.self_attn.out_proj.bias", "transformer.layers.1.linear1.weight", "transformer.layers.1.linear1.bias", "transformer.layers.1.linear2.weight", "transformer.layers.1.linear2.bias", "transformer.layers.1.norm1.weight", "transformer.layers.1.norm1.bias", "transformer.layers.1.norm2.weight", "transformer.layers.1.norm2.bias", "transformer.layers.2.self_attn.in_proj_weight", "transformer.layers.2.self_attn.in_proj_bias", "transformer.layers.2.self_attn.out_proj.weight", "transformer.layers.2.self_attn.out_proj.bias", "transformer.layers.2.linear1.weight", "transformer.layers.2.linear1.bias", "transformer.layers.2.linear2.weight", "transformer.layers.2.linear2.bias", "transformer.layers.2.norm1.weight", "transformer.layers.2.norm1.bias", "transformer.layers.2.norm2.weight", "transformer.layers.2.norm2.bias", "image_projection.weight", "image_projection.bias". 
2025-12-18 21:50:29,177 - INFO - Logging initialize successfully
2025-12-18 21:50:29,362 - INFO - Loaded 29000 rows for split='train'.
2025-12-18 21:50:29,532 - INFO - Loaded 1014 rows for split='val'.
2025-12-18 21:50:31,805 - INFO - Built 29000 samples from annotaions
2025-12-18 21:50:31,806 - INFO - Total train samples: 29000
2025-12-18 21:50:31,871 - INFO - Built 1014 samples from annotaions
2025-12-18 21:50:31,872 - INFO - Total val samples: 1014
2025-12-18 21:50:31,977 - INFO - Built 29000 samples for story 
2025-12-18 21:50:31,981 - INFO - Built 1014 samples for story 
2025-12-18 21:50:32,199 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-18 21:50:32,207 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-18 21:50:38,239 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-18 21:50:39,257 - WARNING - Model architecture mismatch. Starting from scratch. Error: Error(s) in loading state_dict for MultimodelGPT2:
	Missing key(s) in state_dict: "learned_bos_embedding", "gpt2.transformer.wte.weight", "gpt2.transformer.wpe.weight", "gpt2.transformer.h.0.ln_1.weight", "gpt2.transformer.h.0.ln_1.bias", "gpt2.transformer.h.0.attn.c_attn.weight", "gpt2.transformer.h.0.attn.c_attn.bias", "gpt2.transformer.h.0.attn.c_proj.weight", "gpt2.transformer.h.0.attn.c_proj.bias", "gpt2.transformer.h.0.ln_2.weight", "gpt2.transformer.h.0.ln_2.bias", "gpt2.transformer.h.0.mlp.c_fc.weight", "gpt2.transformer.h.0.mlp.c_fc.bias", "gpt2.transformer.h.0.mlp.c_proj.weight", "gpt2.transformer.h.0.mlp.c_proj.bias", "gpt2.transformer.h.1.ln_1.weight", "gpt2.transformer.h.1.ln_1.bias", "gpt2.transformer.h.1.attn.c_attn.weight", "gpt2.transformer.h.1.attn.c_attn.bias", "gpt2.transformer.h.1.attn.c_proj.weight", "gpt2.transformer.h.1.attn.c_proj.bias", "gpt2.transformer.h.1.ln_2.weight", "gpt2.transformer.h.1.ln_2.bias", "gpt2.transformer.h.1.mlp.c_fc.weight", "gpt2.transformer.h.1.mlp.c_fc.bias", "gpt2.transformer.h.1.mlp.c_proj.weight", "gpt2.transformer.h.1.mlp.c_proj.bias", "gpt2.transformer.h.2.ln_1.weight", "gpt2.transformer.h.2.ln_1.bias", "gpt2.transformer.h.2.attn.c_attn.weight", "gpt2.transformer.h.2.attn.c_attn.bias", "gpt2.transformer.h.2.attn.c_proj.weight", "gpt2.transformer.h.2.attn.c_proj.bias", "gpt2.transformer.h.2.ln_2.weight", "gpt2.transformer.h.2.ln_2.bias", "gpt2.transformer.h.2.mlp.c_fc.weight", "gpt2.transformer.h.2.mlp.c_fc.bias", "gpt2.transformer.h.2.mlp.c_proj.weight", "gpt2.transformer.h.2.mlp.c_proj.bias", "gpt2.transformer.h.3.ln_1.weight", "gpt2.transformer.h.3.ln_1.bias", "gpt2.transformer.h.3.attn.c_attn.weight", "gpt2.transformer.h.3.attn.c_attn.bias", "gpt2.transformer.h.3.attn.c_proj.weight", "gpt2.transformer.h.3.attn.c_proj.bias", "gpt2.transformer.h.3.ln_2.weight", "gpt2.transformer.h.3.ln_2.bias", "gpt2.transformer.h.3.mlp.c_fc.weight", "gpt2.transformer.h.3.mlp.c_fc.bias", "gpt2.transformer.h.3.mlp.c_proj.weight", "gpt2.transformer.h.3.mlp.c_proj.bias", "gpt2.transformer.h.4.ln_1.weight", "gpt2.transformer.h.4.ln_1.bias", "gpt2.transformer.h.4.attn.c_attn.weight", "gpt2.transformer.h.4.attn.c_attn.bias", "gpt2.transformer.h.4.attn.c_proj.weight", "gpt2.transformer.h.4.attn.c_proj.bias", "gpt2.transformer.h.4.ln_2.weight", "gpt2.transformer.h.4.ln_2.bias", "gpt2.transformer.h.4.mlp.c_fc.weight", "gpt2.transformer.h.4.mlp.c_fc.bias", "gpt2.transformer.h.4.mlp.c_proj.weight", "gpt2.transformer.h.4.mlp.c_proj.bias", "gpt2.transformer.h.5.ln_1.weight", "gpt2.transformer.h.5.ln_1.bias", "gpt2.transformer.h.5.attn.c_attn.weight", "gpt2.transformer.h.5.attn.c_attn.bias", "gpt2.transformer.h.5.attn.c_proj.weight", "gpt2.transformer.h.5.attn.c_proj.bias", "gpt2.transformer.h.5.ln_2.weight", "gpt2.transformer.h.5.ln_2.bias", "gpt2.transformer.h.5.mlp.c_fc.weight", "gpt2.transformer.h.5.mlp.c_fc.bias", "gpt2.transformer.h.5.mlp.c_proj.weight", "gpt2.transformer.h.5.mlp.c_proj.bias", "gpt2.transformer.h.6.ln_1.weight", "gpt2.transformer.h.6.ln_1.bias", "gpt2.transformer.h.6.attn.c_attn.weight", "gpt2.transformer.h.6.attn.c_attn.bias", "gpt2.transformer.h.6.attn.c_proj.weight", "gpt2.transformer.h.6.attn.c_proj.bias", "gpt2.transformer.h.6.ln_2.weight", "gpt2.transformer.h.6.ln_2.bias", "gpt2.transformer.h.6.mlp.c_fc.weight", "gpt2.transformer.h.6.mlp.c_fc.bias", "gpt2.transformer.h.6.mlp.c_proj.weight", "gpt2.transformer.h.6.mlp.c_proj.bias", "gpt2.transformer.h.7.ln_1.weight", "gpt2.transformer.h.7.ln_1.bias", "gpt2.transformer.h.7.attn.c_attn.weight", "gpt2.transformer.h.7.attn.c_attn.bias", "gpt2.transformer.h.7.attn.c_proj.weight", "gpt2.transformer.h.7.attn.c_proj.bias", "gpt2.transformer.h.7.ln_2.weight", "gpt2.transformer.h.7.ln_2.bias", "gpt2.transformer.h.7.mlp.c_fc.weight", "gpt2.transformer.h.7.mlp.c_fc.bias", "gpt2.transformer.h.7.mlp.c_proj.weight", "gpt2.transformer.h.7.mlp.c_proj.bias", "gpt2.transformer.h.8.ln_1.weight", "gpt2.transformer.h.8.ln_1.bias", "gpt2.transformer.h.8.attn.c_attn.weight", "gpt2.transformer.h.8.attn.c_attn.bias", "gpt2.transformer.h.8.attn.c_proj.weight", "gpt2.transformer.h.8.attn.c_proj.bias", "gpt2.transformer.h.8.ln_2.weight", "gpt2.transformer.h.8.ln_2.bias", "gpt2.transformer.h.8.mlp.c_fc.weight", "gpt2.transformer.h.8.mlp.c_fc.bias", "gpt2.transformer.h.8.mlp.c_proj.weight", "gpt2.transformer.h.8.mlp.c_proj.bias", "gpt2.transformer.h.9.ln_1.weight", "gpt2.transformer.h.9.ln_1.bias", "gpt2.transformer.h.9.attn.c_attn.weight", "gpt2.transformer.h.9.attn.c_attn.bias", "gpt2.transformer.h.9.attn.c_proj.weight", "gpt2.transformer.h.9.attn.c_proj.bias", "gpt2.transformer.h.9.ln_2.weight", "gpt2.transformer.h.9.ln_2.bias", "gpt2.transformer.h.9.mlp.c_fc.weight", "gpt2.transformer.h.9.mlp.c_fc.bias", "gpt2.transformer.h.9.mlp.c_proj.weight", "gpt2.transformer.h.9.mlp.c_proj.bias", "gpt2.transformer.h.10.ln_1.weight", "gpt2.transformer.h.10.ln_1.bias", "gpt2.transformer.h.10.attn.c_attn.weight", "gpt2.transformer.h.10.attn.c_attn.bias", "gpt2.transformer.h.10.attn.c_proj.weight", "gpt2.transformer.h.10.attn.c_proj.bias", "gpt2.transformer.h.10.ln_2.weight", "gpt2.transformer.h.10.ln_2.bias", "gpt2.transformer.h.10.mlp.c_fc.weight", "gpt2.transformer.h.10.mlp.c_fc.bias", "gpt2.transformer.h.10.mlp.c_proj.weight", "gpt2.transformer.h.10.mlp.c_proj.bias", "gpt2.transformer.h.11.ln_1.weight", "gpt2.transformer.h.11.ln_1.bias", "gpt2.transformer.h.11.attn.c_attn.weight", "gpt2.transformer.h.11.attn.c_attn.bias", "gpt2.transformer.h.11.attn.c_proj.weight", "gpt2.transformer.h.11.attn.c_proj.bias", "gpt2.transformer.h.11.ln_2.weight", "gpt2.transformer.h.11.ln_2.bias", "gpt2.transformer.h.11.mlp.c_fc.weight", "gpt2.transformer.h.11.mlp.c_fc.bias", "gpt2.transformer.h.11.mlp.c_proj.weight", "gpt2.transformer.h.11.mlp.c_proj.bias", "gpt2.transformer.ln_f.weight", "gpt2.transformer.ln_f.bias", "gpt2.lm_head.weight", "image_projection.0.weight", "image_projection.0.bias", "image_projection.1.weight", "image_projection.1.bias". 
	Unexpected key(s) in state_dict: "token_embedding.weight", "positional_embedding.weight", "lm_head.weight", "lm_head.bias", "transformer.layers.0.self_attn.in_proj_weight", "transformer.layers.0.self_attn.in_proj_bias", "transformer.layers.0.self_attn.out_proj.weight", "transformer.layers.0.self_attn.out_proj.bias", "transformer.layers.0.linear1.weight", "transformer.layers.0.linear1.bias", "transformer.layers.0.linear2.weight", "transformer.layers.0.linear2.bias", "transformer.layers.0.norm1.weight", "transformer.layers.0.norm1.bias", "transformer.layers.0.norm2.weight", "transformer.layers.0.norm2.bias", "transformer.layers.1.self_attn.in_proj_weight", "transformer.layers.1.self_attn.in_proj_bias", "transformer.layers.1.self_attn.out_proj.weight", "transformer.layers.1.self_attn.out_proj.bias", "transformer.layers.1.linear1.weight", "transformer.layers.1.linear1.bias", "transformer.layers.1.linear2.weight", "transformer.layers.1.linear2.bias", "transformer.layers.1.norm1.weight", "transformer.layers.1.norm1.bias", "transformer.layers.1.norm2.weight", "transformer.layers.1.norm2.bias", "transformer.layers.2.self_attn.in_proj_weight", "transformer.layers.2.self_attn.in_proj_bias", "transformer.layers.2.self_attn.out_proj.weight", "transformer.layers.2.self_attn.out_proj.bias", "transformer.layers.2.linear1.weight", "transformer.layers.2.linear1.bias", "transformer.layers.2.linear2.weight", "transformer.layers.2.linear2.bias", "transformer.layers.2.norm1.weight", "transformer.layers.2.norm1.bias", "transformer.layers.2.norm2.weight", "transformer.layers.2.norm2.bias", "image_projection.weight", "image_projection.bias". 
2025-12-18 21:51:16,984 - ERROR - Model returned None for loss. Check if 'labels' are being passed to the forward pass.
2025-12-18 21:58:01,129 - INFO - Logging initialize successfully
2025-12-18 21:58:01,324 - INFO - Loaded 29000 rows for split='train'.
2025-12-18 21:58:01,509 - INFO - Loaded 1014 rows for split='val'.
2025-12-18 21:58:03,743 - INFO - Built 29000 samples from annotaions
2025-12-18 21:58:03,744 - INFO - Total train samples: 29000
2025-12-18 21:58:03,804 - INFO - Built 1014 samples from annotaions
2025-12-18 21:58:03,804 - INFO - Total val samples: 1014
2025-12-18 21:58:03,910 - INFO - Built 29000 samples for story 
2025-12-18 21:58:03,914 - INFO - Built 1014 samples for story 
2025-12-18 21:58:04,137 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-18 21:58:04,145 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-18 21:58:10,412 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-18 21:58:11,584 - WARNING - Model architecture mismatch. Starting from scratch. Error: Error(s) in loading state_dict for MultimodelGPT2:
	Missing key(s) in state_dict: "learned_bos_embedding", "gpt2.transformer.wte.weight", "gpt2.transformer.wpe.weight", "gpt2.transformer.h.0.ln_1.weight", "gpt2.transformer.h.0.ln_1.bias", "gpt2.transformer.h.0.attn.c_attn.weight", "gpt2.transformer.h.0.attn.c_attn.bias", "gpt2.transformer.h.0.attn.c_proj.weight", "gpt2.transformer.h.0.attn.c_proj.bias", "gpt2.transformer.h.0.ln_2.weight", "gpt2.transformer.h.0.ln_2.bias", "gpt2.transformer.h.0.mlp.c_fc.weight", "gpt2.transformer.h.0.mlp.c_fc.bias", "gpt2.transformer.h.0.mlp.c_proj.weight", "gpt2.transformer.h.0.mlp.c_proj.bias", "gpt2.transformer.h.1.ln_1.weight", "gpt2.transformer.h.1.ln_1.bias", "gpt2.transformer.h.1.attn.c_attn.weight", "gpt2.transformer.h.1.attn.c_attn.bias", "gpt2.transformer.h.1.attn.c_proj.weight", "gpt2.transformer.h.1.attn.c_proj.bias", "gpt2.transformer.h.1.ln_2.weight", "gpt2.transformer.h.1.ln_2.bias", "gpt2.transformer.h.1.mlp.c_fc.weight", "gpt2.transformer.h.1.mlp.c_fc.bias", "gpt2.transformer.h.1.mlp.c_proj.weight", "gpt2.transformer.h.1.mlp.c_proj.bias", "gpt2.transformer.h.2.ln_1.weight", "gpt2.transformer.h.2.ln_1.bias", "gpt2.transformer.h.2.attn.c_attn.weight", "gpt2.transformer.h.2.attn.c_attn.bias", "gpt2.transformer.h.2.attn.c_proj.weight", "gpt2.transformer.h.2.attn.c_proj.bias", "gpt2.transformer.h.2.ln_2.weight", "gpt2.transformer.h.2.ln_2.bias", "gpt2.transformer.h.2.mlp.c_fc.weight", "gpt2.transformer.h.2.mlp.c_fc.bias", "gpt2.transformer.h.2.mlp.c_proj.weight", "gpt2.transformer.h.2.mlp.c_proj.bias", "gpt2.transformer.h.3.ln_1.weight", "gpt2.transformer.h.3.ln_1.bias", "gpt2.transformer.h.3.attn.c_attn.weight", "gpt2.transformer.h.3.attn.c_attn.bias", "gpt2.transformer.h.3.attn.c_proj.weight", "gpt2.transformer.h.3.attn.c_proj.bias", "gpt2.transformer.h.3.ln_2.weight", "gpt2.transformer.h.3.ln_2.bias", "gpt2.transformer.h.3.mlp.c_fc.weight", "gpt2.transformer.h.3.mlp.c_fc.bias", "gpt2.transformer.h.3.mlp.c_proj.weight", "gpt2.transformer.h.3.mlp.c_proj.bias", "gpt2.transformer.h.4.ln_1.weight", "gpt2.transformer.h.4.ln_1.bias", "gpt2.transformer.h.4.attn.c_attn.weight", "gpt2.transformer.h.4.attn.c_attn.bias", "gpt2.transformer.h.4.attn.c_proj.weight", "gpt2.transformer.h.4.attn.c_proj.bias", "gpt2.transformer.h.4.ln_2.weight", "gpt2.transformer.h.4.ln_2.bias", "gpt2.transformer.h.4.mlp.c_fc.weight", "gpt2.transformer.h.4.mlp.c_fc.bias", "gpt2.transformer.h.4.mlp.c_proj.weight", "gpt2.transformer.h.4.mlp.c_proj.bias", "gpt2.transformer.h.5.ln_1.weight", "gpt2.transformer.h.5.ln_1.bias", "gpt2.transformer.h.5.attn.c_attn.weight", "gpt2.transformer.h.5.attn.c_attn.bias", "gpt2.transformer.h.5.attn.c_proj.weight", "gpt2.transformer.h.5.attn.c_proj.bias", "gpt2.transformer.h.5.ln_2.weight", "gpt2.transformer.h.5.ln_2.bias", "gpt2.transformer.h.5.mlp.c_fc.weight", "gpt2.transformer.h.5.mlp.c_fc.bias", "gpt2.transformer.h.5.mlp.c_proj.weight", "gpt2.transformer.h.5.mlp.c_proj.bias", "gpt2.transformer.h.6.ln_1.weight", "gpt2.transformer.h.6.ln_1.bias", "gpt2.transformer.h.6.attn.c_attn.weight", "gpt2.transformer.h.6.attn.c_attn.bias", "gpt2.transformer.h.6.attn.c_proj.weight", "gpt2.transformer.h.6.attn.c_proj.bias", "gpt2.transformer.h.6.ln_2.weight", "gpt2.transformer.h.6.ln_2.bias", "gpt2.transformer.h.6.mlp.c_fc.weight", "gpt2.transformer.h.6.mlp.c_fc.bias", "gpt2.transformer.h.6.mlp.c_proj.weight", "gpt2.transformer.h.6.mlp.c_proj.bias", "gpt2.transformer.h.7.ln_1.weight", "gpt2.transformer.h.7.ln_1.bias", "gpt2.transformer.h.7.attn.c_attn.weight", "gpt2.transformer.h.7.attn.c_attn.bias", "gpt2.transformer.h.7.attn.c_proj.weight", "gpt2.transformer.h.7.attn.c_proj.bias", "gpt2.transformer.h.7.ln_2.weight", "gpt2.transformer.h.7.ln_2.bias", "gpt2.transformer.h.7.mlp.c_fc.weight", "gpt2.transformer.h.7.mlp.c_fc.bias", "gpt2.transformer.h.7.mlp.c_proj.weight", "gpt2.transformer.h.7.mlp.c_proj.bias", "gpt2.transformer.h.8.ln_1.weight", "gpt2.transformer.h.8.ln_1.bias", "gpt2.transformer.h.8.attn.c_attn.weight", "gpt2.transformer.h.8.attn.c_attn.bias", "gpt2.transformer.h.8.attn.c_proj.weight", "gpt2.transformer.h.8.attn.c_proj.bias", "gpt2.transformer.h.8.ln_2.weight", "gpt2.transformer.h.8.ln_2.bias", "gpt2.transformer.h.8.mlp.c_fc.weight", "gpt2.transformer.h.8.mlp.c_fc.bias", "gpt2.transformer.h.8.mlp.c_proj.weight", "gpt2.transformer.h.8.mlp.c_proj.bias", "gpt2.transformer.h.9.ln_1.weight", "gpt2.transformer.h.9.ln_1.bias", "gpt2.transformer.h.9.attn.c_attn.weight", "gpt2.transformer.h.9.attn.c_attn.bias", "gpt2.transformer.h.9.attn.c_proj.weight", "gpt2.transformer.h.9.attn.c_proj.bias", "gpt2.transformer.h.9.ln_2.weight", "gpt2.transformer.h.9.ln_2.bias", "gpt2.transformer.h.9.mlp.c_fc.weight", "gpt2.transformer.h.9.mlp.c_fc.bias", "gpt2.transformer.h.9.mlp.c_proj.weight", "gpt2.transformer.h.9.mlp.c_proj.bias", "gpt2.transformer.h.10.ln_1.weight", "gpt2.transformer.h.10.ln_1.bias", "gpt2.transformer.h.10.attn.c_attn.weight", "gpt2.transformer.h.10.attn.c_attn.bias", "gpt2.transformer.h.10.attn.c_proj.weight", "gpt2.transformer.h.10.attn.c_proj.bias", "gpt2.transformer.h.10.ln_2.weight", "gpt2.transformer.h.10.ln_2.bias", "gpt2.transformer.h.10.mlp.c_fc.weight", "gpt2.transformer.h.10.mlp.c_fc.bias", "gpt2.transformer.h.10.mlp.c_proj.weight", "gpt2.transformer.h.10.mlp.c_proj.bias", "gpt2.transformer.h.11.ln_1.weight", "gpt2.transformer.h.11.ln_1.bias", "gpt2.transformer.h.11.attn.c_attn.weight", "gpt2.transformer.h.11.attn.c_attn.bias", "gpt2.transformer.h.11.attn.c_proj.weight", "gpt2.transformer.h.11.attn.c_proj.bias", "gpt2.transformer.h.11.ln_2.weight", "gpt2.transformer.h.11.ln_2.bias", "gpt2.transformer.h.11.mlp.c_fc.weight", "gpt2.transformer.h.11.mlp.c_fc.bias", "gpt2.transformer.h.11.mlp.c_proj.weight", "gpt2.transformer.h.11.mlp.c_proj.bias", "gpt2.transformer.ln_f.weight", "gpt2.transformer.ln_f.bias", "gpt2.lm_head.weight", "image_projection.0.weight", "image_projection.0.bias", "image_projection.1.weight", "image_projection.1.bias". 
	Unexpected key(s) in state_dict: "token_embedding.weight", "positional_embedding.weight", "lm_head.weight", "lm_head.bias", "transformer.layers.0.self_attn.in_proj_weight", "transformer.layers.0.self_attn.in_proj_bias", "transformer.layers.0.self_attn.out_proj.weight", "transformer.layers.0.self_attn.out_proj.bias", "transformer.layers.0.linear1.weight", "transformer.layers.0.linear1.bias", "transformer.layers.0.linear2.weight", "transformer.layers.0.linear2.bias", "transformer.layers.0.norm1.weight", "transformer.layers.0.norm1.bias", "transformer.layers.0.norm2.weight", "transformer.layers.0.norm2.bias", "transformer.layers.1.self_attn.in_proj_weight", "transformer.layers.1.self_attn.in_proj_bias", "transformer.layers.1.self_attn.out_proj.weight", "transformer.layers.1.self_attn.out_proj.bias", "transformer.layers.1.linear1.weight", "transformer.layers.1.linear1.bias", "transformer.layers.1.linear2.weight", "transformer.layers.1.linear2.bias", "transformer.layers.1.norm1.weight", "transformer.layers.1.norm1.bias", "transformer.layers.1.norm2.weight", "transformer.layers.1.norm2.bias", "transformer.layers.2.self_attn.in_proj_weight", "transformer.layers.2.self_attn.in_proj_bias", "transformer.layers.2.self_attn.out_proj.weight", "transformer.layers.2.self_attn.out_proj.bias", "transformer.layers.2.linear1.weight", "transformer.layers.2.linear1.bias", "transformer.layers.2.linear2.weight", "transformer.layers.2.linear2.bias", "transformer.layers.2.norm1.weight", "transformer.layers.2.norm1.bias", "transformer.layers.2.norm2.weight", "transformer.layers.2.norm2.bias", "image_projection.weight", "image_projection.bias". 
2025-12-18 21:58:51,389 - INFO - Epoch [1] Batch [0/3625] loss: 6.6746 
2025-12-18 22:01:03,452 - INFO - Epoch [1] Batch [1025/3625] loss: 1.4762 
2025-12-18 22:03:07,781 - INFO - Epoch [1] Batch [2050/3625] loss: 1.4567 
2025-12-18 22:05:10,845 - INFO - Epoch [1] Batch [3075/3625] loss: 1.5342 
2025-12-18 22:06:54,839 - INFO - Epoch [1] Batch [0/127] loss: 1.2657 
2025-12-18 22:06:55,361 - INFO - Epoch [1] Batch [10/127] loss: 1.4624 
2025-12-18 22:06:55,845 - INFO - Epoch [1] Batch [20/127] loss: 1.1330 
2025-12-18 22:06:56,299 - INFO - Epoch [1] Batch [30/127] loss: 1.2516 
2025-12-18 22:06:56,830 - INFO - Epoch [1] Batch [40/127] loss: 1.0682 
2025-12-18 22:06:57,283 - INFO - Epoch [1] Batch [50/127] loss: 1.5145 
2025-12-18 22:06:57,737 - INFO - Epoch [1] Batch [60/127] loss: 1.3607 
2025-12-18 22:06:58,187 - INFO - Epoch [1] Batch [70/127] loss: 1.4853 
2025-12-18 22:06:58,646 - INFO - Epoch [1] Batch [80/127] loss: 1.1946 
2025-12-18 22:06:59,186 - INFO - Epoch [1] Batch [90/127] loss: 1.6699 
2025-12-18 22:06:59,652 - INFO - Epoch [1] Batch [100/127] loss: 1.4372 
2025-12-18 22:07:00,106 - INFO - Epoch [1] Batch [110/127] loss: 1.6700 
2025-12-18 22:07:00,600 - INFO - Epoch [1] Batch [120/127] loss: 1.6048 
2025-12-18 22:07:02,536 - INFO - Epoch: [1/15]Train loss: 1.5478 | Val loss: 1.3973
2025-12-18 22:07:02,537 - INFO - --- Generating Sample Story for Epoch 1 ---
2025-12-18 22:07:05,725 - INFO - Generated Story: down the sidewalk with trees behind him while everything else fades into the background. In The Background, this is both sad as it is absolute.
2025-12-18 22:07:05,725 - INFO - Validation loss improved from 10.0000 to 1.3973. Saving Best model.
2025-12-18 22:07:06,985 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 1)
2025-12-18 22:07:08,239 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 1)
2025-12-18 22:07:37,515 - INFO - Epoch [2] Batch [0/3625] loss: 1.4343 
2025-12-18 22:09:43,452 - INFO - Epoch [2] Batch [1025/3625] loss: 1.6330 
2025-12-18 22:11:48,850 - INFO - Epoch [2] Batch [2050/3625] loss: 1.5150 
2025-12-18 22:13:54,169 - INFO - Epoch [2] Batch [3075/3625] loss: 1.6581 
2025-12-18 22:15:31,483 - INFO - Epoch [2] Batch [0/127] loss: 1.2477 
2025-12-18 22:15:31,966 - INFO - Epoch [2] Batch [10/127] loss: 1.4644 
2025-12-18 22:15:32,411 - INFO - Epoch [2] Batch [20/127] loss: 1.0919 
2025-12-18 22:15:32,865 - INFO - Epoch [2] Batch [30/127] loss: 1.2584 
2025-12-18 22:15:33,318 - INFO - Epoch [2] Batch [40/127] loss: 1.0091 
2025-12-18 22:15:33,769 - INFO - Epoch [2] Batch [50/127] loss: 1.4619 
2025-12-18 22:15:34,210 - INFO - Epoch [2] Batch [60/127] loss: 1.3075 
2025-12-18 22:15:34,660 - INFO - Epoch [2] Batch [70/127] loss: 1.4823 
2025-12-18 22:15:35,115 - INFO - Epoch [2] Batch [80/127] loss: 1.1728 
2025-12-18 22:15:35,564 - INFO - Epoch [2] Batch [90/127] loss: 1.6577 
2025-12-18 22:15:36,018 - INFO - Epoch [2] Batch [100/127] loss: 1.4357 
2025-12-18 22:15:36,471 - INFO - Epoch [2] Batch [110/127] loss: 1.6555 
2025-12-18 22:15:36,918 - INFO - Epoch [2] Batch [120/127] loss: 1.5728 
2025-12-18 22:15:38,388 - INFO - Epoch: [2/15]Train loss: 1.3691 | Val loss: 1.3750
2025-12-18 22:15:38,390 - INFO - --- Generating Sample Story for Epoch 2 ---
2025-12-18 22:15:40,638 - INFO - Generated Story: pants is being readied by the camera as the world moves quietly around. In the background, the room feels sad late at night
2025-12-18 22:15:40,638 - INFO - Validation loss improved from 1.3973 to 1.3750. Saving Best model.
2025-12-18 22:15:41,783 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 2)
2025-12-18 22:15:42,915 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 2)
2025-12-18 22:16:09,972 - INFO - Epoch [3] Batch [0/3625] loss: 1.3569 
2025-12-18 22:17:17,342 - INFO - Logging initialize successfully
2025-12-18 22:17:17,461 - INFO - Loaded 29000 rows for split='train'.
2025-12-18 22:17:17,562 - INFO - Loaded 1014 rows for split='val'.
2025-12-18 22:17:18,840 - INFO - Built 29000 samples from annotaions
2025-12-18 22:17:18,840 - INFO - Total train samples: 29000
2025-12-18 22:17:18,882 - INFO - Built 1014 samples from annotaions
2025-12-18 22:17:18,883 - INFO - Total val samples: 1014
2025-12-18 22:17:18,938 - INFO - Built 29000 samples for story 
2025-12-18 22:17:18,949 - INFO - Built 1014 samples for story 
2025-12-18 22:17:19,087 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-18 22:17:19,098 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-18 22:17:24,891 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-18 22:17:26,271 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming at epoch 2. (Scheduler restored: True, Scaler restored: True)
2025-12-18 22:17:26,271 - INFO - Checkpoint loaded from checkpoints/latest.pth (LATEST (Better Performance)). Resuming training from epoch 3, best loss tracked: 1.3750
2025-12-18 22:17:53,102 - INFO - Epoch [3] Batch [0/3625] loss: 1.3043 
2025-12-18 22:20:19,831 - INFO - Epoch [3] Batch [1200/3625] loss: 1.2027 
2025-12-18 22:22:46,430 - INFO - Epoch [3] Batch [2400/3625] loss: 1.3636 
2025-12-18 22:25:13,528 - INFO - Epoch [3] Batch [3600/3625] loss: 1.3802 
2025-12-18 22:25:47,506 - INFO - Epoch [3] Batch [0/127] loss: 1.1660 
2025-12-18 22:25:48,432 - INFO - Epoch [3] Batch [20/127] loss: 1.3266 
2025-12-18 22:25:49,330 - INFO - Epoch [3] Batch [40/127] loss: 1.4455 
2025-12-18 22:25:50,236 - INFO - Epoch [3] Batch [60/127] loss: 1.3076 
2025-12-18 22:25:51,144 - INFO - Epoch [3] Batch [80/127] loss: 1.2499 
2025-12-18 22:25:52,067 - INFO - Epoch [3] Batch [100/127] loss: 1.5317 
2025-12-18 22:25:52,976 - INFO - Epoch [3] Batch [120/127] loss: 1.2760 
2025-12-18 22:25:54,537 - INFO - Epoch: [3/15]Train loss: 1.3711 | Val loss: 1.3511
2025-12-18 22:25:54,537 - INFO - --- Generating Sample Story for Epoch 3 ---
2025-12-18 22:25:56,687 - INFO - Generated Story: with his hands in his pockets as the world moves quietly around. Inthe background, ocean feels playful just before the rain
2025-12-18 22:25:56,690 - INFO - Validation loss improved from 1.3750 to 1.3511. Saving Best model.
2025-12-18 22:25:57,804 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 3)
2025-12-18 22:25:58,854 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 3)
2025-12-18 22:26:25,579 - INFO - Epoch [4] Batch [0/3625] loss: 1.3050 
2025-12-18 22:28:55,650 - INFO - Epoch [4] Batch [1200/3625] loss: 1.1916 
2025-12-18 22:31:23,360 - INFO - Epoch [4] Batch [2400/3625] loss: 1.3479 
2025-12-18 22:33:50,229 - INFO - Epoch [4] Batch [3600/3625] loss: 1.3258 
2025-12-18 22:34:22,168 - INFO - Epoch [4] Batch [0/127] loss: 1.1770 
2025-12-18 22:34:23,068 - INFO - Epoch [4] Batch [20/127] loss: 1.3200 
2025-12-18 22:34:23,938 - INFO - Epoch [4] Batch [40/127] loss: 1.4313 
2025-12-18 22:34:24,809 - INFO - Epoch [4] Batch [60/127] loss: 1.2863 
2025-12-18 22:34:25,682 - INFO - Epoch [4] Batch [80/127] loss: 1.2363 
2025-12-18 22:34:26,555 - INFO - Epoch [4] Batch [100/127] loss: 1.5515 
2025-12-18 22:34:27,424 - INFO - Epoch [4] Batch [120/127] loss: 1.2710 
2025-12-18 22:34:28,813 - INFO - Epoch: [4/15]Train loss: 1.3112 | Val loss: 1.3452
2025-12-18 22:34:28,813 - INFO - --- Generating Sample Story for Epoch 4 ---
2025-12-18 22:34:30,575 - INFO - Generated Story: back to us in a stillness that felt both fragile and absolute. In this photo, it feels sad at the hour of midnight
2025-12-18 22:34:30,575 - INFO - Validation loss improved from 1.3511 to 1.3452. Saving Best model.
2025-12-18 22:34:31,642 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 4)
2025-12-18 22:34:32,673 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 4)
2025-12-18 22:35:00,043 - INFO - Epoch [5] Batch [0/3625] loss: 1.2124 
2025-12-18 22:37:29,699 - INFO - Epoch [5] Batch [1200/3625] loss: 1.3058 
2025-12-18 22:39:58,970 - INFO - Epoch [5] Batch [2400/3625] loss: 1.3257 
2025-12-18 22:42:28,262 - INFO - Epoch [5] Batch [3600/3625] loss: 1.2359 
2025-12-18 22:43:01,819 - INFO - Epoch [5] Batch [0/127] loss: 1.1833 
2025-12-18 22:43:02,837 - INFO - Epoch [5] Batch [20/127] loss: 1.3434 
2025-12-18 22:43:03,745 - INFO - Epoch [5] Batch [40/127] loss: 1.4563 
2025-12-18 22:43:04,655 - INFO - Epoch [5] Batch [60/127] loss: 1.2553 
2025-12-18 22:43:05,566 - INFO - Epoch [5] Batch [80/127] loss: 1.2298 
2025-12-18 22:43:06,483 - INFO - Epoch [5] Batch [100/127] loss: 1.5640 
2025-12-18 22:43:07,390 - INFO - Epoch [5] Batch [120/127] loss: 1.2604 
2025-12-18 22:43:08,902 - INFO - Epoch: [5/15]Train loss: 1.2639 | Val loss: 1.3458
2025-12-18 22:43:08,902 - INFO - --- Generating Sample Story for Epoch 5 ---
2025-12-18 22:43:19,977 - WARNING - '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /gpt2/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001CFC2BA84C0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 7c1553b6-51bc-4337-9afc-7a363194e603)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/tokenizer_config.json
2025-12-18 22:43:19,977 - WARNING - Retrying in 1s [Retry 1/5].
2025-12-18 22:43:24,038 - WARNING - '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /gpt2/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001CFCB68D9D0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 5dc024d8-7144-4c01-b6bc-a979a49a9953)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/tokenizer_config.json
2025-12-18 22:43:24,038 - WARNING - Retrying in 2s [Retry 2/5].
2025-12-18 22:43:26,040 - WARNING - '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /gpt2/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001D0F3AA3E50>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 6e9839eb-186a-4680-ac3d-67a91f8bd56c)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/tokenizer_config.json
2025-12-18 22:43:26,040 - WARNING - Retrying in 4s [Retry 3/5].
2025-12-18 22:43:38,982 - INFO - Generated Story: else fades into the background. In the early morning
2025-12-18 22:43:38,982 - INFO - Validation loss did not improve. Current best loss: 1.3452 (Patience: 1/5)
2025-12-18 22:43:40,005 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 5)
2025-12-18 22:44:07,118 - INFO - Epoch [6] Batch [0/3625] loss: 1.2307 
2025-12-18 22:59:55,042 - INFO - Epoch [6] Batch [1200/3625] loss: 1.5653 
2025-12-19 19:25:35,139 - INFO - Logging initialize successfully
2025-12-19 19:25:35,299 - INFO - Loaded 29000 rows for split='train'.
2025-12-19 19:25:35,421 - INFO - Loaded 1014 rows for split='val'.
2025-12-19 19:25:36,710 - INFO - Built 29000 samples from annotaions
2025-12-19 19:25:36,713 - INFO - Total train samples: 29000
2025-12-19 19:25:36,748 - INFO - Built 1014 samples from annotaions
2025-12-19 19:25:36,748 - INFO - Total val samples: 1014
2025-12-19 19:25:36,815 - INFO - Built 29000 samples for story 
2025-12-19 19:25:36,818 - INFO - Built 1014 samples for story 
2025-12-19 19:25:36,963 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-19 19:25:36,966 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-19 19:25:42,105 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-19 19:25:43,995 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming at epoch 5. (Scheduler restored: True, Scaler restored: True)
2025-12-19 19:25:44,001 - INFO - Checkpoint loaded from checkpoints/latest.pth (LATEST (Better Performance)). Resuming training from epoch 6, best loss tracked: 1.3452
2025-12-19 19:26:12,829 - INFO - Epoch [6] Batch [0/3625] loss: 1.2856 
2025-12-19 19:28:44,124 - INFO - Epoch [6] Batch [1200/3625] loss: 1.2762 
2025-12-19 19:31:14,761 - INFO - Epoch [6] Batch [2400/3625] loss: 1.5104 
2025-12-19 19:33:45,198 - INFO - Epoch [6] Batch [3600/3625] loss: 1.3248 
2025-12-19 19:34:18,451 - INFO - Epoch [6] Batch [0/127] loss: 1.1276 
2025-12-19 19:34:19,347 - INFO - Epoch [6] Batch [20/127] loss: 1.4324 
2025-12-19 19:34:20,208 - INFO - Epoch [6] Batch [40/127] loss: 1.3628 
2025-12-19 19:34:21,069 - INFO - Epoch [6] Batch [60/127] loss: 1.3478 
2025-12-19 19:34:21,934 - INFO - Epoch [6] Batch [80/127] loss: 1.1485 
2025-12-19 19:34:22,797 - INFO - Epoch [6] Batch [100/127] loss: 1.3833 
2025-12-19 19:34:23,656 - INFO - Epoch [6] Batch [120/127] loss: 1.5321 
2025-12-19 19:34:25,060 - INFO - Epoch: [6/15]Train loss: 1.3169 | Val loss: 1.3355
2025-12-19 19:34:25,060 - INFO - --- Generating Sample Story for Epoch 6 ---
2025-12-19 19:34:27,214 - INFO - Generated Story: air on an outdoor garden patio until the feeling was all that remained. In the background, the field feels sad in early morning
2025-12-19 19:34:27,214 - INFO - Validation loss improved from 1.3452 to 1.3355. Saving Best model.
2025-12-19 19:34:28,288 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 6)
2025-12-19 19:34:29,312 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 6)
2025-12-19 19:34:56,543 - INFO - Epoch [7] Batch [0/3625] loss: 1.4199 
2025-12-19 19:37:25,953 - INFO - Epoch [7] Batch [1200/3625] loss: 1.0973 
2025-12-19 19:39:54,144 - INFO - Epoch [7] Batch [2400/3625] loss: 1.2638 
2025-12-19 19:42:23,093 - INFO - Epoch [7] Batch [3600/3625] loss: 1.5874 
2025-12-19 19:42:55,312 - INFO - Epoch [7] Batch [0/127] loss: 1.1285 
2025-12-19 19:42:56,191 - INFO - Epoch [7] Batch [20/127] loss: 1.4316 
2025-12-19 19:42:57,060 - INFO - Epoch [7] Batch [40/127] loss: 1.3808 
2025-12-19 19:42:57,916 - INFO - Epoch [7] Batch [60/127] loss: 1.3792 
2025-12-19 19:42:58,778 - INFO - Epoch [7] Batch [80/127] loss: 1.1230 
2025-12-19 19:42:59,642 - INFO - Epoch [7] Batch [100/127] loss: 1.4287 
2025-12-19 19:43:00,502 - INFO - Epoch [7] Batch [120/127] loss: 1.5377 
2025-12-19 19:43:01,884 - INFO - Epoch: [7/15]Train loss: 1.2652 | Val loss: 1.3344
2025-12-19 19:43:01,884 - INFO - --- Generating Sample Story for Epoch 7 ---
2025-12-19 19:43:03,532 - INFO - Generated Story: until the feeling was all that remained. In the background, the street feels lonely just before the rain
2025-12-19 19:43:03,532 - INFO - Validation loss improved from 1.3355 to 1.3344. Saving Best model.
2025-12-19 19:43:04,565 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 7)
2025-12-19 19:43:05,608 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 7)
2025-12-19 19:43:32,595 - INFO - Epoch [8] Batch [0/3625] loss: 1.1436 
2025-12-19 19:46:00,079 - INFO - Epoch [8] Batch [1200/3625] loss: 1.3552 
2025-12-19 19:48:30,021 - INFO - Epoch [8] Batch [2400/3625] loss: 1.1363 
2025-12-19 19:50:58,950 - INFO - Epoch [8] Batch [3600/3625] loss: 1.2386 
2025-12-19 19:51:32,977 - INFO - Epoch [8] Batch [0/127] loss: 1.1298 
2025-12-19 19:51:33,873 - INFO - Epoch [8] Batch [20/127] loss: 1.4668 
2025-12-19 19:51:34,755 - INFO - Epoch [8] Batch [40/127] loss: 1.3849 
2025-12-19 19:51:35,628 - INFO - Epoch [8] Batch [60/127] loss: 1.4328 
2025-12-19 19:51:36,505 - INFO - Epoch [8] Batch [80/127] loss: 1.1510 
2025-12-19 19:51:37,379 - INFO - Epoch [8] Batch [100/127] loss: 1.4392 
2025-12-19 19:51:38,257 - INFO - Epoch [8] Batch [120/127] loss: 1.5978 
2025-12-19 19:51:39,661 - INFO - Epoch: [8/15]Train loss: 1.2204 | Val loss: 1.3454
2025-12-19 19:51:39,661 - INFO - --- Generating Sample Story for Epoch 8 ---
2025-12-19 19:51:41,858 - INFO - Generated Story: a stillness that felt both fragile and absolute. In the background, the stadium feels sad just before the rain
2025-12-19 19:51:41,858 - INFO - Validation loss did not improve. Current best loss: 1.3344 (Patience: 1/5)
2025-12-19 19:51:42,918 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 8)
2025-12-19 19:52:10,050 - INFO - Epoch [9] Batch [0/3625] loss: 1.2891 
2025-12-19 19:54:37,334 - INFO - Epoch [9] Batch [1200/3625] loss: 1.1785 
2025-12-19 19:57:06,443 - INFO - Epoch [9] Batch [2400/3625] loss: 1.3364 
2025-12-19 19:59:33,263 - INFO - Epoch [9] Batch [3600/3625] loss: 1.0785 
2025-12-19 20:00:05,621 - INFO - Epoch [9] Batch [0/127] loss: 1.1315 
2025-12-19 20:00:06,518 - INFO - Epoch [9] Batch [20/127] loss: 1.5054 
2025-12-19 20:00:07,388 - INFO - Epoch [9] Batch [40/127] loss: 1.3958 
2025-12-19 20:00:08,263 - INFO - Epoch [9] Batch [60/127] loss: 1.4639 
2025-12-19 20:00:09,136 - INFO - Epoch [9] Batch [80/127] loss: 1.1297 
2025-12-19 20:00:10,012 - INFO - Epoch [9] Batch [100/127] loss: 1.4463 
2025-12-19 20:00:10,886 - INFO - Epoch [9] Batch [120/127] loss: 1.6102 
2025-12-19 20:00:12,272 - INFO - Epoch: [9/15]Train loss: 1.1805 | Val loss: 1.3591
2025-12-19 20:00:12,272 - INFO - --- Generating Sample Story for Epoch 9 ---
2025-12-19 20:00:13,916 - INFO - Generated Story: was all that remained. In the background, the field feels playful late at night
2025-12-19 20:00:13,916 - INFO - Validation loss did not improve. Current best loss: 1.3344 (Patience: 2/5)
2025-12-19 20:00:15,000 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 9)
2025-12-19 20:00:41,706 - INFO - Epoch [10] Batch [0/3625] loss: 1.1569 
2025-12-19 20:03:08,720 - INFO - Epoch [10] Batch [1200/3625] loss: 1.0450 
2025-12-19 20:05:35,972 - INFO - Epoch [10] Batch [2400/3625] loss: 1.0193 
2025-12-19 20:08:05,484 - INFO - Epoch [10] Batch [3600/3625] loss: 1.2234 
2025-12-19 20:08:39,474 - INFO - Epoch [10] Batch [0/127] loss: 1.1793 
2025-12-19 20:08:40,516 - INFO - Epoch [10] Batch [20/127] loss: 1.4910 
2025-12-19 20:08:41,432 - INFO - Epoch [10] Batch [40/127] loss: 1.4165 
2025-12-19 20:08:42,356 - INFO - Epoch [10] Batch [60/127] loss: 1.4329 
2025-12-19 20:08:43,287 - INFO - Epoch [10] Batch [80/127] loss: 1.1256 
2025-12-19 20:08:44,217 - INFO - Epoch [10] Batch [100/127] loss: 1.4593 
2025-12-19 20:08:45,135 - INFO - Epoch [10] Batch [120/127] loss: 1.6359 
2025-12-19 20:08:46,656 - INFO - Epoch: [10/15]Train loss: 1.1414 | Val loss: 1.3704
2025-12-19 20:08:46,656 - INFO - --- Generating Sample Story for Epoch 10 ---
2025-12-19 20:08:48,414 - INFO - Generated Story: shown in a stillness that felt both fragile, absolute. In the background, the ocean feels tense just before the rain
2025-12-19 20:08:48,414 - INFO - Validation loss did not improve. Current best loss: 1.3344 (Patience: 3/5)
2025-12-19 20:08:48,414 - INFO - Manually reducing LR from 5.00e-05 to 5.00e-06
2025-12-19 20:08:49,529 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 10)
2025-12-19 20:09:16,749 - INFO - Epoch [11] Batch [0/3625] loss: 1.0076 
2025-12-19 20:11:45,658 - INFO - Epoch [11] Batch [1200/3625] loss: 1.1404 
2025-12-19 20:14:12,505 - INFO - Epoch [11] Batch [2400/3625] loss: 1.1095 
2025-12-19 20:16:41,888 - INFO - Epoch [11] Batch [3600/3625] loss: 1.0559 
2025-12-19 20:17:15,731 - INFO - Epoch [11] Batch [0/127] loss: 1.1817 
2025-12-19 20:17:16,659 - INFO - Epoch [11] Batch [20/127] loss: 1.5228 
2025-12-19 20:17:17,568 - INFO - Epoch [11] Batch [40/127] loss: 1.4164 
2025-12-19 20:17:18,482 - INFO - Epoch [11] Batch [60/127] loss: 1.4923 
2025-12-19 20:17:19,394 - INFO - Epoch [11] Batch [80/127] loss: 1.1180 
2025-12-19 20:17:20,302 - INFO - Epoch [11] Batch [100/127] loss: 1.4765 
2025-12-19 20:17:21,230 - INFO - Epoch [11] Batch [120/127] loss: 1.6602 
2025-12-19 20:17:22,753 - INFO - Epoch: [11/15]Train loss: 1.0717 | Val loss: 1.3897
2025-12-19 20:17:22,753 - INFO - --- Generating Sample Story for Epoch 11 ---
2025-12-19 20:17:25,886 - INFO - Generated Story: green fence in front of them with trees while everything else fades into the background. In the distant, the market feels sad just before the rain
2025-12-19 20:17:25,886 - INFO - Validation loss did not improve. Current best loss: 1.3344 (Patience: 4/5)
2025-12-19 20:17:26,958 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 11)
2025-12-19 20:17:55,427 - INFO - Epoch [12] Batch [0/3625] loss: 1.0505 
2025-12-19 20:20:25,267 - INFO - Epoch [12] Batch [1200/3625] loss: 1.0788 
2025-12-19 20:22:54,422 - INFO - Epoch [12] Batch [2400/3625] loss: 0.9975 
2025-12-19 20:25:21,576 - INFO - Epoch [12] Batch [3600/3625] loss: 0.9082 
2025-12-19 20:25:53,393 - INFO - Epoch [12] Batch [0/127] loss: 1.1864 
2025-12-19 20:25:54,398 - INFO - Epoch [12] Batch [20/127] loss: 1.5247 
2025-12-19 20:25:55,373 - INFO - Epoch [12] Batch [40/127] loss: 1.4209 
2025-12-19 20:25:56,365 - INFO - Epoch [12] Batch [60/127] loss: 1.5039 
2025-12-19 20:25:57,271 - INFO - Epoch [12] Batch [80/127] loss: 1.1227 
2025-12-19 20:25:58,165 - INFO - Epoch [12] Batch [100/127] loss: 1.4819 
2025-12-19 20:25:59,049 - INFO - Epoch [12] Batch [120/127] loss: 1.6625 
2025-12-19 20:26:00,422 - INFO - Epoch: [12/15]Train loss: 1.0600 | Val loss: 1.3931
2025-12-19 20:26:00,427 - INFO - --- Generating Sample Story for Epoch 12 ---
2025-12-19 20:26:02,240 - INFO - Generated Story: world moves quietly around. In the background, the city feels sad in the early morning
2025-12-19 20:26:02,240 - INFO - Validation loss did not improve. Current best loss: 1.3344 (Patience: 5/5)
2025-12-19 20:26:02,240 - INFO - Early stopping triggered after 5 epochs without improvement
2025-12-19 20:27:02,311 - INFO - Logging initialize successfully
2025-12-19 20:27:02,441 - INFO - Loaded 29000 rows for split='train'.
2025-12-19 20:27:02,564 - INFO - Loaded 1014 rows for split='val'.
2025-12-19 20:27:03,835 - INFO - Built 29000 samples from annotaions
2025-12-19 20:27:03,836 - INFO - Total train samples: 29000
2025-12-19 20:27:03,872 - INFO - Built 1014 samples from annotaions
2025-12-19 20:27:03,872 - INFO - Total val samples: 1014
2025-12-19 20:27:03,933 - INFO - Built 29000 samples for story 
2025-12-19 20:27:03,941 - INFO - Built 1014 samples for story 
2025-12-19 20:27:04,090 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-19 20:27:04,096 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-19 20:27:09,297 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-19 20:27:11,091 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming at epoch 11. (Scheduler restored: True, Scaler restored: True)
2025-12-19 20:27:11,091 - INFO - Checkpoint loaded from checkpoints/latest.pth (LATEST (Better Performance)). Resuming training from epoch 12, best loss tracked: 1.3344
2025-12-19 20:27:39,666 - INFO - Epoch [12] Batch [0/3625] loss: 1.6054 
2025-12-19 20:30:06,881 - INFO - Epoch [12] Batch [1200/3625] loss: 1.4167 
2025-12-19 20:32:33,773 - INFO - Epoch [12] Batch [2400/3625] loss: 1.0052 
2025-12-19 20:35:01,000 - INFO - Epoch [12] Batch [3600/3625] loss: 1.2652 
2025-12-19 20:35:33,608 - INFO - Epoch [12] Batch [0/127] loss: 1.0695 
2025-12-19 20:35:34,504 - INFO - Epoch [12] Batch [20/127] loss: 1.1930 
2025-12-19 20:35:35,368 - INFO - Epoch [12] Batch [40/127] loss: 1.2969 
2025-12-19 20:35:36,236 - INFO - Epoch [12] Batch [60/127] loss: 1.3630 
2025-12-19 20:35:37,096 - INFO - Epoch [12] Batch [80/127] loss: 1.3017 
2025-12-19 20:35:37,967 - INFO - Epoch [12] Batch [100/127] loss: 1.3170 
2025-12-19 20:35:38,831 - INFO - Epoch [12] Batch [120/127] loss: 1.2878 
2025-12-19 20:35:40,209 - INFO - Epoch: [12/20]Train loss: 1.2682 | Val loss: 1.3285
2025-12-19 20:35:40,210 - INFO - --- Generating Sample Story for Epoch 12 ---
2025-12-19 20:35:42,336 - INFO - Generated Story: and cowboy hat until the feeling was all that remained. In the background, the park feels sad under the afternoon sun
2025-12-19 20:35:42,336 - INFO - Validation loss improved from 1.3344 to 1.3285. Saving Best model.
2025-12-19 20:35:43,502 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 12)
2025-12-19 20:35:44,618 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 12)
2025-12-19 20:36:12,245 - INFO - Epoch [13] Batch [0/3625] loss: 1.0117 
2025-12-19 20:38:43,328 - INFO - Epoch [13] Batch [1200/3625] loss: 1.1268 
2025-12-19 20:41:22,572 - INFO - Epoch [13] Batch [2400/3625] loss: 1.2005 
2025-12-19 20:44:10,682 - INFO - Epoch [13] Batch [3600/3625] loss: 1.4589 
2025-12-19 20:44:56,977 - INFO - Epoch [13] Batch [0/127] loss: 1.0666 
2025-12-19 20:44:58,154 - INFO - Epoch [13] Batch [20/127] loss: 1.1906 
2025-12-19 20:44:59,350 - INFO - Epoch [13] Batch [40/127] loss: 1.2860 
2025-12-19 20:45:00,523 - INFO - Epoch [13] Batch [60/127] loss: 1.3590 
2025-12-19 20:45:01,710 - INFO - Epoch [13] Batch [80/127] loss: 1.2950 
2025-12-19 20:45:02,863 - INFO - Epoch [13] Batch [100/127] loss: 1.3121 
2025-12-19 20:45:04,020 - INFO - Epoch [13] Batch [120/127] loss: 1.2795 
2025-12-19 20:45:05,712 - INFO - Epoch: [13/20]Train loss: 1.2545 | Val loss: 1.3225
2025-12-19 20:45:05,712 - INFO - --- Generating Sample Story for Epoch 13 ---
2025-12-19 20:45:07,876 - INFO - Generated Story: , holding flowers until the feeling was all that remained. In the background, the room feels happy under the afternoon sun
2025-12-19 20:45:07,877 - INFO - Validation loss improved from 1.3285 to 1.3225. Saving Best model.
2025-12-19 20:45:09,391 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 13)
2025-12-19 20:45:10,876 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 13)
2025-12-19 20:45:43,333 - INFO - Epoch [14] Batch [0/3625] loss: 1.0530 
2025-12-19 20:48:31,198 - INFO - Epoch [14] Batch [1200/3625] loss: 1.0752 
2025-12-19 20:51:21,046 - INFO - Epoch [14] Batch [2400/3625] loss: 1.2118 
2025-12-19 20:54:10,433 - INFO - Epoch [14] Batch [3600/3625] loss: 1.3746 
2025-12-19 20:54:56,896 - INFO - Epoch [14] Batch [0/127] loss: 1.0638 
2025-12-19 20:54:58,093 - INFO - Epoch [14] Batch [20/127] loss: 1.1966 
2025-12-19 20:54:59,281 - INFO - Epoch [14] Batch [40/127] loss: 1.2800 
2025-12-19 20:55:00,437 - INFO - Epoch [14] Batch [60/127] loss: 1.3601 
2025-12-19 20:55:01,627 - INFO - Epoch [14] Batch [80/127] loss: 1.2921 
2025-12-19 20:55:02,791 - INFO - Epoch [14] Batch [100/127] loss: 1.3100 
2025-12-19 20:55:03,958 - INFO - Epoch [14] Batch [120/127] loss: 1.2753 
2025-12-19 20:55:05,718 - INFO - Epoch: [14/20]Train loss: 1.2472 | Val loss: 1.3205
2025-12-19 20:55:05,718 - INFO - --- Generating Sample Story for Epoch 14 ---
2025-12-19 20:55:08,129 - INFO - Generated Story: grass with trees and the truth hung heavy in the silence. In the background, the street feels sad at the hour of midnight
2025-12-19 20:55:08,130 - INFO - Validation loss improved from 1.3225 to 1.3205. Saving Best model.
2025-12-19 20:55:09,590 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 14)
2025-12-19 20:55:11,074 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 14)
2025-12-19 20:55:47,927 - INFO - Epoch [15] Batch [0/3625] loss: 1.1066 
2025-12-20 11:49:32,179 - INFO - Logging initialize successfully
2025-12-20 11:49:32,303 - INFO - Loaded 29000 rows for split='train'.
2025-12-20 11:49:32,412 - INFO - Loaded 1014 rows for split='val'.
2025-12-20 11:49:33,655 - INFO - Built 29000 samples from annotaions
2025-12-20 11:49:33,661 - INFO - Total train samples: 29000
2025-12-20 11:49:33,696 - INFO - Built 1014 samples from annotaions
2025-12-20 11:49:33,696 - INFO - Total val samples: 1014
2025-12-20 11:49:33,750 - INFO - Built 29000 samples for story 
2025-12-20 11:49:33,765 - INFO - Built 1014 samples for story 
2025-12-20 11:49:33,898 - INFO - Cleaned data saved to data/processed\stories_train.jsonl
2025-12-20 11:49:33,914 - INFO - Cleaned data saved to data/processed\stories_val.jsonl
2025-12-20 11:49:39,431 - INFO - Model initialized. Trainable parameters: 73250304
2025-12-20 11:49:41,136 - INFO - Checkpoint loaded from checkpoints/latest.pth. Resuming at epoch 14. (Scheduler restored: True, Scaler restored: True)
2025-12-20 11:49:41,136 - INFO - Checkpoint loaded from checkpoints/latest.pth (LATEST (Better Performance)). Resuming training from epoch 15, best loss tracked: 1.3205
2025-12-20 11:50:08,642 - INFO - Epoch [15] Batch [0/3625] loss: 1.4294 
2025-12-20 11:52:35,327 - INFO - Epoch [15] Batch [1200/3625] loss: 1.1744 
2025-12-20 11:55:04,621 - INFO - Epoch [15] Batch [2400/3625] loss: 1.0728 
2025-12-20 11:57:33,647 - INFO - Epoch [15] Batch [3600/3625] loss: 1.0991 
2025-12-20 11:58:07,793 - INFO - Epoch [15] Batch [0/127] loss: 1.2055 
2025-12-20 11:58:08,724 - INFO - Epoch [15] Batch [20/127] loss: 1.2609 
2025-12-20 11:58:09,626 - INFO - Epoch [15] Batch [40/127] loss: 1.4494 
2025-12-20 11:58:10,533 - INFO - Epoch [15] Batch [60/127] loss: 1.2946 
2025-12-20 11:58:11,435 - INFO - Epoch [15] Batch [80/127] loss: 0.9649 
2025-12-20 11:58:12,348 - INFO - Epoch [15] Batch [100/127] loss: 1.3366 
2025-12-20 11:58:13,257 - INFO - Epoch [15] Batch [120/127] loss: 1.3461 
2025-12-20 11:58:14,794 - INFO - Epoch: [15/20]Train loss: 1.2531 | Val loss: 1.3020
2025-12-20 11:58:14,796 - INFO - --- Generating Sample Story for Epoch 15 ---
2025-12-20 11:58:17,206 - INFO - Generated Story: eled sandals is being shown in a stillness that felt both fragile and absolute. In the background, the field feels lonely late at night
2025-12-20 11:58:17,206 - INFO - Validation loss improved from 1.3205 to 1.3020. Saving Best model.
2025-12-20 11:58:18,346 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 15)
2025-12-20 11:58:19,451 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 15)
2025-12-20 11:58:47,457 - INFO - Epoch [16] Batch [0/3625] loss: 1.1742 
2025-12-20 12:01:16,722 - INFO - Epoch [16] Batch [1200/3625] loss: 1.3870 
2025-12-20 12:03:43,827 - INFO - Epoch [16] Batch [2400/3625] loss: 1.1585 
2025-12-20 12:06:10,660 - INFO - Epoch [16] Batch [3600/3625] loss: 1.0825 
2025-12-20 12:06:43,051 - INFO - Epoch [16] Batch [0/127] loss: 1.2034 
2025-12-20 12:06:43,934 - INFO - Epoch [16] Batch [20/127] loss: 1.2602 
2025-12-20 12:06:44,802 - INFO - Epoch [16] Batch [40/127] loss: 1.4487 
2025-12-20 12:06:45,660 - INFO - Epoch [16] Batch [60/127] loss: 1.2950 
2025-12-20 12:06:46,521 - INFO - Epoch [16] Batch [80/127] loss: 0.9674 
2025-12-20 12:06:47,394 - INFO - Epoch [16] Batch [100/127] loss: 1.3384 
2025-12-20 12:06:48,252 - INFO - Epoch [16] Batch [120/127] loss: 1.3433 
2025-12-20 12:06:49,642 - INFO - Epoch: [16/20]Train loss: 1.2471 | Val loss: 1.3006
2025-12-20 12:06:49,642 - INFO - --- Generating Sample Story for Epoch 16 ---
2025-12-20 12:06:52,629 - INFO - Generated Story: in a stillness that felt both fragile and absolute. In the background, the city feels lonely at the hour of midnight
2025-12-20 12:06:52,629 - INFO - Validation loss improved from 1.3020 to 1.3006. Saving Best model.
2025-12-20 12:06:53,744 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 16)
2025-12-20 12:06:54,827 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 16)
2025-12-20 12:07:21,913 - INFO - Epoch [17] Batch [0/3625] loss: 1.1867 
2025-12-20 12:09:48,867 - INFO - Epoch [17] Batch [1200/3625] loss: 1.0936 
2025-12-20 12:12:16,766 - INFO - Epoch [17] Batch [2400/3625] loss: 0.9957 
2025-12-20 12:14:46,029 - INFO - Epoch [17] Batch [3600/3625] loss: 1.1492 
2025-12-20 12:15:17,095 - INFO - Epoch [17] Batch [0/127] loss: 1.2011 
2025-12-20 12:15:18,016 - INFO - Epoch [17] Batch [20/127] loss: 1.2606 
2025-12-20 12:15:18,932 - INFO - Epoch [17] Batch [40/127] loss: 1.4462 
2025-12-20 12:15:19,828 - INFO - Epoch [17] Batch [60/127] loss: 1.2931 
2025-12-20 12:15:20,740 - INFO - Epoch [17] Batch [80/127] loss: 0.9709 
2025-12-20 12:15:21,647 - INFO - Epoch [17] Batch [100/127] loss: 1.3352 
2025-12-20 12:15:22,549 - INFO - Epoch [17] Batch [120/127] loss: 1.3422 
2025-12-20 12:15:24,013 - INFO - Epoch: [17/20]Train loss: 1.2416 | Val loss: 1.2994
2025-12-20 12:15:24,013 - INFO - --- Generating Sample Story for Epoch 17 ---
2025-12-20 12:15:25,742 - INFO - Generated Story: feeling was all that remained. In the background, the field feels calm as the evening settles
2025-12-20 12:15:25,744 - INFO - Validation loss improved from 1.3006 to 1.2994. Saving Best model.
2025-12-20 12:15:26,861 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 17)
2025-12-20 12:15:27,980 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 17)
2025-12-20 12:15:55,168 - INFO - Epoch [18] Batch [0/3625] loss: 1.0620 
2025-12-20 12:18:24,420 - INFO - Epoch [18] Batch [1200/3625] loss: 1.1116 
2025-12-20 12:20:54,105 - INFO - Epoch [18] Batch [2400/3625] loss: 1.1122 
2025-12-20 12:23:22,129 - INFO - Epoch [18] Batch [3600/3625] loss: 1.1885 
2025-12-20 12:23:54,342 - INFO - Epoch [18] Batch [0/127] loss: 1.2005 
2025-12-20 12:23:55,233 - INFO - Epoch [18] Batch [20/127] loss: 1.2598 
2025-12-20 12:23:56,092 - INFO - Epoch [18] Batch [40/127] loss: 1.4454 
2025-12-20 12:23:56,948 - INFO - Epoch [18] Batch [60/127] loss: 1.2897 
2025-12-20 12:23:57,813 - INFO - Epoch [18] Batch [80/127] loss: 0.9687 
2025-12-20 12:23:58,673 - INFO - Epoch [18] Batch [100/127] loss: 1.3363 
2025-12-20 12:23:59,539 - INFO - Epoch [18] Batch [120/127] loss: 1.3401 
2025-12-20 12:24:00,918 - INFO - Epoch: [18/20]Train loss: 1.2364 | Val loss: 1.2989
2025-12-20 12:24:00,918 - INFO - --- Generating Sample Story for Epoch 18 ---
2025-12-20 12:24:03,104 - INFO - Generated Story: middle of an open park while everything else fades into the background. In the foreground, the stadium feels cheerful late at night
2025-12-20 12:24:03,104 - INFO - Validation loss improved from 1.2994 to 1.2989. Saving Best model.
2025-12-20 12:24:04,240 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 18)
2025-12-20 12:24:05,308 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 18)
2025-12-20 12:24:32,016 - INFO - Epoch [19] Batch [0/3625] loss: 1.3722 
2025-12-20 12:26:59,208 - INFO - Epoch [19] Batch [1200/3625] loss: 1.1604 
2025-12-20 12:29:27,659 - INFO - Epoch [19] Batch [2400/3625] loss: 1.0563 
2025-12-20 12:31:56,991 - INFO - Epoch [19] Batch [3600/3625] loss: 1.3727 
2025-12-20 12:32:28,396 - INFO - Epoch [19] Batch [0/127] loss: 1.2030 
2025-12-20 12:32:29,324 - INFO - Epoch [19] Batch [20/127] loss: 1.2620 
2025-12-20 12:32:30,219 - INFO - Epoch [19] Batch [40/127] loss: 1.4441 
2025-12-20 12:32:31,122 - INFO - Epoch [19] Batch [60/127] loss: 1.2897 
2025-12-20 12:32:32,022 - INFO - Epoch [19] Batch [80/127] loss: 0.9715 
2025-12-20 12:32:32,917 - INFO - Epoch [19] Batch [100/127] loss: 1.3348 
2025-12-20 12:32:33,822 - INFO - Epoch [19] Batch [120/127] loss: 1.3408 
2025-12-20 12:32:35,293 - INFO - Epoch: [19/20]Train loss: 1.2320 | Val loss: 1.2989
2025-12-20 12:32:35,293 - INFO - --- Generating Sample Story for Epoch 19 ---
2025-12-20 12:32:36,929 - INFO - Generated Story: and the truth hung heavy in the silence. In the background, the stadium feels sad as the evening settles
2025-12-20 12:32:36,929 - INFO - Validation loss improved from 1.2989 to 1.2989. Saving Best model.
2025-12-20 12:32:38,063 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 19)
2025-12-20 12:32:39,194 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 19)
2025-12-20 12:33:06,291 - INFO - Epoch [20] Batch [0/3625] loss: 1.3846 
2025-12-20 12:35:33,201 - INFO - Epoch [20] Batch [1200/3625] loss: 1.0709 
2025-12-20 12:38:00,118 - INFO - Epoch [20] Batch [2400/3625] loss: 1.3755 
2025-12-20 12:40:27,011 - INFO - Epoch [20] Batch [3600/3625] loss: 1.1362 
2025-12-20 12:40:57,458 - INFO - Epoch [20] Batch [0/127] loss: 1.2025 
2025-12-20 12:40:58,346 - INFO - Epoch [20] Batch [20/127] loss: 1.2597 
2025-12-20 12:40:59,208 - INFO - Epoch [20] Batch [40/127] loss: 1.4446 
2025-12-20 12:41:00,072 - INFO - Epoch [20] Batch [60/127] loss: 1.2900 
2025-12-20 12:41:00,931 - INFO - Epoch [20] Batch [80/127] loss: 0.9736 
2025-12-20 12:41:01,803 - INFO - Epoch [20] Batch [100/127] loss: 1.3335 
2025-12-20 12:41:02,659 - INFO - Epoch [20] Batch [120/127] loss: 1.3362 
2025-12-20 12:41:04,005 - INFO - Epoch: [20/20]Train loss: 1.2275 | Val loss: 1.2987
2025-12-20 12:41:04,005 - INFO - --- Generating Sample Story for Epoch 20 ---
2025-12-20 12:41:05,990 - INFO - Generated Story: orange shirt walking on the sidewalk until the feeling was all that remained. In the background, the park feels cheerful as the evening settles
2025-12-20 12:41:05,990 - INFO - Validation loss improved from 1.2989 to 1.2987. Saving Best model.
2025-12-20 12:41:07,129 - INFO - Checkpoint saved to checkpoints/best.pth (resumption epoch: 20)
2025-12-20 12:41:08,285 - INFO - Checkpoint saved to checkpoints/latest.pth (resumption epoch: 20)